# Azure ML Pipeline Definition (v2 YAML)
# Defines the sequence of steps for training the fraud detection model.

$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline

display_name: Fraud_Detection_Training_Pipeline
description: Pipeline to preprocess data, train (XGBoost GridSearch), evaluate, and register a fraud detection model.
experiment_name: fraud-detection-training

# --- Pipeline Inputs ---
inputs:
  raw_data_input:
    type: uri_folder
    path: azureml:raw-fraud-data@latest # Assumes raw data is registered
    mode: download
  # Date range for the *output* of the prep step (and input to train step)
  # This also defines the end date for raw data loading in prep step.
  prep_output_start_date: "2025-06-11"
  prep_output_end_date: "2025-08-14"
  # Anchor date for training splits (used *inside* train.py)
  anchor_date_str: "2025-07-25"
  # Deltas used for calculating splits (used *inside* train.py)
  delta_train: 7
  delta_delay: 7
  delta_assessment: 7
  # Folds for prequential validation (used *inside* train.py)
  n_folds: 4
  # Evaluation threshold
  deploy_threshold_value: 0.7

# --- Pipeline Outputs ---
# Define outputs accessible after the pipeline run completes
outputs:
  trained_model_output: # Output name for the trained model artifact
    type: mlflow_model # Specific type for MLflow models
    mode: upload # Upload the output from the compute target
  evaluation_report_output: # Output name for evaluation results (maps from evaluate_model's output)
    type: uri_folder
    mode: upload
  model_info_output: # Output name for the registered model info JSON
     type: uri_folder
     mode: upload

# --- Pipeline Settings ---
settings:
  default_datastore: azureml:workspaceblobstore # Default datastore for outputs if not specified
  default_compute: azureml:cpu-cluster # Default compute target for steps
  continue_on_step_failure: false # Fail the pipeline if any step fails

# --- Pipeline Jobs (Steps) ---
jobs:
  # 1. Preparation Step
  prep_data:
    type: command
    name: prep_raw_data
    display_name: Prepare Raw Data
    description: Applies feature transformations considering lookback.
    inputs:
      raw_data: ${{parent.inputs.raw_data_input}}
      # Pass the desired *output* date range for transformed data
      output_start_date: ${{parent.inputs.prep_output_start_date}}
      output_end_date: ${{parent.inputs.prep_output_end_date}}
    outputs:
      transformed_data: # Output folder containing transformed daily files
        type: uri_folder
        mode: upload
    code: ../../../data-science/src
    command: >-
      python prep.py
      --raw_data ${{inputs.raw_data}}
      --transformed_data ${{outputs.transformed_data}}
      --output_start_date ${{inputs.output_start_date}}
      --output_end_date ${{inputs.output_end_date}}
    environment: azureml:fraud-detection-train-env@latest

  # 2. Training Step (Includes Model Selection for XGBoost)
  train_model:
    type: command
    name: train_fraud_model
    display_name: Train Fraud Detection Model (XGBoost GridSearch)
    description: Performs grid search for XGBoost and trains the final model.
    inputs:
      transformed_data: ${{parent.jobs.prep_data.outputs.transformed_data}} # Input folder of transformed data
      # --- Pass the DATES defining the window of transformed data to load ---
      train_load_start_date: ${{parent.inputs.prep_output_start_date}} # Use the start date from prep output
      train_load_end_date: ${{parent.inputs.prep_output_end_date}}   # Use the end date from prep output
      # --- Pass the ANCHOR date and DELTAS for splitting ---
      anchor_date_str: ${{parent.inputs.anchor_date_str}}
      delta_train: ${{parent.inputs.delta_train}}
      delta_delay: ${{parent.inputs.delta_delay}}
      delta_assessment: ${{parent.inputs.delta_assessment}}
      n_folds: ${{parent.inputs.n_folds}}
      # Optional: Pass top_k, n_jobs if needed
      # top_k_value: 100 : # --top_k_value ${{inputs.top_k_value}}
      n_jobs: -1
    outputs:
      model_output: ${{parent.outputs.trained_model_output}}
      test_data_output: # Output folder containing the final_test_data.pkl
          type: uri_folder # Output is a folder now
          mode: upload
    code: ../../../data-science/src
    command: >-
      python train.py
      --transformed_data ${{inputs.transformed_data}}
      --model_output ${{outputs.model_output}}
      --test_data_output ${{outputs.test_data_output}}
      # --- Pass correct arguments to train.py ---
      --train_load_start_date ${{inputs.train_load_start_date}}
      --train_load_end_date ${{inputs.train_load_end_date}}
      --anchor_date_str ${{inputs.anchor_date_str}}      # <-- Added this argument
      --delta_train ${{inputs.delta_train}}
      --delta_delay ${{inputs.delta_delay}}
      --delta_assessment ${{inputs.delta_assessment}}
      --n_folds ${{inputs.n_folds}}
      --n_jobs ${{inputs.n_jobs}}
    environment: azureml:fraud-detection-train-env@latest

  # 3. Evaluation Step (Corrected Outputs)
  evaluate_model:
    type: command
    name: evaluate_trained_model
    display_name: Evaluate Model Performance
    description: Evaluates the trained model on the test set and determines deploy flag.
    inputs:
      model_input: ${{parent.jobs.train_model.outputs.model_output}}
      # --- Input the test data file directly ---
      # Reference the file *inside* the output folder from the train step
      test_data: ${{parent.jobs.train_model.outputs.test_data_output}}/final_test_data.pkl
      deploy_threshold_value: ${{parent.inputs.deploy_threshold_value}}
    outputs:
      evaluation_output: # Define the output locally
        type: uri_folder # Specify type (folder containing metrics.csv, deploy_flag)
        mode: upload     # Upload results from compute
        # This output implicitly maps to the pipeline-level output 'evaluation_report_output'
        # if the names match the script's output argument, or explicitly map if needed.
        # For simplicity, let the framework handle the mapping.
    code: ../../../data-science/src
    command: >-
      python evaluate.py
      --model_input ${{inputs.model_input}}
      --test_data ${{inputs.test_data}}                 # Use the direct test data input
      --evaluation_output ${{outputs.evaluation_output}} # Command arg still uses the output name
      --deploy_threshold_value ${{inputs.deploy_threshold_value}}
    environment: azureml:fraud-detection-train-env@latest

  # 4. Registration Step (Input reference is now valid)
  register_model:
    type: command
    name: register_evaluated_model
    display_name: Register Model (if approved)
    description: Registers the model in the Azure ML workspace if deploy flag is set.
    inputs:
      model_path: ${{parent.jobs.train_model.outputs.model_output}}
      evaluation_output: ${{parent.jobs.evaluate_model.outputs.evaluation_output}} # Reference the direct output
      model_name: "fraud-detection-model"
    outputs:
       model_info_output_path: ${{parent.outputs.model_info_output}} # Map to pipeline output
    code: ../../../data-science/src
    command: >-
      python register.py
      --model_name ${{inputs.model_name}}
      --model_path ${{inputs.model_path}}
      --evaluation_output ${{inputs.evaluation_output}} # Command arg uses the input name
      --model_info_output_path ${{outputs.model_info_output_path}}
    environment: azureml:fraud-detection-train-env@latest