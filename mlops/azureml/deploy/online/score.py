# File: mlops/azureml/deploy/online/score.py
import os
import logging
import json
import pandas as pd
import mlflow
from azureml.ai.monitoring import Collector
from azureml.ai.monitoring.context import BasicCorrelationContext

# Define the expected input features for your model (adjust as needed!)
INPUT_FEATURES = [
    'TX_AMOUNT', 'TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',
    'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',
    'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',
    'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',
    'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',
    'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',
    'TERMINAL_ID_RISK_30DAY_WINDOW'
]

# Global variables for the model and collectors
model = None
inputs_collector = None
outputs_collector = None

def init():
    """
    This function is called when the container is initialized/started, typically after create/update of the deployment.
    You can write the logic here to perform init operations like caching the model in memory
    """
    global model, inputs_collector, outputs_collector

    # Define collectors with names. Using 'model_inputs' and 'model_outputs'
    # helps Azure ML Model Monitoring auto-detect them.
    # Adding error handling for robustness during collection
    inputs_collector = Collector(name='model_inputs', on_error=lambda e: logging.error(f"Inputs Collector Error: {e}"))
    outputs_collector = Collector(name='model_outputs', on_error=lambda e: logging.error(f"Outputs Collector Error: {e}"))

    # Get the path to the deployed model folder
    # AZUREML_MODEL_DIR is an environment variable created during deployment. It is the path to the model folder
    # (./azureml-models/$MODEL_NAME/$VERSION)
    model_path = os.path.join(
        os.getenv("AZUREML_MODEL_DIR", default="."), "fraud-detection-model" # Assuming model artifact folder name is 'fraud-detection-model'
    )

    # Load the MLflow model
    try:
        model = mlflow.sklearn.load_model(model_path)
        logging.info("Model loaded successfully")
    except Exception as e:
        logging.error(f"Failed to load model from path: {model_path}. Error: {e}")
        raise

    logging.info("Init complete")

def run(raw_data):
    """
    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.
    """
    global model, inputs_collector, outputs_collector
    logging.info("Request received")

    try:
        # Expecting a JSON string with a single transaction's data as a dictionary
        data_dict = json.loads(raw_data)
        logging.info(f"Input data (dict): {data_dict}")

        # Extract the custom ID for correlation
        transaction_id = data_dict.get('TRANSACTION_ID', None)
        if transaction_id is None:
             logging.warning("TRANSACTION_ID not found in payload. Correlation ID will be auto-generated by collector.")
             # Fallback or raise error depending on requirements
             # For this example, we'll let the collector auto-generate if missing
             custom_context = None
        else:
            # Create the correlation context with the custom ID
             custom_context = BasicCorrelationContext(id=str(transaction_id)) # Ensure it's a string
             logging.info(f"Using custom correlation ID: {transaction_id}")

        # Convert the single dictionary to a DataFrame for the collector and model
        # We wrap the dictionary in a list before creating the DataFrame
        input_df = pd.DataFrame([data_dict])

        # --- Collect Input Data ---
        # The context returned contains info to correlate inputs and outputs
        # Pass the custom context object here
        try:
            context = inputs_collector.collect(input_df, custom_context) # Pass the custom context if available
            logging.info("Input data collected.")
        except Exception as e:
            logging.error(f"Error during input data collection: {e}")
            # Decide if you want to proceed without collection or raise error
            context = None # Ensure context is None if collection fails

        # --- Perform Prediction ---
        # Ensure only the required features are passed to the model
        # Make a copy to avoid SettingWithCopyWarning if input_df is used later
        model_input_df = input_df[INPUT_FEATURES].copy()

        # Check for required columns (optional but good practice)
        missing_cols = [col for col in INPUT_FEATURES if col not in model_input_df.columns]
        if missing_cols:
            raise ValueError(f"Missing required input columns for model: {missing_cols}")

        # Check for NaNs before prediction (optional, depends on how model/pipeline handles them)
        if model_input_df.isnull().values.any():
            logging.warning("NaNs detected in input data before prediction. Ensure model pipeline handles them.")
            # Add imputation logic here if necessary and not handled by the model's pipeline

        prediction = model.predict(model_input_df)
        probabilities = model.predict_proba(model_input_df)
        logging.info("Prediction generated")

        # --- Prepare output DataFrame for collection with 'TX_FRAUD' column ---
        output_df = pd.DataFrame({
            'TX_FRAUD': prediction, # Changed column name here
            'probability_non_fraud': probabilities[:, 0], # Renamed for clarity
            'probability_fraud': probabilities[:, 1] # Assuming class 1 is fraud
        })

        # --- Collect Output Data ---
        if context: # Only collect output if input collection was successful and returned context
            try:
                outputs_collector.collect(output_df, context) # Pass the context from input collection
                logging.info("Output data collected.")
            except Exception as e:
                logging.error(f"Error during output data collection: {e}")
                # Decide if you want to proceed without collection or raise error
        else:
             logging.warning("Skipping output data collection because input collection failed or context is missing.")


        # --- Format Response ---
        # Return prediction and probabilities (adjust format as needed by consuming application)
        # Keep the response format potentially different from the collected data format
        result = {
            "TX_FRAUD": int(prediction[0]), # Using TX_FRAUD in the response as well for consistency
            "probability_fraud": float(probabilities[0, 1]) # Probability of class 1 (fraud)
        }
        logging.info(f"Sending response: {result}")
        return result

    except json.JSONDecodeError as e:
        logging.error(f"Bad JSON format received: {e}")
        return {"error": "Invalid JSON input", "details": str(e)}, 400 # Example error response
    except KeyError as e:
         logging.error(f"Missing expected key in input data: {e}")
         return {"error": f"Missing key: {e}"}, 400
    except ValueError as e:
         logging.error(f"Data validation or processing error: {e}")
         return {"error": f"Data error: {e}"}, 400
    except Exception as e:
        logging.error(f"An error occurred during scoring: {e}")
        import traceback
        traceback.print_exc()
        return {"error": "Prediction failed", "details": str(e)}, 500