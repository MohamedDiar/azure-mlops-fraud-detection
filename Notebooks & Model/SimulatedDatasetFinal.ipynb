{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Transaction_data_Simulator)=\n",
    "# Transaction data simulator\n",
    "\n",
    "\n",
    "This section presents a transaction data simulator of legitimate and fraudulent transactions. This simulator will be used throughout the rest of this book to motivate and assess the efficiency of different fraud detection techniques in a reproducible way. \n",
    "\n",
    "A simulation is necessarily an approximation of reality. Compared to the complexity of the dynamics underlying real-world payment card transaction data, the data simulator that we present below follows a simple design. \n",
    "\n",
    "This simple design is a choice. First, having simple rules to generate transactions and fraudulent behaviors will help in interpreting the kind of patterns that different fraud detection techniques can identify. Second, while simple in its design, the data simulator will generate datasets that are challenging to deal with. \n",
    "\n",
    "The simulated datasets will highlight most of the issues that practitioners of fraud detection face using real-world data. In particular, they will include class imbalance (less than 1% of fraudulent transactions), a mix of numerical and categorical features (with categorical features involving a very large number of values), non-trivial relationships between features, and time-dependent fraud scenarios.\n",
    "\n",
    "<h3>Design choices</h3>\n",
    "\n",
    "<h4>Transaction features</h4>\n",
    "\n",
    "Our focus will be on the most essential features of a transaction. In essence, a payment card transaction consists of any amount paid to a merchant by a customer at a certain time. The six main features that summarise a transaction therefore are:\n",
    "\n",
    "1. The transaction ID: A unique identifier for the transaction\n",
    "2. The date and time: Date and time at which the transaction occurs\n",
    "3. The customer ID: The identifier for the customer. Each customer has a unique identifier\n",
    "4. The terminal ID: The identifier for the merchant (or more precisely the terminal). Each terminal has a unique identifier\n",
    "5. The transaction amount: The amount of the transaction.\n",
    "6. The fraud label: A binary variable, with the value $0$ for a legitimate transaction, or the value $1$ for a fraudulent transaction.\n",
    "\n",
    "These features will be referred to as `TRANSACTION_ID`, `TX_DATETIME`, `CUSTOMER_ID`, `TERMINAL_ID`, `TX_AMOUNT`, and `TX_FRAUD`. \n",
    "\n",
    "The goal of the transaction data simulator will be to generate a table of transactions with these features. This table will be referred to as the *labeled transactions* table. Such a table is illustrated in Fig. 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/tx_table.png)\n",
    "<p style=\"text-align: center;\">\n",
    "Fig. 1. Example of labeled transaction table. Each transaction is represented as a row in the table,<br> together with its label (TX_FRAUD variable, 0 for legitimate, and 1 for fraudulent transactions).\n",
    "</p>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Necessary imports for this notebook\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer profiles generation\n",
    "\n",
    "Each customer will be defined by the following properties:\n",
    "\n",
    "* `CUSTOMER_ID`: The customer unique ID\n",
    "* (`x_customer_id`,`y_customer_id`): A pair of real coordinates (`x_customer_id`,`y_customer_id`) in a 100 * 100 grid, that defines the geographical location of the customer\n",
    "* (`mean_amount`, `std_amount`):  The mean and standard deviation of the transaction amounts for the customer, assuming that the transaction amounts follow a normal distribution. The `mean_amount` will be drawn from a uniform distribution (5,100) and the `std_amount` will be set as the `mean_amount` divided by two. \n",
    "* `mean_nb_tx_per_day`:  The average number of transactions per day for the customer, assuming that the number of transactions per day follows a Poisson distribution. This number will be drawn from a uniform distribution (0,4). \n",
    "\n",
    "The `generate_customer_profiles_table` function provides an implementation for generating a table of customer profiles. It takes as input the number of customers for which to generate a profile and a random state for reproducibility. It returns a DataFrame containing the properties for each customer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Transaction generation process</h4>\n",
    "\n",
    "The simulation will consist of five main steps:\n",
    "\n",
    "1. Generation of customer profiles: Every customer is different in their spending habits. This will be simulated by defining some properties for each customer. The main properties will be their geographical location, their spending frequency, and their spending amounts. The customer properties will be represented as a table, referred to as the *customer profile table*. \n",
    "2. Generation of terminal profiles: Terminal properties will simply consist of a geographical location. The terminal properties will be represented as a table, referred to as the *terminal profile table*.\n",
    "3. Association of customer profiles to terminals: We will assume that customers only make transactions on terminals that are within a radius of $r$ of their geographical locations. This makes the simple assumption that a customer only makes transactions on terminals that are geographically close to their location. This step will consist of adding a feature 'list_terminals' to each customer profile, that contains the set of terminals that a customer can use.\n",
    "4. Generation of transactions: The simulator will loop over the set of customer profiles, and generate transactions according to their properties (spending frequencies and amounts, and available terminals). This will result in a table of transactions.\n",
    "5. Generation of fraud scenarios: This last step will label the transactions as legitimate or genuine. This will be done by following three different fraud scenarios.\n",
    "\n",
    "The transaction generation process is illustrated below. \n",
    "\n",
    "![alt text](images/FlowDatasetGenerator.png)\n",
    "<p style=\"text-align: center;\">\n",
    "Fig. 2. Transaction generation process. The customer and terminal profiles are used to generate  <br> a set of transactions. The final step, which generates fraud scenarios, provides the labeled transactions table.\n",
    "   \n",
    "\n",
    "The following sections detail the implementation for each of these steps. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_customer_profiles_table(n_customers, random_state=0):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    customer_id_properties=[]\n",
    "    \n",
    "    # Generate customer properties from random distributions \n",
    "    for customer_id in range(n_customers):\n",
    "        \n",
    "        x_customer_id = np.random.uniform(0,100)\n",
    "        y_customer_id = np.random.uniform(0,100)\n",
    "        \n",
    "        mean_amount = np.random.uniform(5,100) # Arbitrary (but sensible) value \n",
    "        std_amount = mean_amount/2 # Arbitrary (but sensible) value\n",
    "        \n",
    "        mean_nb_tx_per_day = np.random.uniform(0,4) # Arbitrary (but sensible) value \n",
    "        \n",
    "        customer_id_properties.append([customer_id,\n",
    "                                      x_customer_id, y_customer_id,\n",
    "                                      mean_amount, std_amount,\n",
    "                                      mean_nb_tx_per_day])\n",
    "        \n",
    "    customer_profiles_table = pd.DataFrame(customer_id_properties, columns=['CUSTOMER_ID',\n",
    "                                                                      'x_customer_id', 'y_customer_id',\n",
    "                                                                      'mean_amount', 'std_amount',\n",
    "                                                                      'mean_nb_tx_per_day'])\n",
    "    \n",
    "    return customer_profiles_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let us generate a customer profile table for five customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_customers = 5\n",
    "customer_profiles_table = generate_customer_profiles_table(n_customers, random_state = 0)\n",
    "customer_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal profiles generation\n",
    "\n",
    "Each terminal will be defined by the following properties:\n",
    "\n",
    "* `TERMINAL_ID`: The terminal ID\n",
    "* (`x_terminal_id`,`y_terminal_id`): A pair of real coordinates (`x_terminal_id`,`y_terminal_id`) in a 100 * 100 grid, that defines the geographical location of the terminal\n",
    "\n",
    "The `generate_terminal_profiles_table` function provides an implementation for generating a table of terminal profiles. It takes as input the number of terminals for which to generate a profile and a random state for reproducibility. It returns a DataFrame containing the properties for each terminal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_terminal_profiles_table(n_terminals, random_state=0):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    terminal_id_properties=[]\n",
    "    \n",
    "    # Generate terminal properties from random distributions \n",
    "    for terminal_id in range(n_terminals):\n",
    "        \n",
    "        x_terminal_id = np.random.uniform(0,100)\n",
    "        y_terminal_id = np.random.uniform(0,100)\n",
    "        \n",
    "        terminal_id_properties.append([terminal_id,\n",
    "                                      x_terminal_id, y_terminal_id])\n",
    "                                       \n",
    "    terminal_profiles_table = pd.DataFrame(terminal_id_properties, columns=['TERMINAL_ID',\n",
    "                                                                      'x_terminal_id', 'y_terminal_id'])\n",
    "    \n",
    "    return terminal_profiles_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let us generate a customer terminal table for five terminals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_terminals = 5\n",
    "terminal_profiles_table = generate_terminal_profiles_table(n_terminals, random_state = 0)\n",
    "terminal_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association of customer profiles to terminals\n",
    "\n",
    "Let us now associate terminals with the customer profiles. In our design, customers can only perform transactions on terminals that are within a radius of `r` of their geographical locations. \n",
    "\n",
    "Let us first write a function, called `get_list_terminals_within_radius`, which finds these terminals for a customer profile. The function will take as input a customer profile (any row in the customer profiles table), an array that contains the geographical location of all terminals, and the radius `r`. It will return the list of terminals within a radius of `r` for that customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_list_terminals_within_radius(customer_profile, x_y_terminals, r):\n",
    "    \n",
    "    # Use numpy arrays in the following to speed up computations\n",
    "    \n",
    "    # Location (x,y) of customer as numpy array\n",
    "    x_y_customer = customer_profile[['x_customer_id','y_customer_id']].values.astype(float)\n",
    "    \n",
    "    # Squared difference in coordinates between customer and terminal locations\n",
    "    squared_diff_x_y = np.square(x_y_customer - x_y_terminals)\n",
    "    \n",
    "    # Sum along rows and compute suared root to get distance\n",
    "    dist_x_y = np.sqrt(np.sum(squared_diff_x_y, axis=1))\n",
    "    \n",
    "    # Get the indices of terminals which are at a distance less than r\n",
    "    available_terminals = list(np.where(dist_x_y<r)[0])\n",
    "    \n",
    "    # Return the list of terminal IDs\n",
    "    return available_terminals\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let us get the list of terminals that are within a radius $r=50$ of the last customer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We first get the geographical locations of all terminals as a numpy array\n",
    "x_y_terminals = terminal_profiles_table[['x_terminal_id','y_terminal_id']].values.astype(float)\n",
    "# And get the list of terminals within radius of $50$ for the last customer\n",
    "get_list_terminals_within_radius(customer_profiles_table.iloc[4], x_y_terminals=x_y_terminals, r=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list contains the third and fourth terminals, which are indeed the only ones within a radius of $50$ of the last customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better visualization, let us plot \n",
    "\n",
    "* The locations of all terminals (in red)\n",
    "* The location of the last customer (in blue)\n",
    "* The region within radius of 50 of the first customer (in green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "terminals_available_to_customer_fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Plot locations of terminals\n",
    "ax.scatter(terminal_profiles_table.x_terminal_id.values, \n",
    "           terminal_profiles_table.y_terminal_id.values, \n",
    "           color='blue', label = 'Locations of terminals')\n",
    "\n",
    "# Plot location of the last customer\n",
    "customer_id=4\n",
    "ax.scatter(customer_profiles_table.iloc[customer_id].x_customer_id, \n",
    "           customer_profiles_table.iloc[customer_id].y_customer_id, \n",
    "           color='red',label=\"Location of last customer\")\n",
    "\n",
    "ax.legend(loc = 'upper left', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "# Plot the region within a radius of 50 of the last customer\n",
    "circ = plt.Circle((customer_profiles_table.iloc[customer_id].x_customer_id,\n",
    "                   customer_profiles_table.iloc[customer_id].y_customer_id), radius=50, color='g', alpha=0.2)\n",
    "ax.add_patch(circ)\n",
    "\n",
    "fontsize=15\n",
    "\n",
    "ax.set_title(\"Green circle: \\n Terminals within a radius of 50 \\n of the last customer\")\n",
    "ax.set_xlim([0, 100])\n",
    "ax.set_ylim([0, 100])\n",
    "    \n",
    "ax.set_xlabel('x_terminal_id', fontsize=fontsize)\n",
    "ax.set_ylabel('y_terminal_id', fontsize=fontsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminals_available_to_customer_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the list of available terminals for each customer is then straightforward, using the panda `apply` function. We store the results as a new column `available_terminals` in the customer profiles table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_profiles_table['available_terminals']=customer_profiles_table.apply(lambda x : get_list_terminals_within_radius(x, x_y_terminals=x_y_terminals, r=50), axis=1)\n",
    "customer_profiles_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the radius $r$ controls the number of terminals that will be on average available for each customer. As the number of terminals is increased, this radius should be adapted to match the average number of available terminals per customer that is desired in a simulation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of transactions\n",
    "\n",
    "The customer profiles now contain all the information that we require to generate transactions. The transaction generation will be done by a function `generate_transactions_table` that takes as input a customer profile, a starting date, and a number of days for which to generate transactions. It will return a table of transactions, which follows the format presented above (without the transaction label, which will be added in [fraud scenarios generation](Fraud_Scenarios_Generation)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_transactions_table(customer_profile, start_date = \"2025-04-01\", nb_days = 10):\n",
    "    \n",
    "    customer_transactions = []\n",
    "    \n",
    "    random.seed(int(customer_profile.CUSTOMER_ID))\n",
    "    np.random.seed(int(customer_profile.CUSTOMER_ID))\n",
    "    \n",
    "    # For all days\n",
    "    for day in range(nb_days):\n",
    "        \n",
    "        # Random number of transactions for that day \n",
    "        nb_tx = np.random.poisson(customer_profile.mean_nb_tx_per_day)\n",
    "        \n",
    "        # If nb_tx positive, let us generate transactions\n",
    "        if nb_tx>0:\n",
    "            \n",
    "            for tx in range(nb_tx):\n",
    "                \n",
    "                # Time of transaction: Around noon, std 20000 seconds. This choice aims at simulating the fact that \n",
    "                # most transactions occur during the day.\n",
    "                time_tx = int(np.random.normal(86400/2, 20000))\n",
    "                \n",
    "                # If transaction time between 0 and 86400, let us keep it, otherwise, let us discard it\n",
    "                if (time_tx>0) and (time_tx<86400):\n",
    "                    \n",
    "                    # Amount is drawn from a normal distribution  \n",
    "                    amount = np.random.normal(customer_profile.mean_amount, customer_profile.std_amount)\n",
    "                    \n",
    "                    # If amount negative, draw from a uniform distribution\n",
    "                    if amount<0:\n",
    "                        amount = np.random.uniform(0,customer_profile.mean_amount*2)\n",
    "                    \n",
    "                    amount=np.round(amount,decimals=2)\n",
    "                    \n",
    "                    if len(customer_profile.available_terminals)>0:\n",
    "                        \n",
    "                        terminal_id = random.choice(customer_profile.available_terminals)\n",
    "                    \n",
    "                        customer_transactions.append([time_tx+day*86400, day,\n",
    "                                                      customer_profile.CUSTOMER_ID, \n",
    "                                                      terminal_id, amount])\n",
    "            \n",
    "    customer_transactions = pd.DataFrame(customer_transactions, columns=['TX_TIME_SECONDS', 'TX_TIME_DAYS', 'CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT'])\n",
    "    \n",
    "    if len(customer_transactions)>0:\n",
    "        customer_transactions['TX_DATETIME'] = pd.to_datetime(customer_transactions[\"TX_TIME_SECONDS\"], unit='s', origin=start_date)\n",
    "        customer_transactions=customer_transactions[['TX_DATETIME','CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT','TX_TIME_SECONDS', 'TX_TIME_DAYS']]\n",
    "    \n",
    "    return customer_transactions  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us for example generate transactions for the first customer, for five days, starting at the date 2025-04-01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transaction_table_customer_0=generate_transactions_table(customer_profiles_table.iloc[0], \n",
    "                                                         start_date = \"2025-04-01\", \n",
    "                                                         nb_days = 5)\n",
    "transaction_table_customer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a quick check that the generated transactions follow the customer profile properties:\n",
    "\n",
    "* The terminal IDs are indeed those in the list of available terminals (0, 1, 2 and 3)\n",
    "* The transaction amounts seem to follow the amount parameters of the customer (`mean_amount`=62.26 and\t`std_amount`=31.13)\n",
    "* The number of transactions per day varies according to the transaction frequency parameters of the customer (`mean_nb_tx_per_day`=2.18).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now generate the transactions for all customers. This is straightforward using the pandas `groupby` and `apply` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df=customer_profiles_table.groupby('CUSTOMER_ID').apply(lambda x : generate_transactions_table(x.iloc[0], nb_days=5)).reset_index(drop=True)\n",
    "transactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a set of 65 transactions, with 5 customers, 5 terminals, and 5 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Scaling up to a larger dataset</h3>\n",
    "\n",
    "We now have all the building blocks to generate a larger dataset. Let us write a `generate_dataset` function, that will take care of running all the previous steps. It will \n",
    "\n",
    "* take as inputs the number of desired customers, terminals and days, as well as the starting date and the radius `r`\n",
    "* return the generated customer and terminal profiles table, and the DataFrame of transactions.\n",
    "\n",
    "```{note}\n",
    "In order to speed up the computations, one can use the `parallel_apply` function of the `pandarallel` module. This function replaces the panda `apply` function, and allows the distribution of the computation on all the available CPUs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_dataset(n_customers = 10000, n_terminals = 1000000, nb_days=90, start_date=\"2025-04-01\", r=5):\n",
    "    \n",
    "    start_time=time.time()\n",
    "    customer_profiles_table = generate_customer_profiles_table(n_customers, random_state = 0)\n",
    "    print(\"Time to generate customer profiles table: {0:.2}s\".format(time.time()-start_time))\n",
    "    \n",
    "    start_time=time.time()\n",
    "    terminal_profiles_table = generate_terminal_profiles_table(n_terminals, random_state = 1)\n",
    "    print(\"Time to generate terminal profiles table: {0:.2}s\".format(time.time()-start_time))\n",
    "    \n",
    "    start_time=time.time()\n",
    "    x_y_terminals = terminal_profiles_table[['x_terminal_id','y_terminal_id']].values.astype(float)\n",
    "    customer_profiles_table['available_terminals'] = customer_profiles_table.apply(lambda x : get_list_terminals_within_radius(x, x_y_terminals=x_y_terminals, r=r), axis=1)\n",
    "    # With Pandarallel\n",
    "    #customer_profiles_table['available_terminals'] = customer_profiles_table.parallel_apply(lambda x : get_list_closest_terminals(x, x_y_terminals=x_y_terminals, r=r), axis=1)\n",
    "    customer_profiles_table['nb_terminals']=customer_profiles_table.available_terminals.apply(len)\n",
    "    print(\"Time to associate terminals to customers: {0:.2}s\".format(time.time()-start_time))\n",
    "    \n",
    "    start_time=time.time()\n",
    "    transactions_df=customer_profiles_table.groupby('CUSTOMER_ID').apply(lambda x : generate_transactions_table(x.iloc[0], nb_days=nb_days)).reset_index(drop=True)\n",
    "    # With Pandarallel\n",
    "    #transactions_df=customer_profiles_table.groupby('CUSTOMER_ID').parallel_apply(lambda x : generate_transactions_table(x.iloc[0], nb_days=nb_days)).reset_index(drop=True)\n",
    "    print(\"Time to generate transactions: {0:.2}s\".format(time.time()-start_time))\n",
    "    \n",
    "    # Sort transactions chronologically\n",
    "    transactions_df=transactions_df.sort_values('TX_DATETIME')\n",
    "    # Reset indices, starting from 0\n",
    "    transactions_df.reset_index(inplace=True,drop=True)\n",
    "    transactions_df.reset_index(inplace=True)\n",
    "    # TRANSACTION_ID are the dataframe indices, starting from 0\n",
    "    transactions_df.rename(columns = {'index':'TRANSACTION_ID'}, inplace = True)\n",
    "    \n",
    "    return (customer_profiles_table, terminal_profiles_table, transactions_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate a dataset that features \n",
    "\n",
    "* 5000 customers\n",
    "* 10000 terminals\n",
    "* 183 days of transactions (which corresponds to a simulated period from 2025/04/01 to 2025/09/30)\n",
    "\n",
    "The starting date is arbitrarily fixed at 2025/04/01. The radius $r$ is set to 5, which corresponds to around 100 available terminals for each customer.\n",
    "\n",
    "It takes around 3 minutes to generate this dataset on a standard laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(customer_profiles_table, terminal_profiles_table, transactions_df)=\\\n",
    "    generate_dataset(n_customers = 5000, \n",
    "                     n_terminals = 10000, \n",
    "                     nb_days=270, \n",
    "                     start_date=\"2025-04-01\", \n",
    "                     r=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 2588146 transactions were generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this number is low compared to real-world fraud detection systems, where millions of transactions may need to be processed every day. This will however be enough for the purpose of this book, in particular to keep reasonable executions times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let us plot the distribution of transaction amounts and transaction times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "distribution_amount_times_fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = transactions_df[transactions_df.TX_TIME_DAYS<10]['TX_AMOUNT'].sample(n=10000).values\n",
    "time_val = transactions_df[transactions_df.TX_TIME_DAYS<10]['TX_TIME_SECONDS'].sample(n=10000).values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r', hist = True, kde = False)\n",
    "ax[0].set_title('Distribution of transaction amounts', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "ax[0].set(xlabel = \"Amount\", ylabel=\"Number of transactions\")\n",
    "\n",
    "# We divide the time variables by 86400 to transform seconds to days in the plot\n",
    "sns.distplot(time_val/86400, ax=ax[1], color='b', bins = 100, hist = True, kde = False)\n",
    "ax[1].set_title('Distribution of transaction times', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val/86400), max(time_val/86400)])\n",
    "ax[1].set_xticks(range(10))\n",
    "ax[1].set(xlabel = \"Time (days)\", ylabel=\"Number of transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution_amount_times_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of transaction amounts has most of its mass for small amounts. The distribution of transaction times  follows a gaussian distribution on a daily basis, centered around noon. These two distributions are in accordance with the simulation parameters used in the previous sections.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Fraud_Scenarios_Generation)=\n",
    "## Fraud scenarios generation\n",
    "\n",
    "This last step of the simulation adds fraudulent transactions to the dataset, using the following fraud scenarios:\n",
    "\n",
    "* Scenario 1: Any transaction whose amount is more than 220 is a fraud. This scenario is not inspired by a real-world scenario. Rather, it will provide an obvious fraud pattern that should be detected by any baseline fraud detector. This will be useful to validate the implementation of a fraud detection technique.  \n",
    "\n",
    "* Scenario 2: Every day, a list of two terminals is drawn at random. All transactions on these terminals in the next 28 days will be marked as fraudulent. This scenario simulates a criminal use of a terminal, through phishing for example. Detecting this scenario will be possible by adding features that keep track of the number of fraudulent transactions on the terminal. Since the terminal is only compromised for 28 days, additional strategies that involve concept drift will need to be designed to efficiently deal with this scenario.     \n",
    "\n",
    "* Scenario 3: Every day, a list of 3 customers is drawn at random. In the next 14 days, 1/3 of their transactions have their amounts multiplied by 5 and marked as fraudulent. This scenario simulates a card-not-present fraud where the credentials of a customer have been leaked. The customer continues to make transactions, and transactions of higher values are made by the fraudster who tries to maximize their gains. Detecting this scenario will require adding features that keep track of the spending habits of the customer. As for scenario 2, since the card is only temporarily compromised, additional strategies that involve concept drift should also be designed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def add_frauds(customer_profiles_table, terminal_profiles_table, transactions_df):\n",
    "    \n",
    "    # By default, all transactions are genuine\n",
    "    transactions_df['TX_FRAUD']=0\n",
    "    transactions_df['TX_FRAUD_SCENARIO']=0\n",
    "    \n",
    "    # Scenario 1\n",
    "    transactions_df.loc[transactions_df.TX_AMOUNT>220, 'TX_FRAUD']=1\n",
    "    transactions_df.loc[transactions_df.TX_AMOUNT>220, 'TX_FRAUD_SCENARIO']=1\n",
    "    nb_frauds_scenario_1=transactions_df.TX_FRAUD.sum()\n",
    "    print(\"Number of frauds from scenario 1: \"+str(nb_frauds_scenario_1))\n",
    "    \n",
    "    # Scenario 2\n",
    "    for day in range(transactions_df.TX_TIME_DAYS.max()):\n",
    "        \n",
    "        compromised_terminals = terminal_profiles_table.TERMINAL_ID.sample(n=2, random_state=day)\n",
    "        \n",
    "        compromised_transactions=transactions_df[(transactions_df.TX_TIME_DAYS>=day) & \n",
    "                                                    (transactions_df.TX_TIME_DAYS<day+28) & \n",
    "                                                    (transactions_df.TERMINAL_ID.isin(compromised_terminals))]\n",
    "                            \n",
    "        transactions_df.loc[compromised_transactions.index,'TX_FRAUD']=1\n",
    "        transactions_df.loc[compromised_transactions.index,'TX_FRAUD_SCENARIO']=2\n",
    "    \n",
    "    nb_frauds_scenario_2=transactions_df.TX_FRAUD.sum()-nb_frauds_scenario_1\n",
    "    print(\"Number of frauds from scenario 2: \"+str(nb_frauds_scenario_2))\n",
    "    \n",
    "    # Scenario 3\n",
    "    for day in range(transactions_df.TX_TIME_DAYS.max()):\n",
    "        \n",
    "        compromised_customers = customer_profiles_table.CUSTOMER_ID.sample(n=3, random_state=day).values\n",
    "        \n",
    "        compromised_transactions=transactions_df[(transactions_df.TX_TIME_DAYS>=day) & \n",
    "                                                    (transactions_df.TX_TIME_DAYS<day+14) & \n",
    "                                                    (transactions_df.CUSTOMER_ID.isin(compromised_customers))]\n",
    "        \n",
    "        nb_compromised_transactions=len(compromised_transactions)\n",
    "        \n",
    "        \n",
    "        random.seed(day)\n",
    "        index_fauds = random.sample(list(compromised_transactions.index.values),k=int(nb_compromised_transactions/3))\n",
    "        \n",
    "        transactions_df.loc[index_fauds,'TX_AMOUNT']=transactions_df.loc[index_fauds,'TX_AMOUNT']*5\n",
    "        transactions_df.loc[index_fauds,'TX_FRAUD']=1\n",
    "        transactions_df.loc[index_fauds,'TX_FRAUD_SCENARIO']=3\n",
    "        \n",
    "                             \n",
    "    nb_frauds_scenario_3=transactions_df.TX_FRAUD.sum()-nb_frauds_scenario_2-nb_frauds_scenario_1\n",
    "    print(\"Number of frauds from scenario 3: \"+str(nb_frauds_scenario_3))\n",
    "    \n",
    "    return transactions_df                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add fraudulent transactions using these scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time transactions_df = add_frauds(customer_profiles_table, terminal_profiles_table, transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of fraudulent transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df.TX_FRAUD.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of fraudulent transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df.TX_FRAUD.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 14681 transactions were marked as fraudulent. This amounts to 0.8% of the transactions. Note that the sum of the frauds for each scenario does not equal the total amount of fraudulent transactions. This is because the same transactions may have been marked as fraudulent by two or more fraud scenarios.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simulated transaction dataset is now complete, with a fraudulent label added to all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_FRAUD_SCENARIO==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_FRAUD_SCENARIO==2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_FRAUD_SCENARIO==3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check how the number of transactions, the number of fraudulent transactions, and the number of compromised cards vary on a daily basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_stats(transactions_df):\n",
    "    #Number of transactions per day\n",
    "    nb_tx_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['CUSTOMER_ID'].count()\n",
    "    #Number of fraudulent transactions per day\n",
    "    nb_fraud_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['TX_FRAUD'].sum()\n",
    "    #Number of fraudulent cards per day\n",
    "    nb_fraudcard_per_day=transactions_df[transactions_df['TX_FRAUD']>0].groupby(['TX_TIME_DAYS']).CUSTOMER_ID.nunique()\n",
    "    \n",
    "    return (nb_tx_per_day,nb_fraud_per_day,nb_fraudcard_per_day)\n",
    "\n",
    "(nb_tx_per_day,nb_fraud_per_day,nb_fraudcard_per_day)=get_stats(transactions_df)\n",
    "\n",
    "n_days=len(nb_tx_per_day)\n",
    "tx_stats=pd.DataFrame({\"value\":pd.concat([nb_tx_per_day/50,nb_fraud_per_day,nb_fraudcard_per_day])})\n",
    "tx_stats['stat_type']=[\"nb_tx_per_day\"]*n_days+[\"nb_fraud_per_day\"]*n_days+[\"nb_fraudcard_per_day\"]*n_days\n",
    "tx_stats=tx_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.4)\n",
    "\n",
    "fraud_and_transactions_stats_fig = plt.gcf()\n",
    "\n",
    "fraud_and_transactions_stats_fig.set_size_inches(15, 8)\n",
    "\n",
    "sns_plot = sns.lineplot(x=\"TX_TIME_DAYS\", y=\"value\", data=tx_stats, hue=\"stat_type\", hue_order=[\"nb_tx_per_day\",\"nb_fraud_per_day\",\"nb_fraudcard_per_day\"], legend=False)\n",
    "\n",
    "sns_plot.set_title('Total transactions, and number of fraudulent transactions \\n and number of compromised cards per day', fontsize=20)\n",
    "sns_plot.set(xlabel = \"Number of days since beginning of data generation\", ylabel=\"Number\")\n",
    "\n",
    "sns_plot.set_ylim([0,300])\n",
    "\n",
    "labels_legend = [\"# transactions per day (/50)\", \"# fraudulent txs per day\", \"# fraudulent cards per day\"]\n",
    "\n",
    "sns_plot.legend(loc='upper left', labels=labels_legend,bbox_to_anchor=(1.05, 1), fontsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_and_transactions_stats_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simulation generated around 10000 transactions per day. The number of fraudulent transactions per day is around 85, and the number of fraudulent cards around 80. It is worth noting that the first month has a lower number of fraudulent transactions, which is due to the fact that frauds from scenarios 2 and 3 span periods of 28 and 14 days, respectively. \n",
    "\n",
    "The resulting dataset is interesting: It features class imbalance (less than 1% of fraudulent transactions), a mix of numerical and categorical features, non-trivial relationships between features, and time-dependent fraud scenarios.\n",
    "\n",
    "Let us finally save the dataset for reuse in the rest of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Saving of dataset\n",
    "\n",
    "Instead of saving the whole transaction dataset, we split the dataset into daily batches. This will allow later the loading of specific periods instead of the whole dataset. The pickle format is used, rather than CSV, to speed up the loading times. All files are saved in the `DIR_OUTPUT` folder. The names of the files are the dates, with the `.pkl` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Connect to Workspace and Datastore\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Azure ML Core SDK (v1) for connection and datastore object\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "# --- Prerequisites ---\n",
    "# 1. Ensure 'azureml-core' is installed: pip install azureml-core\n",
    "# 2. Ensure you can connect to your workspace (e.g., run in Azure ML, have config.json, etc.)\n",
    "\n",
    "# --- Azure ML Connection ---\n",
    "try:\n",
    "    # Load the workspace from the current environment or config file\n",
    "    ws = Workspace.from_config() # Assumes config.json or Azure ML environment\n",
    "    print(f\"Connected to Azure ML workspace: {ws.name}\")\n",
    "\n",
    "    # Get the default datastore (SDK v1 object)\n",
    "    datastore = ws.get_default_datastore()\n",
    "    print(f\"Using default datastore: {datastore.name} (Type: {datastore.datastore_type})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Azure ML Workspace or getting datastore: {e}\")\n",
    "    print(\"Please ensure you are running in an Azure ML environment or have a valid config.json.\")\n",
    "    raise SystemExit(\"Failed to connect to Azure ML Workspace.\")\n",
    "\n",
    "print(\"\\nWorkspace and Datastore objects are ready for the next step.\")\n",
    "# Make sure 'transactions_df' DataFrame is loaded before running the next cell.\n",
    "# Example: transactions_df = pd.read_csv(...) or generated from previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Define the target directory *within* the Azure Blob Storage container\n",
    "# associated with the default datastore.\n",
    "TARGET_BLOB_DIR = \"simulated-data-raw\"\n",
    "\n",
    "start_date = datetime.datetime.strptime(\"2025-04-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# --- Processing and Uploading Loop ---\n",
    "print(f\"\\nStarting data processing and upload to datastore '{datastore.name}'...\")\n",
    "\n",
    "# Use a temporary directory to stage files locally before uploading\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(f\"Using temporary directory for staging: {temp_dir}\")\n",
    "\n",
    "    max_days = transactions_df.TX_TIME_DAYS.max()\n",
    "    for day in range(max_days + 1):\n",
    "        print(f\"\\nProcessing Day {day}/{max_days}...\")\n",
    "\n",
    "        # Filter data for the current day and sort\n",
    "        transactions_day = transactions_df[transactions_df.TX_TIME_DAYS == day].sort_values('TX_TIME_SECONDS')\n",
    "\n",
    "        if transactions_day.empty:\n",
    "            print(f\"  No transactions found for day {day}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate date and create filename\n",
    "        date = start_date + datetime.timedelta(days=day)\n",
    "        filename_output = date.strftime(\"%Y-%m-%d\") + '.pkl'\n",
    "\n",
    "        # --- Save to Temporary Local File ---\n",
    "        temp_local_filepath = os.path.join(temp_dir, filename_output)\n",
    "        try:\n",
    "            # Protocol=4 is often needed for compatibility (e.g., Python versions)\n",
    "            transactions_day.to_pickle(temp_local_filepath, protocol=4)\n",
    "            print(f\"  Saved data locally to temporary file: {temp_local_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving day {day} to temporary pickle file: {e}\")\n",
    "            continue # Skip to the next day if saving fails\n",
    "\n",
    "        # --- Upload to Azure ML Datastore ---\n",
    "        try:\n",
    "            print(f\"  Uploading '{filename_output}' to datastore path: '{TARGET_BLOB_DIR}/'\")\n",
    "            # datastore.upload_files expects a list of files to upload\n",
    "            # target_path is the destination *directory* within the datastore\n",
    "            datastore.upload_files(\n",
    "                files=[temp_local_filepath],    # List containing the path to the temp file\n",
    "                target_path=TARGET_BLOB_DIR,    # Target directory in the blob container\n",
    "                overwrite=True,                 # Overwrite if file already exists in blob\n",
    "                show_progress=False             # Set to True for verbose progress bars\n",
    "            )\n",
    "            # The final path in the blob store will be: <container_root>/TARGET_BLOB_DIR/filename_output\n",
    "            print(f\"  Successfully uploaded to {datastore.name}/{TARGET_BLOB_DIR}/{filename_output}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error uploading file {filename_output} to datastore: {e}\")\n",
    "            # Decide how to handle upload errors (e.g., retry, log, raise)\n",
    "\n",
    "        # The temporary file (temp_local_filepath) is automatically removed\n",
    "        # when the 'with tempfile.TemporaryDirectory()' block exits.\n",
    "\n",
    "print(\"\\nFinished processing and uploading all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Assuming 'transactions_df' holds the full generated dataframe from previous cells\n",
    "# Assuming 'datastore' is an Azure ML SDK v1 Datastore object already connected\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the date range for data to be saved in the primary target directory\n",
    "# Data WITHIN this range (inclusive) goes to TARGET_BLOB_DIR_PRIMARY\n",
    "# Data OUTSIDE this range goes to TARGET_BLOB_DIR_SIMULATION\n",
    "# Use 'YYYY-MM-DD' format\n",
    "SPLIT_START_DATE_STR = \"2025-04-01\" # Example: Start date for primary data\n",
    "SPLIT_END_DATE_STR   = \"2025-08-14\" # Example: End date for primary data\n",
    "\n",
    "# Define the target directory names *within* the Azure Blob Storage container\n",
    "TARGET_BLOB_DIR_PRIMARY    = \"fraud-data-raw\"           # Primary location (e.g., for versioning later)\n",
    "TARGET_BLOB_DIR_SIMULATION = \"for_simulation_raw\"       # Location for data outside the range\n",
    "\n",
    "# Define the overall start date of the simulation (must match generation logic)\n",
    "simulation_start_date_obj = datetime.datetime.strptime(f\"{SPLIT_START_DATE_STR}\", \"%Y-%m-%d\")\n",
    "\n",
    "# Convert split dates to datetime objects for comparison\n",
    "try:\n",
    "    split_start_date = datetime.datetime.strptime(SPLIT_START_DATE_STR, \"%Y-%m-%d\")\n",
    "    split_end_date = datetime.datetime.strptime(SPLIT_END_DATE_STR, \"%Y-%m-%d\")\n",
    "    print(f\"Data between {SPLIT_START_DATE_STR} and {SPLIT_END_DATE_STR} will go to '{TARGET_BLOB_DIR_PRIMARY}'.\")\n",
    "    print(f\"Data outside this range will go to '{TARGET_BLOB_DIR_SIMULATION}'.\")\n",
    "except ValueError:\n",
    "    print(\"Error: Invalid date format in SPLIT_START_DATE_STR or SPLIT_END_DATE_STR. Use YYYY-MM-DD.\")\n",
    "    raise SystemExit(\"Invalid date configuration.\")\n",
    "\n",
    "# --- Processing and Uploading Loop ---\n",
    "print(f\"\\nStarting data processing and upload, splitting by date range to datastore '{datastore.name}'...\")\n",
    "\n",
    "# Use a temporary directory to stage files locally before uploading\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(f\"Using temporary directory for staging: {temp_dir}\")\n",
    "\n",
    "    max_days = transactions_df.TX_TIME_DAYS.max()\n",
    "    for day in range(max_days + 1):\n",
    "        # Calculate the actual date for the current day in the loop\n",
    "        current_day_date = simulation_start_date_obj + datetime.timedelta(days=day)\n",
    "        print(f\"\\nProcessing Day {day}/{max_days} ({current_day_date.strftime('%Y-%m-%d')})...\")\n",
    "\n",
    "        # Filter data for the current day and sort\n",
    "        transactions_day = transactions_df[transactions_df.TX_TIME_DAYS == day].sort_values('TX_TIME_SECONDS')\n",
    "\n",
    "        if transactions_day.empty:\n",
    "            print(f\"  No transactions found for day {day}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Create filename\n",
    "        filename_output = current_day_date.strftime(\"%Y-%m-%d\") + '.pkl'\n",
    "\n",
    "        # --- Save to Temporary Local File ---\n",
    "        temp_local_filepath = os.path.join(temp_dir, filename_output)\n",
    "        try:\n",
    "            # Protocol=4 is often needed for compatibility (e.g., Python versions)\n",
    "            transactions_day.to_pickle(temp_local_filepath, protocol=4)\n",
    "            # print(f\"  Saved data locally to temporary file: {temp_local_filepath}\") # Optional: uncomment for verbose logging\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving day {day} to temporary pickle file: {e}\")\n",
    "            continue # Skip to the next day if saving fails\n",
    "\n",
    "        # --- Determine Target Blob Directory based on date ---\n",
    "        if split_start_date <= current_day_date <= split_end_date:\n",
    "            upload_target_path = TARGET_BLOB_DIR_PRIMARY\n",
    "            print(f\"  -> Date in range. Uploading to: '{upload_target_path}/'\")\n",
    "        else:\n",
    "            upload_target_path = TARGET_BLOB_DIR_SIMULATION\n",
    "            print(f\"  -> Date outside range. Uploading to: '{upload_target_path}/'\")\n",
    "\n",
    "        # --- Upload to Azure ML Datastore (using the determined path) ---\n",
    "        try:\n",
    "            # datastore.upload_files expects a list of files to upload\n",
    "            # target_path is the destination *directory* within the datastore\n",
    "            datastore.upload_files(\n",
    "                files=[temp_local_filepath],    # List containing the path to the temp file\n",
    "                target_path=upload_target_path, # Use the dynamically determined path here\n",
    "                overwrite=True,                 # Overwrite if file already exists in blob\n",
    "                show_progress=False             # Set to True for verbose progress bars\n",
    "            )\n",
    "            # The final path in the blob store will be: <container_root>/upload_target_path/filename_output\n",
    "            print(f\"  Successfully uploaded to {datastore.name}/{upload_target_path}/{filename_output}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error uploading file {filename_output} to datastore path '{upload_target_path}': {e}\")\n",
    "            # Decide how to handle upload errors (e.g., retry, log, raise)\n",
    "\n",
    "        # The temporary file (temp_local_filepath) is automatically removed\n",
    "        # when the 'with tempfile.TemporaryDirectory()' block exits.\n",
    "\n",
    "print(f\"\\nFinished processing and uploading all files.\")\n",
    "print(f\"Files within {SPLIT_START_DATE_STR} - {SPLIT_END_DATE_STR} are in '{TARGET_BLOB_DIR_PRIMARY}'.\")\n",
    "print(f\"Files outside that range are in '{TARGET_BLOB_DIR_SIMULATION}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Register the Primary Data Folder as a Versioned Dataset (SDK v1) - CORRECTED\n",
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.exceptions import UserErrorException # More specific exception handling\n",
    "\n",
    "# Define the name for the Azure ML Dataset asset (consistent name for versioning)\n",
    "DATASET_NAME = \"transformed_fraud_data\"\n",
    "# Define the path *within* the datastore that holds the data to be versioned\n",
    "VERSIONED_DATA_PATH = \"fraud-data-transformed\" # This is the folder path on the datastore\n",
    "\n",
    "print(f\"\\nRegistering Azure ML Dataset asset '{DATASET_NAME}' using SDK v1...\")\n",
    "\n",
    "# --- Check for prerequisite objects ---\n",
    "if 'ws' not in locals():\n",
    "     raise NameError(\"Workspace object 'ws' not found. Ensure Cell 1 executed successfully.\")\n",
    "if 'datastore' not in locals():\n",
    "     raise NameError(\"Datastore object 'datastore' not found. Ensure Cell 1 executed successfully.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    file_dataset = Dataset.File.from_files(path=(datastore, VERSIONED_DATA_PATH))\n",
    "\n",
    "    # 2. Register this definition in the workspace.\n",
    "    # Call .register() on the FileDataset object created above.\n",
    "    # Setting create_new_version=True ensures versioning.\n",
    "    registered_dataset = file_dataset.register(\n",
    "        workspace=ws,\n",
    "        name=DATASET_NAME,\n",
    "        description=f\"Transformed daily fraud transaction data (.pkl files) stored in the '{VERSIONED_DATA_PATH}' folder.\",\n",
    "        create_new_version=True, # Explicitly create a new version each time this runs\n",
    "        tags={'source': 'sdk_v1_registration'} # Example of adding tags\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSuccessfully registered/updated dataset asset:\")\n",
    "    print(f\"  Name: {registered_dataset.name}\")\n",
    "    print(f\"  Version: {registered_dataset.version}\")\n",
    "    print(f\"  ID: {registered_dataset.id}\") # Dataset ID in Azure ML\n",
    "\n",
    "except UserErrorException as ue:\n",
    "    # Catch specific Azure ML errors, like dataset not found if create_new_version=False was used incorrectly\n",
    "     print(f\"\\nAzure ML User Error registering dataset asset '{DATASET_NAME}' using SDK v1: {ue}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred registering dataset asset '{DATASET_NAME}' using SDK v1: {e}\")\n",
    "    # Consider adding more specific error handling if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIR_OUTPUT = \"./simulated-data-raw/\"\n",
    "\n",
    "# if not os.path.exists(DIR_OUTPUT):\n",
    "#     os.makedirs(DIR_OUTPUT)\n",
    "\n",
    "# start_date = datetime.datetime.strptime(\"2018-04-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# for day in range(transactions_df.TX_TIME_DAYS.max()+1):\n",
    "    \n",
    "#     transactions_day = transactions_df[transactions_df.TX_TIME_DAYS==day].sort_values('TX_TIME_SECONDS')\n",
    "    \n",
    "#     date = start_date + datetime.timedelta(days=day)\n",
    "#     filename_output = date.strftime(\"%Y-%m-%d\")+'.pkl'\n",
    "    \n",
    "#     # Protocol=4 required for Google Colab\n",
    "#     transactions_day.to_pickle(DIR_OUTPUT+filename_output, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "import datetime # Make sure datetime is imported\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "# --- Placeholder Data (Replace with your actual DataFrame loading) ---\n",
    "# Create a sample transactions_df for demonstration\n",
    "# It needs 'TX_TIME_DAYS' and 'TX_TIME_SECONDS' columns\n",
    "sample_data = []\n",
    "start_sim_date = datetime.datetime.strptime(\"2025-04-01\", \"%Y-%m-%d\")\n",
    "for day in range(5): # Simulate 5 days of data\n",
    "    current_date = start_sim_date + datetime.timedelta(days=day)\n",
    "    for i in range(3): # Simulate 3 transactions per day\n",
    "        seconds = i * 1000 + day * 100 # Some variation in seconds\n",
    "        sample_data.append({\n",
    "            'TX_TIME_DAYS': day,\n",
    "            'TX_TIME_SECONDS': seconds,\n",
    "            'value': (day + 1) * (i + 1) * 10,\n",
    "            'category': chr(65 + i), # A, B, C\n",
    "            'timestamp': current_date + datetime.timedelta(seconds=seconds)\n",
    "        })\n",
    "transactions_df = pd.DataFrame(sample_data)\n",
    "print(\"Sample transactions_df created:\")\n",
    "print(transactions_df.head())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the target directory *within* the Azure Blob Storage container\n",
    "# associated with the default datastore.\n",
    "TARGET_BLOB_DIR = \"simulated-data-daily-parq\" # Changed name slightly for clarity\n",
    "\n",
    "# Define the start date corresponding to day 0 in your data\n",
    "start_date = datetime.datetime.strptime(\"2025-04-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# --- 1. Connect to Azure ML Workspace ---\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"Connected to Azure ML Workspace: {ws.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not automatically connect to workspace: {e}\")\n",
    "    print(\"Attempting manual connection details if needed (ensure you fill them in)\")\n",
    "    \n",
    "if 'ws' not in locals():\n",
    "     print(\"ERROR: Workspace connection failed. Cannot proceed.\")\n",
    "     exit()\n",
    "\n",
    "# --- 2. Get the Default Datastore ---\n",
    "try:\n",
    "    datastore = ws.get_default_datastore() # Renamed to 'datastore' to match your snippet\n",
    "    print(f\"\\nUsing Default Datastore: {datastore.name} ({datastore.datastore_type})\")\n",
    "\n",
    "    # --- Processing and Uploading Loop ---\n",
    "    print(f\"\\nStarting data processing and upload to datastore '{datastore.name}'...\")\n",
    "    print(f\"Target directory in datastore: '{TARGET_BLOB_DIR}'\")\n",
    "\n",
    "    # Use a temporary directory to stage files locally before uploading\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(f\"Using temporary directory for staging: {temp_dir}\")\n",
    "\n",
    "        # Assuming 'TX_TIME_DAYS' column exists and represents days since start_date\n",
    "        if 'TX_TIME_DAYS' not in transactions_df.columns:\n",
    "             raise ValueError(\"DataFrame must contain a 'TX_TIME_DAYS' column.\")\n",
    "\n",
    "        max_days = transactions_df.TX_TIME_DAYS.max()\n",
    "        print(f\"Processing data for {max_days + 1} days (0 to {max_days})...\")\n",
    "\n",
    "        for day in range(max_days + 1):\n",
    "            print(f\"\\nProcessing Day {day}/{max_days}...\")\n",
    "\n",
    "            # Filter data for the current day and sort (as per your original logic)\n",
    "            # Ensure 'TX_TIME_SECONDS' exists if you need sorting\n",
    "            if 'TX_TIME_SECONDS' in transactions_df.columns:\n",
    "                transactions_day = transactions_df[transactions_df.TX_TIME_DAYS == day].sort_values('TX_TIME_SECONDS')\n",
    "            else:\n",
    "                transactions_day = transactions_df[transactions_df.TX_TIME_DAYS == day]\n",
    "                print(\"  Warning: 'TX_TIME_SECONDS' column not found, skipping sort.\")\n",
    "\n",
    "\n",
    "            if transactions_day.empty:\n",
    "                print(f\"  No transactions found for day {day}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate date and create filename (now with .parquet extension)\n",
    "            date = start_date + datetime.timedelta(days=day)\n",
    "            filename_output = date.strftime(\"%Y-%m-%d\") + '.parquet' # Changed extension\n",
    "\n",
    "            # --- Save to Temporary Local Parquet File ---\n",
    "            temp_local_filepath = os.path.join(temp_dir, filename_output)\n",
    "            try:\n",
    "                # Save as Parquet using pyarrow engine\n",
    "                transactions_day.to_parquet(\n",
    "                    temp_local_filepath,\n",
    "                    version='1.0',\n",
    "                    engine='pyarrow',\n",
    "                    index=False # Often don't need the pandas index saved\n",
    "                )\n",
    "                print(f\"  Saved data locally to temporary file: {temp_local_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error saving day {day} to temporary parquet file: {e}\")\n",
    "                continue # Skip to the next day if saving fails\n",
    "\n",
    "            # --- Upload the single Parquet file to Azure ML Datastore ---\n",
    "            try:\n",
    "                print(f\"  Uploading '{filename_output}' to datastore path: '{TARGET_BLOB_DIR}/'\")\n",
    "                # datastore.upload_files expects a list of files to upload\n",
    "                # target_path is the destination *directory* within the datastore\n",
    "                datastore.upload_files(\n",
    "                    files=[temp_local_filepath],    # List containing the path to the temp file\n",
    "                    target_path=TARGET_BLOB_DIR,    # Target directory in the blob container\n",
    "                    overwrite=True,                 # Overwrite if file already exists in blob\n",
    "                    show_progress=False             # Set to True for verbose progress bars\n",
    "                )\n",
    "                # The final path in the blob store will be: <container_root>/TARGET_BLOB_DIR/filename_output\n",
    "                print(f\"  Successfully uploaded to {datastore.name}/{TARGET_BLOB_DIR}/{filename_output}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error uploading file {filename_output} to datastore: {e}\")\n",
    "                # Decide how to handle upload errors (e.g., retry, log, raise)\n",
    "\n",
    "            # No need to manually remove temp_local_filepath, the TemporaryDirectory context manager handles it.\n",
    "\n",
    "    print(\"\\nFinished processing and uploading all files.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred outside the main processing loop: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"- Ensure 'azureml-core', 'pyarrow', and 'pandas' are installed\")\n",
    "    print(\"- Check storage permissions (e.g., Storage Blob Data Contributor role on the storage account)\")\n",
    "    print(\"- Validate Azure authentication (running in AML Compute, logged in via CLI, etc.)\")\n",
    "    print(\"- Ensure your DataFrame ('transactions_df') is loaded correctly and has 'TX_TIME_DAYS'.\")\n",
    "\n",
    "# --- Dependency Notes ---\n",
    "print(\"\\n--- Package Recommendations ---\")\n",
    "print(\"If you see urllib3 warnings, run:\")\n",
    "print('pip install \"urllib3<1.27\"')\n",
    "print(\"Required packages: pandas, azureml-core, pyarrow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated dataset is also available from Github at `https://github.com/Fraud-Detection-Handbook/simulated-data-raw`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
