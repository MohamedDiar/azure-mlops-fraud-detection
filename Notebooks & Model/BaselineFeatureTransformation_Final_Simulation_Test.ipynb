{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msKL_BVo6xfp"
   },
   "source": [
    "(Baseline_Feature_Transformation)=\n",
    "# Baseline feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLSlMy9d6xfy"
   },
   "source": [
    "The simulated dataset generated in the previous section is simple. It only contains the essential features that characterize a payment card transaction. These are: a unique identifier for the transaction, the date and time of the transaction, the transaction amount, a unique identifier for the customer, a unique number for the merchant, and a binary variable that labels the transaction as legitimate or fraudulent (0 for legitimate or 1 for fraudulent). Fig. 1 provides the first three rows of the simulated dataset:\n",
    "\n",
    "![alt text](images/tx_table.png)\n",
    "<p style=\"text-align: center;\">\n",
    "Fig. 1. The first three transactions in the simulated dataset used in this chapter.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcJjxARm6xf2"
   },
   "source": [
    "What each row essentially summarizes is that, at 00:00:31, on the 1st of April 2018, a customer with ID 596 made a payment of a value of 57.19 to a merchant with ID 3156, and that the transaction was not fraudulent. Then, at 00:02:10, on the 1st of April 2018, a customer with ID 4961 made a payment of a value of 81.51 to a merchant with ID 3412, and that the transaction was not fraudulent. And so on. The simulated dataset is a long list of such transactions (1.8 million in total). The variable `transaction_ID` is a unique identifier for each transaction.  \n",
    "\n",
    "While conceptually simple for a human, such a set of features is however not appropriate for a machine learning predictive model. Machine learning algorithms typically require *numerical* and *ordered* features. Numerical means that the type of the variable must be an integer or a real number. Ordered means that the order of the values of a variable is meaningful.\n",
    "\n",
    "In this dataset, the only numerical and ordered features are the transaction amount and the fraud label. The date is a Panda timestamp, and therefore not numerical. The identifiers for the transactions, customers, and terminals are numerical but not ordered: it would not make sense to assume for example that the terminal with ID 3548 is 'bigger' or 'larger' than the terminal with ID 1983. Rather, these identifiers represent distinct 'entities', which are referred to as *categorical* features.\n",
    "\n",
    "There is unfortunately no standard procedure to deal with non-numerical or categorical features. The topic is known in the machine learning literature as *feature engineering* or *feature transformation*. In essence, the goal of feature engineering is to design new features that are assumed to be relevant for a predictive problem. The design of these features is usually problem-dependent, and involves domain knowledge.\n",
    "\n",
    "In this section, we will implement three types of feature transformation that are known to be relevant for payment card fraud detection.\n",
    "\n",
    "![encoding](images/encoding_variables.png)\n",
    "\n",
    "The first type of transformation involves the date/time variable, and consists in creating binary features that characterize potentially relevant periods. We will create two such features. The first one will characterize whether a transaction occurs during a weekday or during the weekend. The second will characterize whether a transaction occurs during the day or the night. These features can be useful since it has been observed in real-world datasets that fraudulent patterns differ between weekdays and weekends, and between the day and night.  \n",
    "\n",
    "The second type of transformation involves the customer ID and consists in creating features that characterize the customer spending behaviors. We will follow the RFM (Recency, Frequency, Monetary value) framework proposed in {cite}`VANVLASSELAER201538`, and keep track of the average spending amount and number of transactions for each customer and for three window sizes. This will lead to the creation of six new features.\n",
    "\n",
    "The third type of transformation involves the terminal ID and consists in creating new features that characterize the 'risk' associated with the terminal. The risk will be defined as the average number of frauds that were observed on the terminal for three window sizes. This will lead to the creation of three new features.\n",
    "\n",
    "The table below summarizes the types of transformation that will be performed and the new features that will be created.\n",
    "\n",
    "|Original feature name|Original feature type|Transformation|Number of new features|New feature(s) type|\n",
    "|---|---|---|---|---|\n",
    "|TX\\_DATE\\_TIME | Panda timestamp |0 if transaction during a weekday, 1 if transaction during a weekend. The new feature is called TX_DURING_WEEKEND.|1|Integer (0/1)|\n",
    "|TX\\_DATE\\_TIME | Panda timestamp |0 if transaction between 6am and 0pm, 1 if transaction between 0pm and 6am. The new feature is called TX_DURING_NIGHT.|1|Integer (0/1)|\n",
    "|CUSTOMER\\_ID | Categorical variable |Number of transactions by the customer in the last n day(s), for n in {1,7,30}. The new features are called CUSTOMER_ID_NB_TX_nDAY_WINDOW.|3|Integer|\n",
    "|CUSTOMER\\_ID | Categorical variable |Average spending amount in the last n day(s), for n in {1,7,30}. The new features are called CUSTOMER_ID_AVG_AMOUNT_nDAY_WINDOW.|3|Real|\n",
    "|TERMINAL\\_ID | Categorical variable |Number of transactions on the terminal in the last n+d day(s), for n in {1,7,30} and d=7. The parameter d is called delay and will be discussed later in this notebook. The new features are called TERMINAL_ID_NB_TX_nDAY_WINDOW.|3|Integer|\n",
    "|TERMINAL\\_ID | Categorical variable |Average number of frauds on the terminal in the last n+d day(s), for n in {1,7,30} and d=7. The parameter d is called delay and will be discussed later in this notebook. The new features are called TERMINAL_ID_RISK_nDAY_WINDOW.|3|Real|\n",
    "\n",
    "The following sections provide the implementation for each of these three transformations. After the transformations, a set of 14 new features will be created. Note that some of the features are the result of aggregation functions over the values of other features or conditions (same customer, given time window). These features are often referred to as *aggregated features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sX-RWg3M6xf8",
    "outputId": "a3db35da-e141-4313-eb87-8d772080adbf",
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# # Initialization: Load shared functions and simulated data\n",
    "\n",
    "# # Load shared functions\n",
    "# !curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/Chapter_References/shared_functions.py\n",
    "# %run shared_functions.py\n",
    "\n",
    "# # Get simulated data from Github repository\n",
    "# if not os.path.exists(\"simulated-data-raw\"):\n",
    "#     !git clone https://github.com/Fraud-Detection-Handbook/simulated-data-raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W0LPRv36xgF"
   },
   "source": [
    "## Loading of dataset\n",
    "\n",
    "Let us first load the transaction data simulated in the previous notebook. We will load the transaction files from April to September. Files can be loaded using the `read_from_files` function in the [shared functions](shared_functions) notebook. The function was put in this notebook since it will be used frequently throughout this book.\n",
    "\n",
    "The function takes as input the folder where the data files are located, and the dates that define the period to load (between `BEGIN_DATE` and `END_DATE`). It returns a DataFrame of transactions. The transactions are sorted by chronological order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tempfile # To create temporary local files before uploading\n",
    "from azureml.core import Workspace, Datastore, Dataset # Import Dataset class\n",
    "from azureml.exceptions import UserErrorException # For specific Azure ML errors\n",
    "from azureml.data.datapath import DataPath # Needed to reference paths in datastores\n",
    "\n",
    "# --- Azure ML Connection ---\n",
    "try:\n",
    "    ws = Workspace.from_config(path=\"config.json\")\n",
    "    print(f\"Connected to Azure ML workspace: {ws.name}\")\n",
    "\n",
    "    # --- Get the Datastore ---\n",
    "    # Assuming the folder is in your default datastore.\n",
    "    # If it's in a different datastore, use: ws.datastores['your_datastore_name']\n",
    "    datastore = ws.get_default_datastore()\n",
    "    print(f\"Using default datastore: {datastore.name} (Type: {datastore.datastore_type})\")\n",
    "    print(f\"Datastore points to container: {datastore.container_name}\") # Good to confirm\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Azure ML Workspace or getting datastore: {e}\")\n",
    "    print(\"Please ensure you are running in an Azure ML environment or have a valid config.json.\")\n",
    "    raise SystemExit(\"Failed to connect to Azure ML Workspace.\")\n",
    "\n",
    "relative_folder_path_in_datastore = \"simulated-data-raw/\"\n",
    "print(f\"\\nTarget folder path within datastore '{datastore.name}': {relative_folder_path_in_datastore}\")\n",
    "\n",
    "# --- Create a FileDataset Object Dynamically ---\n",
    "# Create a DataPath object referencing the specific folder in the datastore\n",
    "data_path_object = DataPath(datastore=datastore, path_on_datastore=relative_folder_path_in_datastore)\n",
    "print(f\"Creating dynamic FileDataset object pointing to: {data_path_object}\")\n",
    "folder_as_dataset = Dataset.File.from_files(path=data_path_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdJwbnJc6xgI",
    "outputId": "1b670784-e6c1-4f54-81e7-0612e10657dd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    \"\"\"\n",
    "    Reads pickle files within a date range from a directory, concatenates them,\n",
    "    and performs basic cleaning. (Original version)\n",
    "    \"\"\"\n",
    "    start_file = BEGIN_DATE + '.pkl'\n",
    "    end_file = END_DATE + '.pkl'\n",
    "\n",
    "    # Filter files based on the date range embedded in the filename\n",
    "    files = sorted([\n",
    "        os.path.join(DIR_INPUT, f) for f in os.listdir(DIR_INPUT)\n",
    "        if f.endswith('.pkl') and start_file <= f <= end_file\n",
    "    ])\n",
    "\n",
    "    if not files:\n",
    "        print(f\"Warning: No files found in {DIR_INPUT} for range {BEGIN_DATE} to {END_DATE}\")\n",
    "        return pd.DataFrame() # Return empty if no files match\n",
    "\n",
    "    frames = []\n",
    "    for f_path in files:\n",
    "        try:\n",
    "            df = pd.read_pickle(f_path)\n",
    "            frames.append(df)\n",
    "            del df # Optional: help memory\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f_path}: {e}. Skipping.\")\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame() # Return empty if reading failed for all files\n",
    "\n",
    "    df_final = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Post-processing\n",
    "    df_final = df_final.sort_values('TRANSACTION_ID')\n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "    df_final = df_final.replace([-1], 0)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "BEGIN_DATE = \"2025-04-01\"\n",
    "END_DATE = \"2025-12-26\"\n",
    "\n",
    "\n",
    "# --- Mount Dataset and Read Files ---\n",
    "transactions_df = None # Initialize\n",
    "\n",
    "\n",
    "print(f\"\\nMounting dataset and loading files from {BEGIN_DATE} to {END_DATE}...\")\n",
    "with folder_as_dataset.mount() as mount_context:\n",
    "    mounted_path = mount_context.mount_point # The temporary path to the data\n",
    "\n",
    "    # *** Use %time (if in IPython/Jupyter) and call your function ***\n",
    "    # Pass the 'mounted_path' as the input directory\n",
    "    %time transactions_df = read_from_files(mounted_path, BEGIN_DATE, END_DATE)\n",
    "\n",
    "# Mount automatically released here\n",
    "\n",
    "# --- Print Final Summary ---\n",
    "if transactions_df is not None and not transactions_df.empty:\n",
    "     # Check if the expected column exists before summing\n",
    "    fraud_sum = 0\n",
    "    if 'TX_FRAUD' in transactions_df.columns:\n",
    "        fraud_sum = transactions_df.TX_FRAUD.sum()\n",
    "    else:\n",
    "        print(\"Warning: Column 'TX_FRAUD' not found.\")\n",
    "\n",
    "    print(f\"\\n{len(transactions_df)} transactions loaded, containing {fraud_sum} fraudulent transactions.\")\n",
    "    # print(transactions_df.head()) # Optional: view first few rows\n",
    "elif transactions_df is not None and transactions_df.empty:\n",
    "     print(\"\\nLoading process completed, but no data was loaded (check date range and file availability).\")\n",
    "else:\n",
    "     # This case might occur if read_from_files returned None unexpectedly, though it shouldn't with current logic\n",
    "     print(\"\\nLoading failed or resulted in None.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_G7UvqnB6xgP",
    "outputId": "6fcb623f-16f4-4bbf-ea95-3b8d76f130e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNYiw3Ex6xgT"
   },
   "source": [
    "## Date and time transformations\n",
    "\n",
    "We will create two new binary features from the transaction dates and times:\n",
    "\n",
    "* The first will characterize whether a transaction occurs during a weekday (value 0) or a weekend (1), and will be called `TX_DURING_WEEKEND`\n",
    "* The second will characterize whether a transaction occurs during the day or during the day (0) or during the night (1). The night is defined as hours that are between 0pm and 6am. It will be called `TX_DURING_NIGHT`.\n",
    "\n",
    "For the `TX_DURING_WEEKEND` feature, we define a function `is_weekend` that takes as input a Panda timestamp, and returns 1 if the date is during a weekend, or 0 otherwise. The timestamp object conveniently provides the `weekday` function to help in computing this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMs6DnaU6xgY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_weekend(tx_datetime):\n",
    "\n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = tx_datetime.weekday()\n",
    "    # Binary value: 0 if weekday, 1 if weekend\n",
    "    is_weekend = weekday>=5\n",
    "\n",
    "    return int(is_weekend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMDG5TFB6xgf"
   },
   "source": [
    "It is then straghtforward to compute this feature for all transactions using the Panda `apply` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2LIPSS96xgi",
    "outputId": "5489ee5a-2fec-4157-d6cb-804262258694",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time transactions_df['TX_DURING_WEEKEND']=transactions_df.TX_DATETIME.apply(is_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4AfHz_p6xgk"
   },
   "source": [
    "We follow the same logic to implement the `TX_DURING_NIGHT` feature. First, a function `is_night` that takes as input a Panda timestamp, and returns 1 if the time is during the night, or 0 otherwise. The timestamp object conveniently provides the hour property to help in computing this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyf8eO1D6xgm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_night(tx_datetime):\n",
    "\n",
    "    # Get the hour of the transaction\n",
    "    tx_hour = tx_datetime.hour\n",
    "    # Binary value: 1 if hour less than 6, and 0 otherwise\n",
    "    is_night = tx_hour<=6\n",
    "\n",
    "    return int(is_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bddLlpm16xgn",
    "outputId": "54c16251-7c51-48ab-916f-1a4b803ba117",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time transactions_df['TX_DURING_NIGHT']=transactions_df.TX_DATETIME.apply(is_night)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwfkITM6xgo"
   },
   "source": [
    "Let us check that these features where correctly computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Y1DAOc2E6xgp",
    "outputId": "ae965d0a-2b68-4d03-ad9b-2667daa4f204",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_TIME_DAYS>=30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAqpJNAZ6xgr"
   },
   "source": [
    "The 2018-05-01 was a Monday, and the 2018-09-30 a Sunday. These dates are correctly flagged as weekday, and weekend, respectively. The day and night feature is also correctly set for the first transactions, that happen closely after 0 pm, and the last transactions that happen closely before 0 pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmF2H3th6xgt"
   },
   "source": [
    "## Customer ID transformations\n",
    "\n",
    "Let us now proceed with customer ID transformations. We will take inspiration from the RFM (Recency, Frequency, Monetary value) framework proposed in {cite}`VANVLASSELAER201538`, and compute two of these features over three time windows. The first feature will be the number of transactions that occur within a time window (Frequency). The second will be the average amount spent in these transactions (Monetary value). The time windows will be set to one, seven, and thirty days. This will generate six new features. Note that these time windows could later be optimized along with the models using a model selection procedure ([Chapter 5](Model_Selection)).\n",
    "\n",
    "Let us implement these transformations by writing a `get_customer_spending_behaviour_features` function. The function takes as inputs the set of transactions for a customer and a set of window sizes. It returns a DataFrame with the six new features. Our implementation relies on the Panda `rolling` function, which makes easy the computation of aggregates over a time window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvEfj2FV6xgu",
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_customer_spending_behaviour_features(customer_transactions, windows_size_in_days=[1,7,30]):\n",
    "\n",
    "    # Let us first order transactions chronologically\n",
    "    customer_transactions=customer_transactions.sort_values('TX_DATETIME')\n",
    "\n",
    "    # The transaction date and time is set as the index, which will allow the use of the rolling function\n",
    "    customer_transactions.index=customer_transactions.TX_DATETIME\n",
    "\n",
    "    # For each window size\n",
    "    for window_size in windows_size_in_days:\n",
    "\n",
    "        # Compute the sum of the transaction amounts and the number of transactions for the given window size\n",
    "        SUM_AMOUNT_TX_WINDOW=customer_transactions['TX_AMOUNT'].rolling(str(window_size)+'d').sum()\n",
    "        NB_TX_WINDOW=customer_transactions['TX_AMOUNT'].rolling(str(window_size)+'d').count()\n",
    "\n",
    "        # Compute the average transaction amount for the given window size\n",
    "        # NB_TX_WINDOW is always >0 since current transaction is always included\n",
    "        AVG_AMOUNT_TX_WINDOW=SUM_AMOUNT_TX_WINDOW/NB_TX_WINDOW\n",
    "\n",
    "        # Save feature values\n",
    "        customer_transactions['CUSTOMER_ID_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)\n",
    "        customer_transactions['CUSTOMER_ID_AVG_AMOUNT_'+str(window_size)+'DAY_WINDOW']=list(AVG_AMOUNT_TX_WINDOW)\n",
    "\n",
    "    # Reindex according to transaction IDs\n",
    "    customer_transactions.index=customer_transactions.TRANSACTION_ID\n",
    "\n",
    "    # And return the dataframe with the new features\n",
    "    return customer_transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V54EQiiq6xgw"
   },
   "source": [
    "Let us compute these aggregates for the first customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "gPpUTBN56xgy",
    "outputId": "4540290c-28ef-4162-b25d-45a29085d409",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spending_behaviour_customer_0=get_customer_spending_behaviour_features(transactions_df[transactions_df.CUSTOMER_ID==0])\n",
    "spending_behaviour_customer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vcBEj5r6xg0"
   },
   "source": [
    "We can check that the new features are consistent with the customer profile (see the previous notebook). For customer 0, the mean amount was `mean_amount`=62.26, and the transaction frequency was `mean_nb_tx_per_day`=2.18. These values are indeed closely matched by the features `CUSTOMER_ID_NB_TX_30DAY_WINDOW` and `CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW`, especially after 30 days.\n",
    "\n",
    "Let us now generate these features for all customers. This is straightforward using the Panda `groupby` and `apply` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcCG8_FW6xg1",
    "outputId": "198285be-91c6-4e48-df9e-38814af7c0e6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time transactions_df=transactions_df.groupby('CUSTOMER_ID').apply(lambda x: get_customer_spending_behaviour_features(x, windows_size_in_days=[1,7,30]))\n",
    "transactions_df=transactions_df.sort_values('TX_DATETIME').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "zfGQlz4j6xg3",
    "outputId": "2a0dec7d-32d2-4447-9f9f-e1a65bc496ba",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXDGDcTV6xg5"
   },
   "source": [
    "## Terminal ID transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLSWNrtU6xg6"
   },
   "source": [
    "Finally, let us proceed with the terminal ID transformations. The main goal will be to extract a *risk score*, that assesses the exposure of a given terminal ID to fraudulent transactions. The risk score will be defined as the average number of fraudulent transactions that occurred on a terminal ID over a time window. As for customer ID transformations, we will use three window sizes, of 1, 7, and 30 days.\n",
    "\n",
    "Contrary to customer ID transformations, the time windows will not directly precede a given transaction. Instead, they will be shifted back by a *delay period*. The delay period accounts for the fact that, in practice, the fraudulent transactions are only discovered after a fraud investigation or a customer complaint. Hence, the fraudulent labels, which are needed to compute the risk score, are only available after this delay period. To a first approximation, this delay period will be set to one week. The motivations for the delay period will be further argued in [Chapter 5, Validation strategies](Validation_Strategies).\n",
    "\n",
    "Let us perform the computation of the risk scores by defining a `get_count_risk_rolling_window` function. The function takes as inputs the DataFrame of transactions for a given terminal ID, the delay period, and a list of window sizes. In the first stage, the number of transactions and fraudulent transactions are computed for the delay period (`NB_TX_DELAY` and `NB_FRAUD_DELAY`). In the second stage, the number of transactions and fraudulent transactions are computed for each window size plus the delay period (`NB_TX_DELAY_WINDOW` and `NB_FRAUD_DELAY_WINDOW`). The number of transactions and fraudulent transactions that occurred for a given window size, shifted back by the delay period, is then obtained by simply computing the differences of the quantities obtained for the delay period, and the window size plus delay period:\n",
    "\n",
    "```\n",
    "NB_FRAUD_WINDOW=NB_FRAUD_DELAY_WINDOW-NB_FRAUD_DELAY\n",
    "NB_TX_WINDOW=NB_TX_DELAY_WINDOW-NB_TX_DELAY\n",
    "```\n",
    "\n",
    "The risk score is finally obtained by computing the proportion of fraudulent transactions for each window size (or 0 if no transaction occurred for the given window):\n",
    "\n",
    "```\n",
    "RISK_WINDOW=NB_FRAUD_WINDOW/NB_TX_WINDOW\n",
    "```\n",
    "\n",
    "Additionally to the risk score, the function also returns the number of transactions for each window size. This results in the addition of six new features: The risk and number of transactions, for three window sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuEeCIJ36xg-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_count_risk_rolling_window(terminal_transactions, delay_period=7, windows_size_in_days=[1,7,30], feature=\"TERMINAL_ID\"):\n",
    "\n",
    "    terminal_transactions=terminal_transactions.sort_values('TX_DATETIME')\n",
    "\n",
    "    terminal_transactions.index=terminal_transactions.TX_DATETIME\n",
    "\n",
    "    NB_FRAUD_DELAY=terminal_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').sum()\n",
    "    NB_TX_DELAY=terminal_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').count()\n",
    "\n",
    "    for window_size in windows_size_in_days:\n",
    "\n",
    "        NB_FRAUD_DELAY_WINDOW=terminal_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').sum()\n",
    "        NB_TX_DELAY_WINDOW=terminal_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').count()\n",
    "\n",
    "        NB_FRAUD_WINDOW=NB_FRAUD_DELAY_WINDOW-NB_FRAUD_DELAY\n",
    "        NB_TX_WINDOW=NB_TX_DELAY_WINDOW-NB_TX_DELAY\n",
    "\n",
    "        RISK_WINDOW=NB_FRAUD_WINDOW/NB_TX_WINDOW\n",
    "\n",
    "        terminal_transactions[feature+'_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)\n",
    "        terminal_transactions[feature+'_RISK_'+str(window_size)+'DAY_WINDOW']=list(RISK_WINDOW)\n",
    "\n",
    "    terminal_transactions.index=terminal_transactions.TRANSACTION_ID\n",
    "\n",
    "    # Replace NA values with 0 (all undefined risk scores where NB_TX_WINDOW is 0)\n",
    "    terminal_transactions.fillna(0,inplace=True)\n",
    "\n",
    "    return terminal_transactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "MqwUSZD76xg_",
    "outputId": "07b43be8-a6a9-4804-d618-e102bff63abf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df[transactions_df.TX_FRAUD==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJk7ye0c6xhB"
   },
   "source": [
    "Let us compute these six features for the first terminal ID containing at least one fraud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBFpcbvK6xhC",
    "outputId": "0d6e7ff9-0a8b-4d16-e87b-ba1b57ac9e0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the first terminal ID that contains frauds\n",
    "transactions_df[transactions_df.TX_FRAUD==0].TERMINAL_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "asTxImDO6xhD",
    "outputId": "c42b8dab-10ac-41a8-a495-f0de2f0e0a80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_count_risk_rolling_window(transactions_df[transactions_df.TERMINAL_ID==3059], delay_period=7, windows_size_in_days=[1,7,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1ikGlsS6xhF"
   },
   "source": [
    "We can check that the first fraud occurred on the 2018/09/10, and that risk scores only start being counted with a one-week delay.\n",
    "\n",
    "Let us finally generate these features for all terminals. This is straightforward using the Panda `groupby` and `apply` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwxYxdk66xhG",
    "outputId": "de02261f-edcb-4880-d53b-b926b9c5cfb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time transactions_df=transactions_df.groupby('TERMINAL_ID').apply(lambda x: get_count_risk_rolling_window(x, delay_period=7, windows_size_in_days=[1,7,30], feature=\"TERMINAL_ID\"))\n",
    "transactions_df=transactions_df.sort_values('TX_DATETIME').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTiDXwYj6xhH",
    "outputId": "289d9273-af11-4afb-ac52-a17b92f6be11",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjcfZPOk6xhI"
   },
   "source": [
    "## Saving of dataset\n",
    "\n",
    "Let us finally save the dataset, split into daily batches, using the pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Assuming 'transactions_df' holds the full generated dataframe from previous cells\n",
    "# Assuming 'datastore' is an Azure ML SDK v1 Datastore object already connected\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the date range for data to be saved in the primary target directory\n",
    "# Data WITHIN this range (inclusive) goes to TARGET_BLOB_DIR_PRIMARY\n",
    "# Data OUTSIDE this range goes to TARGET_BLOB_DIR_SIMULATION\n",
    "# Use 'YYYY-MM-DD' format\n",
    "SPLIT_START_DATE_STR = \"2025-04-01\"\n",
    "SPLIT_END_DATE_STR   = \"2025-08-14\" \n",
    "\n",
    "# Define the target directory names *within* the Azure Blob Storage container\n",
    "TARGET_BLOB_DIR_PRIMARY    = \"fraud-data-transformed\"           # Primary location (e.g., for versioning later)\n",
    "TARGET_BLOB_DIR_SIMULATION = \"for_simulation_transformed\"       # Location for data outside the range\n",
    "\n",
    "# Define the overall start date of the simulation (must match generation logic)\n",
    "simulation_start_date_obj = datetime.datetime.strptime(f\"{SPLIT_START_DATE_STR}\", \"%Y-%m-%d\")\n",
    "\n",
    "# Convert split dates to datetime objects for comparison\n",
    "try:\n",
    "    split_start_date = datetime.datetime.strptime(SPLIT_START_DATE_STR, \"%Y-%m-%d\")\n",
    "    split_end_date = datetime.datetime.strptime(SPLIT_END_DATE_STR, \"%Y-%m-%d\")\n",
    "    print(f\"Data between {SPLIT_START_DATE_STR} and {SPLIT_END_DATE_STR} will go to '{TARGET_BLOB_DIR_PRIMARY}'.\")\n",
    "    print(f\"Data outside this range will go to '{TARGET_BLOB_DIR_SIMULATION}'.\")\n",
    "except ValueError:\n",
    "    print(\"Error: Invalid date format in SPLIT_START_DATE_STR or SPLIT_END_DATE_STR. Use YYYY-MM-DD.\")\n",
    "    raise SystemExit(\"Invalid date configuration.\")\n",
    "\n",
    "# --- Processing and Uploading Loop ---\n",
    "print(f\"\\nStarting data processing and upload, splitting by date range to datastore '{datastore.name}'...\")\n",
    "\n",
    "# Use a temporary directory to stage files locally before uploading\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(f\"Using temporary directory for staging: {temp_dir}\")\n",
    "\n",
    "    max_days = transactions_df.TX_TIME_DAYS.max()\n",
    "    for day in range(max_days + 1):\n",
    "        # Calculate the actual date for the current day in the loop\n",
    "        current_day_date = simulation_start_date_obj + datetime.timedelta(days=day)\n",
    "        print(f\"\\nProcessing Day {day}/{max_days} ({current_day_date.strftime('%Y-%m-%d')})...\")\n",
    "\n",
    "        # Filter data for the current day and sort\n",
    "        transactions_day = transactions_df[transactions_df.TX_TIME_DAYS == day].sort_values('TX_TIME_SECONDS')\n",
    "\n",
    "        if transactions_day.empty:\n",
    "            print(f\"  No transactions found for day {day}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Create filename\n",
    "        filename_output = current_day_date.strftime(\"%Y-%m-%d\") + '.pkl'\n",
    "\n",
    "        # --- Save to Temporary Local File ---\n",
    "        temp_local_filepath = os.path.join(temp_dir, filename_output)\n",
    "        try:\n",
    "            # Protocol=4 is often needed for compatibility (e.g., Python versions)\n",
    "            transactions_day.to_pickle(temp_local_filepath, protocol=4)\n",
    "            # print(f\"  Saved data locally to temporary file: {temp_local_filepath}\") # Optional: uncomment for verbose logging\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving day {day} to temporary pickle file: {e}\")\n",
    "            continue # Skip to the next day if saving fails\n",
    "\n",
    "        # --- Determine Target Blob Directory based on date ---\n",
    "        if split_start_date <= current_day_date <= split_end_date:\n",
    "            upload_target_path = TARGET_BLOB_DIR_PRIMARY\n",
    "            print(f\"  -> Date in range. Uploading to: '{upload_target_path}/'\")\n",
    "        else:\n",
    "            upload_target_path = TARGET_BLOB_DIR_SIMULATION\n",
    "            print(f\"  -> Date outside range. Uploading to: '{upload_target_path}/'\")\n",
    "\n",
    "        # --- Upload to Azure ML Datastore (using the determined path) ---\n",
    "        try:\n",
    "            # datastore.upload_files expects a list of files to upload\n",
    "            # target_path is the destination *directory* within the datastore\n",
    "            datastore.upload_files(\n",
    "                files=[temp_local_filepath],    # List containing the path to the temp file\n",
    "                target_path=upload_target_path, # Use the dynamically determined path here\n",
    "                overwrite=True,                 # Overwrite if file already exists in blob\n",
    "                show_progress=False             # Set to True for verbose progress bars\n",
    "            )\n",
    "            # The final path in the blob store will be: <container_root>/upload_target_path/filename_output\n",
    "            print(f\"  Successfully uploaded to {datastore.name}/{upload_target_path}/{filename_output}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error uploading file {filename_output} to datastore path '{upload_target_path}': {e}\")\n",
    "            # Decide how to handle upload errors (e.g., retry, log, raise)\n",
    "\n",
    "        # The temporary file (temp_local_filepath) is automatically removed\n",
    "        # when the 'with tempfile.TemporaryDirectory()' block exits.\n",
    "\n",
    "print(f\"\\nFinished processing and uploading all files.\")\n",
    "print(f\"Files within {SPLIT_START_DATE_STR} - {SPLIT_END_DATE_STR} are in '{TARGET_BLOB_DIR_PRIMARY}'.\")\n",
    "print(f\"Files outside that range are in '{TARGET_BLOB_DIR_SIMULATION}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.exceptions import UserErrorException # More specific exception handling\n",
    "\n",
    "DATASET_NAME = \"transformed_fraud_data\"\n",
    "VERSIONED_DATA_PATH = \"fraud-data-transformed\" \n",
    "\n",
    "print(f\"\\nRegistering Azure ML Dataset asset '{DATASET_NAME}' using SDK v1...\")\n",
    "\n",
    "# --- Check for prerequisite objects ---\n",
    "if 'ws' not in locals():\n",
    "     raise NameError(\"Workspace object 'ws' not found. Ensure Cell 1 executed successfully.\")\n",
    "if 'datastore' not in locals():\n",
    "     raise NameError(\"Datastore object 'datastore' not found. Ensure Cell 1 executed successfully.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    file_dataset = Dataset.File.from_files(path=(datastore, VERSIONED_DATA_PATH))\n",
    "\n",
    "\n",
    "    registered_dataset = file_dataset.register(\n",
    "        workspace=ws,\n",
    "        name=DATASET_NAME,\n",
    "        description=f\"Transformed daily fraud transaction data (.pkl files) stored in the '{VERSIONED_DATA_PATH}' folder.\",\n",
    "        create_new_version=True, \n",
    "        tags={'source': 'sdk_v1_registration'} \n",
    "    )\n",
    "\n",
    "    print(f\"\\nSuccessfully registered/updated dataset asset:\")\n",
    "    print(f\"  Name: {registered_dataset.name}\")\n",
    "    print(f\"  Version: {registered_dataset.version}\")\n",
    "    print(f\"  ID: {registered_dataset.id}\") \n",
    "\n",
    "except UserErrorException as ue:\n",
    "     print(f\"\\nAzure ML User Error registering dataset asset '{DATASET_NAME}' using SDK v1: {ue}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred registering dataset asset '{DATASET_NAME}' using SDK v1: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR29aY6q6xhI"
   },
   "outputs": [],
   "source": [
    "# DIR_OUTPUT = \"./simulated-data-transformed/\"\n",
    "\n",
    "# if not os.path.exists(DIR_OUTPUT):\n",
    "#     os.makedirs(DIR_OUTPUT)\n",
    "\n",
    "# start_date = datetime.datetime.strptime(\"2018-04-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# for day in range(transactions_df.TX_TIME_DAYS.max()+1):\n",
    "\n",
    "#     transactions_day = transactions_df[transactions_df.TX_TIME_DAYS==day].sort_values('TX_TIME_SECONDS')\n",
    "\n",
    "#     date = start_date + datetime.timedelta(days=day)\n",
    "#     filename_output = date.strftime(\"%Y-%m-%d\")+'.pkl'\n",
    "\n",
    "#     # Protocol=4 required for Google Colab\n",
    "#     transactions_day.to_pickle(DIR_OUTPUT+filename_output, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAoEN9KR6xhJ"
   },
   "source": [
    "The generated dataset is also available from Github at `https://github.com/Fraud-Detection-Handbook/simulated-data-transformed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huWGCAnv6xhK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Custom Env (YAML)",
   "language": "python",
   "name": "custom-notebook-env-conda-adapted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
