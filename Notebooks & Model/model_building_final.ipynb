{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1d006c9c22514db8878069e5216114d6_aligned",
    "deepnote_cell_type": "markdown",
    "id": "UiQ_zkKgDSY4_aligned"
   },
   "source": [
    "# Fraud Detection: MLOps-Integrated Experimentation Path (Aligned with Original)\n",
    "\n",
    "This notebook follows the experimentation path for fraud detection models, integrating MLOps principles using Azure ML, MLflow, and DVC.\n",
    "**Crucially, this version is meticulously aligned with the exact functions, parameters, dates, and hyperparameter grids from the original `model_building_fraud.ipynb` provided.**\n",
    "It includes:\n",
    "1. Setup: Connecting to Azure ML, initializing MLflow, defining parameters and DVC paths.\n",
    "2. Loading Versioned Data (Transformed Features)\n",
    "3. Defining Training and Test Sets (Temporal Split with Delay - using original dates and functions)\n",
    "4. Model Selection Process (Prequential Validation with MLflow Tracking - using original grids and functions)\n",
    "5. Training the Final Model (Logging parameters and artifacts - using original dates and functions)\n",
    "6. Evaluating and Logging the Final Model Performance (using original dates and functions)\n",
    "7. Registering the Model in Azure ML\n",
    "\n",
    "Visualizations and performance comparisons are included, with results logged via MLflow, aiming to replicate the original notebook's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4ee33670aac74a09b01707a4762fbbdf_aligned",
    "deepnote_cell_type": "code",
    "id": "install_libs_aligned",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d9639a4f81e47738b1a6fb92adeaa40_aligned",
    "deepnote_cell_type": "markdown",
    "id": "h0zEFNSPDSY6_aligned"
   },
   "source": [
    "## 0. Setup: MLOps Integration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e49c1e0adb0b4f64a844869fea5f307f_aligned",
    "deepnote_cell_type": "code",
    "id": "mC3uLCw8DSY8_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Core Libraries (from original + MLOps) ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "import joblib # For saving/loading model locally\n",
    "\n",
    "# --- MLOps & Tracking Libraries ---\n",
    "import mlflow\n",
    "import mlflow.sklearn # Required for autologging sklearn pipelines\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Model, Dataset \n",
    "import dvc.api # For potentially reading params/metadata from DVC\n",
    "import git # For logging git commit\n",
    "\n",
    "# --- ML Libraries (from original) ---\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "# from sklearn import metrics, preprocessing, model_selection, pipeline, tree, ensemble, linear_model # Be explicit if needed\n",
    "import imblearn # Used in shared functions, though maybe not in main path\n",
    "import xgboost\n",
    "\n",
    "# --- Visualization (from original) ---\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "import graphviz # For plotting decision trees\n",
    "import warnings\n",
    "\n",
    "# --- Settings (from original + MLOps) ---\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "warnings.filterwarnings('ignore')\n",
    "import pkg_resources # For checking azureml-mlflow version\n",
    "\n",
    "print(f\"mlflow version: {mlflow.__version__}\")\n",
    "print(f\"azureml-core version: {azureml.core.VERSION}\")\n",
    "try:\n",
    "    print(f\"azureml-mlflow version: {pkg_resources.get_distribution('azureml-mlflow').version}\")\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(\"azureml-mlflow is NOT installed.\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"xgboost version: {xgboost.__version__}\")\n",
    "\n",
    "# --- Azure ML Workspace Connection ---\n",
    "try:\n",
    "    # Assumes config.json is in the specified path or notebook directory\n",
    "    ws = Workspace.from_config(path='./config.json') # Adjust path if needed\n",
    "    print(f\"Connected to Azure ML Workspace: {ws.name} in {ws.location}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load workspace from config.json: {e}\")\n",
    "    print(\"Ensure config.json is present or Azure ML environment is configured.\")\n",
    "    ws = None\n",
    "\n",
    "# --- MLflow Integration with Azure ML ---\n",
    "if ws:\n",
    "    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "    print(f\"MLflow tracking URI set to Azure ML: {mlflow.get_tracking_uri()}\")\n",
    "else:\n",
    "    print(\"WARNING: Azure ML Workspace not connected. MLflow will track locally.\")\n",
    "\n",
    "# --- Define MLflow Experiment ---\n",
    "# experiment_name = 'Credit_Risk_Fraud_Detection_MLOps_Final'\n",
    "experiment_name = 'New_Final_Credit_Risk_Fraud_Detection_MLOps'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "azure_experiment = Experiment(workspace=ws, name=experiment_name) if ws else None\n",
    "print(f\"MLflow experiment set to: {experiment_name}\")\n",
    "\n",
    "INPUT_DATASET_NAME = \"transformed_fraud_data\"\n",
    "input_dataset = None # Initialize dataset variable\n",
    "\n",
    "print(f\"\\nAttempting to retrieve Azure ML Dataset '{INPUT_DATASET_NAME}'...\")\n",
    "\n",
    "# Get the LATEST version of the dataset by name\n",
    "input_dataset = Dataset.get_by_name(workspace=ws, name=INPUT_DATASET_NAME)\n",
    "print(f\"Successfully retrieved dataset:\")\n",
    "print(f\"  Name: {input_dataset.name}\")\n",
    "print(f\"  Version: {input_dataset.version} (Latest)\")\n",
    "print(f\"  ID: {input_dataset.id}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Define Output Model Information ---\n",
    "MODEL_OUTPUT_DIR = \"outputs/model\"\n",
    "MODEL_NAME_AML = \"credit_risk_fraud_model_mlops_Final\"\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "60052527c13d4ee78ee8bfe034e482df_aligned",
    "deepnote_cell_type": "markdown",
    "id": "44Aj7nYwDSY9_aligned"
   },
   "source": [
    "## 0.1 Shared Functions Definition (Copied from Original)\n",
    "\n",
    "These functions are copied **exactly** from the original `model_building_fraud.ipynb` notebook (cell `ab97f2268d8f4ddea5577c96fa1571f6`) to ensure identical behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ab97f2268d8f4ddea5577c96fa1571f6_aligned",
    "deepnote_cell_type": "code",
    "id": "3j9jLLdpDSY__aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Loading and saving data\n",
    "\n",
    "# ### read_from_files\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Feature Transformation](Baseline_Feature_Transformation).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Load a set of pickle files, put them together in a single DataFrame, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "\n",
    "    # Ensure DIR_INPUT exists\n",
    "    if not os.path.isdir(DIR_INPUT):\n",
    "        print(f\"ERROR: Input directory not found: {DIR_INPUT}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    try:\n",
    "        files = [os.path.join(DIR_INPUT, f) for f in os.listdir(DIR_INPUT) if f.endswith('.pkl') and f>=BEGIN_DATE+'.pkl' and f<=END_DATE+'.pkl']\n",
    "        files.sort() # Sort files chronologically by name\n",
    "    except FileNotFoundError:\n",
    "         print(f\"ERROR: Input directory not found during listdir: {DIR_INPUT}\")\n",
    "         return pd.DataFrame()\n",
    "         \n",
    "    if not files:\n",
    "        print(f\"WARNING: No '.pkl' files found in {DIR_INPUT} for date range {BEGIN_DATE} to {END_DATE}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(f\"Found {len(files)} files to load.\")\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_pickle(f)\n",
    "            # Basic validation after loading each file\n",
    "            if 'TRANSACTION_ID' not in df.columns:\n",
    "                 print(f\"Warning: TRANSACTION_ID missing in {f}. Skipping file.\")\n",
    "                 continue\n",
    "            frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f}: {e}\")\n",
    "        # del df # Not typically necessary\n",
    "        \n",
    "    if not frames:\n",
    "        print(\"No dataframes were successfully loaded.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df_final = pd.concat(frames)\n",
    "\n",
    "    df_final=df_final.sort_values('TRANSACTION_ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data\n",
    "    df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    # Ensure TX_DATETIME is datetime type if it exists\n",
    "    if 'TX_DATETIME' in df_final.columns and not pd.api.types.is_datetime64_any_dtype(df_final['TX_DATETIME']):\n",
    "        try:\n",
    "            df_final['TX_DATETIME'] = pd.to_datetime(df_final['TX_DATETIME'])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert TX_DATETIME to datetime: {e}\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# ### save_object\n",
    "#\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Save oject as pickle file\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# ## Data preprocessing\n",
    "\n",
    "# ### scaleData\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "# NOTE: This function is NOT used when using sklearn.pipeline.Pipeline, as the pipeline handles scaling internally during fit/transform.\n",
    "# It's kept here only for strict adherence to the original notebook's function definitions.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def scaleData(train,test,features):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(train[features])\n",
    "    # Create copies to avoid modifying original dataframes\n",
    "    train_scaled = train.copy()\n",
    "    test_scaled = test.copy()\n",
    "    train_scaled[features]=scaler.transform(train[features])\n",
    "    test_scaled[features]=scaler.transform(test[features])\n",
    "\n",
    "    # Returning the scaler might be useful, though the original didn't explicitly\n",
    "    # return (train_scaled, test_scaled, scaler) \n",
    "    return (train_scaled, test_scaled)\n",
    "\n",
    "\n",
    "# ## Train/Test splitting strategies\n",
    "\n",
    "# ### get_train_test_set\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "# Sampling ratio added in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def get_train_test_set(transactions_df,\n",
    "                       start_date_training,\n",
    "                       delta_train=7,delta_delay=7,delta_test=7,\n",
    "                       sampling_ratio=1.0,\n",
    "                       random_state=0):\n",
    "    \n",
    "    # Validate inputs\n",
    "    if 'TX_DATETIME' not in transactions_df.columns or 'TX_TIME_DAYS' not in transactions_df.columns or 'CUSTOMER_ID' not in transactions_df.columns or 'TX_FRAUD' not in transactions_df.columns or 'TRANSACTION_ID' not in transactions_df.columns:\n",
    "        raise ValueError(\"Missing required columns in transactions_df for get_train_test_set\")\n",
    "    if not isinstance(start_date_training, datetime.datetime):\n",
    "         raise ValueError(\"start_date_training must be a datetime object\")\n",
    "         \n",
    "    # Get the training set data\n",
    "    train_df = transactions_df[(transactions_df.TX_DATETIME>=start_date_training) &\n",
    "                               (transactions_df.TX_DATETIME<start_date_training+datetime.timedelta(days=delta_train))]\n",
    "\n",
    "    # Get the test set data\n",
    "    test_df = []\n",
    "\n",
    "    # Note: Cards known to be compromised after the delay period are removed from the test set\n",
    "    # That is, for each test day, all frauds known at (test_day-delay_period) are removed\n",
    "\n",
    "    # First, get known defrauded customers from the training set\n",
    "    known_defrauded_customers = set(train_df[train_df.TX_FRAUD==1].CUSTOMER_ID)\n",
    "\n",
    "    # Get the relative starting day of training set (easier than TX_DATETIME to collect test data)\n",
    "    if train_df.empty:\n",
    "        print(f\"Warning: Training period starting {start_date_training.strftime('%Y-%m-%d')} is empty.\")\n",
    "        # Return empty DataFrames matching expected columns\n",
    "        return (train_df.copy(), pd.DataFrame(columns=transactions_df.columns))\n",
    "        \n",
    "    start_tx_time_days_training = train_df.TX_TIME_DAYS.min()\n",
    "\n",
    "    # Then, for each day of the test set\n",
    "    for day in range(delta_test):\n",
    "\n",
    "        # Get test data for that day\n",
    "        test_day_date = start_tx_time_days_training + delta_train + delta_delay + day\n",
    "        test_df_day = transactions_df[transactions_df.TX_TIME_DAYS == test_day_date]\n",
    "\n",
    "        # Compromised cards from that test day, minus the delay period, are added to the pool of known defrauded customers\n",
    "        # **Correction:** Original notebook used `day-1` relative to `start_tx_time_days_training+delta_train` for delay period check, NOT `test_day_date - delta_delay`\n",
    "        delay_period_check_day = start_tx_time_days_training + delta_train + day - 1 \n",
    "        test_df_day_delay_period = transactions_df[transactions_df.TX_TIME_DAYS == delay_period_check_day]\n",
    "\n",
    "        new_defrauded_customers = set(test_df_day_delay_period[test_df_day_delay_period.TX_FRAUD==1].CUSTOMER_ID)\n",
    "        known_defrauded_customers = known_defrauded_customers.union(new_defrauded_customers)\n",
    "\n",
    "        test_df_day = test_df_day[~test_df_day.CUSTOMER_ID.isin(known_defrauded_customers)]\n",
    "\n",
    "        test_df.append(test_df_day)\n",
    "\n",
    "    if not test_df:\n",
    "        print(f\"Warning: Test period for training start {start_date_training.strftime('%Y-%m-%d')} resulted in an empty set after filtering.\")\n",
    "        test_df = pd.DataFrame(columns=transactions_df.columns)\n",
    "    else:\n",
    "        test_df = pd.concat(test_df)\n",
    "\n",
    "    # If subsample\n",
    "    if sampling_ratio<1:\n",
    "\n",
    "        train_df_frauds=train_df[train_df.TX_FRAUD==1].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df_genuine=train_df[train_df.TX_FRAUD==0].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df=pd.concat([train_df_frauds,train_df_genuine])\n",
    "\n",
    "    # Sort data sets by ascending order of transaction ID\n",
    "    train_df=train_df.sort_values('TRANSACTION_ID')\n",
    "    test_df=test_df.sort_values('TRANSACTION_ID')\n",
    "\n",
    "    return (train_df, test_df)\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# This function was present in the original shared functions but NOT explicitly called in the main flow of model_building_fraud.ipynb.\n",
    "# Keeping it here for strict adherence to the shared function definitions.\n",
    "def get_train_delay_test_set(transactions_df,\n",
    "                             start_date_training,\n",
    "                             delta_train=7,delta_delay=7,delta_test=7,\n",
    "                             sampling_ratio=1.0,\n",
    "                             random_state=0):\n",
    "\n",
    "    # Get the training set data\n",
    "    train_df = transactions_df[(transactions_df.TX_DATETIME>=start_date_training) &\n",
    "                               (transactions_df.TX_DATETIME<start_date_training+datetime.timedelta(days=delta_train))]\n",
    "\n",
    "    # Get the delay set data\n",
    "    delay_df = transactions_df[(transactions_df.TX_DATETIME>=start_date_training+datetime.timedelta(days=delta_train)) &\n",
    "                               (transactions_df.TX_DATETIME<start_date_training+datetime.timedelta(days=delta_train)+\n",
    "                                                                               +datetime.timedelta(days=delta_delay))]\n",
    "\n",
    "    # Get the test set data (reusing logic from get_train_test_set)\n",
    "    test_df = []\n",
    "    known_defrauded_customers = set(train_df[train_df.TX_FRAUD==1].CUSTOMER_ID)\n",
    "    if train_df.empty:\n",
    "        print(f\"Warning (get_train_delay_test_set): Training period starting {start_date_training.strftime('%Y-%m-%d')} is empty.\")\n",
    "        return (train_df.copy(), delay_df.copy(), pd.DataFrame(columns=transactions_df.columns))\n",
    "        \n",
    "    start_tx_time_days_training = train_df.TX_TIME_DAYS.min()\n",
    "\n",
    "    for day in range(delta_test):\n",
    "        test_day_date = start_tx_time_days_training + delta_train + delta_delay + day\n",
    "        test_df_day = transactions_df[transactions_df.TX_TIME_DAYS == test_day_date]\n",
    "        \n",
    "        delay_period_check_day = start_tx_time_days_training + delta_train + day - 1 \n",
    "        test_df_day_delay_period = transactions_df[transactions_df.TX_TIME_DAYS == delay_period_check_day]\n",
    "        \n",
    "        new_defrauded_customers = set(test_df_day_delay_period[test_df_day_delay_period.TX_FRAUD==1].CUSTOMER_ID)\n",
    "        known_defrauded_customers = known_defrauded_customers.union(new_defrauded_customers)\n",
    "\n",
    "        test_df_day = test_df_day[~test_df_day.CUSTOMER_ID.isin(known_defrauded_customers)]\n",
    "        test_df.append(test_df_day)\n",
    "\n",
    "    if not test_df:\n",
    "        test_df = pd.DataFrame(columns=transactions_df.columns)\n",
    "    else:\n",
    "        test_df = pd.concat(test_df)\n",
    "\n",
    "    # If subsample\n",
    "    if sampling_ratio<1:\n",
    "        train_df_frauds=train_df[train_df.TX_FRAUD==1].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df_genuine=train_df[train_df.TX_FRAUD==0].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df=pd.concat([train_df_frauds,train_df_genuine])\n",
    "\n",
    "    # Sort data sets by ascending order of transaction ID\n",
    "    train_df=train_df.sort_values('TRANSACTION_ID')\n",
    "    test_df=test_df.sort_values('TRANSACTION_ID')\n",
    "\n",
    "    return (train_df, delay_df, test_df)\n",
    "\n",
    "\n",
    "# ### prequentialSplit\n",
    "#\n",
    "# First use in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def prequentialSplit(transactions_df,\n",
    "                     start_date_training,\n",
    "                     n_folds=4,\n",
    "                     delta_train=7,\n",
    "                     delta_delay=7,\n",
    "                     delta_assessment=7):\n",
    "\n",
    "    prequential_split_indices=[]\n",
    "\n",
    "    # For each fold\n",
    "    for fold in range(n_folds):\n",
    "\n",
    "        # Shift back start date for training by the fold index times the assessment period (delta_assessment)\n",
    "        # (See Fig. 5)\n",
    "        start_date_training_fold = start_date_training-datetime.timedelta(days=fold*delta_assessment)\n",
    "\n",
    "        # Get the training and test (assessment) sets\n",
    "        (train_df, test_df)=get_train_test_set(transactions_df,\n",
    "                                               start_date_training=start_date_training_fold,\n",
    "                                               delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_assessment)\n",
    "\n",
    "        # Get the indices from the two sets, and add them to the list of prequential splits\n",
    "        # Check if sets are empty before getting indices\n",
    "        if not train_df.empty and not test_df.empty:\n",
    "            indices_train=list(train_df.index)\n",
    "            indices_test=list(test_df.index)\n",
    "            prequential_split_indices.append((indices_train,indices_test))\n",
    "        else:\n",
    "             print(f\"Warning (prequentialSplit): Fold {fold} generated empty train ({train_df.shape}) or test ({test_df.shape}) set for start date {start_date_training_fold.strftime('%Y-%m-%d')}. Skipping fold.\")\n",
    "\n",
    "    if not prequential_split_indices:\n",
    "        print(f\"Warning (prequentialSplit): No valid folds generated for start date {start_date_training.strftime('%Y-%m-%d')} and {n_folds} folds.\")\n",
    "        \n",
    "    return prequential_split_indices\n",
    "\n",
    "\n",
    "# ## Predictions functions\n",
    "\n",
    "# ### fit_model_and_get_predictions\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "# NOTE: Less used with Pipelines/GridSearchCV, kept for original function adherence.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def fit_model_and_get_predictions(classifier, train_df, test_df,\n",
    "                                  input_features, output_feature=\"TX_FRAUD\",scale=True):\n",
    "\n",
    "    train_df_processed = train_df.copy()\n",
    "    test_df_processed = test_df.copy()\n",
    "    scaler = None # Initialize scaler\n",
    "    \n",
    "    # By default, scales input data using the separate scaleData function (as original)\n",
    "    if scale:\n",
    "        # scaleData returns tuple (train_scaled, test_scaled)\n",
    "        (train_df_processed, test_df_processed)=scaleData(train_df_processed, test_df_processed, input_features)\n",
    "        # If scaleData were modified to return scaler, capture it here\n",
    "        # (train_df_processed, test_df_processed, scaler) = scaleData(train_df_processed, test_df_processed, input_features)\n",
    "\n",
    "    # We first train the classifier using the `fit` method, and pass as arguments the input and output features\n",
    "    start_time=time.time()\n",
    "    classifier.fit(train_df_processed[input_features], train_df_processed[output_feature])\n",
    "    training_execution_time=time.time()-start_time\n",
    "\n",
    "    # We then get the predictions on the training and test data using the `predict_proba` method\n",
    "    # The predictions are returned as a numpy array, that provides the probability of fraud for each transaction\n",
    "    start_time=time.time()\n",
    "    predictions_test=classifier.predict_proba(test_df_processed[input_features])[:,1]\n",
    "    prediction_execution_time=time.time()-start_time\n",
    "\n",
    "    predictions_train=classifier.predict_proba(train_df_processed[input_features])[:,1]\n",
    "\n",
    "    # The result is returned as a dictionary containing the fitted models,\n",
    "    # and the predictions on the training and test sets\n",
    "    model_and_predictions_dictionary = {'classifier': classifier,\n",
    "                                        # 'scaler': scaler, # Only include if scaleData returns it\n",
    "                                        'predictions_test': predictions_test,\n",
    "                                        'predictions_train': predictions_train,\n",
    "                                        'training_execution_time': training_execution_time,\n",
    "                                        'prediction_execution_time': prediction_execution_time\n",
    "                                       }\n",
    "\n",
    "    return model_and_predictions_dictionary\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Performance assessment\n",
    "\n",
    "# ### card_precision_top_k_day\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "# Detailed in [Chapter 4, Precision_top_K_Metrics](Precision_Top_K_Metrics).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def card_precision_top_k_day(df_day,top_k):\n",
    "\n",
    "    # Ensure required columns are present\n",
    "    if not all(col in df_day.columns for col in ['CUSTOMER_ID', 'predictions', 'TX_FRAUD']):\n",
    "        print(\"Warning (card_precision_top_k_day): Missing required columns. Returning empty list and 0.\")\n",
    "        return [], 0.0\n",
    "        \n",
    "    # This takes the max of the predictions AND the max of label TX_FRAUD for each CUSTOMER_ID,\n",
    "    # and sorts by decreasing order of fraudulent prediction\n",
    "    # Handle empty df_day case\n",
    "    if df_day.empty:\n",
    "        return [], 0.0\n",
    "        \n",
    "    df_day_grouped = df_day.groupby('CUSTOMER_ID').max().sort_values(by=\"predictions\", ascending=False).reset_index(drop=False)\n",
    "\n",
    "    # Get the top k most suspicious cards\n",
    "    df_day_top_k=df_day_grouped.head(top_k)\n",
    "    list_detected_compromised_cards=list(df_day_top_k[df_day_top_k.TX_FRAUD==1].CUSTOMER_ID)\n",
    "\n",
    "    # Compute precision top k, handle top_k = 0\n",
    "    if top_k > 0:\n",
    "        card_precision_top_k = len(list_detected_compromised_cards) / top_k\n",
    "    else:\n",
    "        card_precision_top_k = 0.0\n",
    "\n",
    "    return list_detected_compromised_cards, card_precision_top_k\n",
    "\n",
    "\n",
    "# ### card_precision_top_k\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "# Detailed in [Chapter 4, Precision_top_K_Metrics](Precision_Top_K_Metrics).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def card_precision_top_k(predictions_df, top_k, remove_detected_compromised_cards=True):\n",
    "\n",
    "    # Ensure required columns are present\n",
    "    if not all(col in predictions_df.columns for col in ['TX_TIME_DAYS', 'CUSTOMER_ID', 'predictions', 'TX_FRAUD']):\n",
    "        raise ValueError(\"Missing required columns in predictions_df for card_precision_top_k\")\n",
    "        \n",
    "    # Sort days by increasing order\n",
    "    list_days=list(predictions_df['TX_TIME_DAYS'].unique())\n",
    "    list_days.sort()\n",
    "\n",
    "    # At first, the list of detected compromised cards is empty\n",
    "    list_detected_compromised_cards = []\n",
    "\n",
    "    card_precision_top_k_per_day_list = []\n",
    "    nb_compromised_cards_per_day = []\n",
    "\n",
    "    # For each day, compute precision top k\n",
    "    for day in list_days:\n",
    "\n",
    "        df_day = predictions_df[predictions_df['TX_TIME_DAYS']==day]\n",
    "        # Select only necessary columns for the daily calculation\n",
    "        df_day = df_day[['predictions', 'CUSTOMER_ID', 'TX_FRAUD']]\n",
    "\n",
    "        # Let us remove detected compromised cards from the set of daily transactions\n",
    "        df_day = df_day[df_day.CUSTOMER_ID.isin(list_detected_compromised_cards)==False]\n",
    "\n",
    "        # If df_day is empty after filtering, record 0 and continue\n",
    "        if df_day.empty:\n",
    "             nb_compromised_cards_per_day.append(0)\n",
    "             card_precision_top_k_per_day_list.append(0.0)\n",
    "             continue\n",
    "             \n",
    "        nb_compromised_cards_per_day.append(len(df_day[df_day.TX_FRAUD==1].CUSTOMER_ID.unique()))\n",
    "\n",
    "        detected_compromised_cards, card_precision_top_k_daily = card_precision_top_k_day(df_day,top_k)\n",
    "\n",
    "        card_precision_top_k_per_day_list.append(card_precision_top_k_daily)\n",
    "\n",
    "        # Let us update the list of detected compromised cards\n",
    "        if remove_detected_compromised_cards:\n",
    "            list_detected_compromised_cards.extend(detected_compromised_cards)\n",
    "\n",
    "    # Compute the mean, handle case where list is empty\n",
    "    if not card_precision_top_k_per_day_list:\n",
    "        mean_card_precision_top_k = 0.0\n",
    "    else:\n",
    "        mean_card_precision_top_k = np.array(card_precision_top_k_per_day_list).mean()\n",
    "\n",
    "    # Returns precision top k per day as a list, and resulting mean\n",
    "    return nb_compromised_cards_per_day, card_precision_top_k_per_day_list, mean_card_precision_top_k\n",
    "\n",
    "\n",
    "# ### card_precision_top_k_custom\n",
    "#\n",
    "# First use in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "# This is the scorer function for GridSearchCV.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def card_precision_top_k_custom(y_true, y_pred, top_k, transactions_df):\n",
    "\n",
    "    # Check inputs\n",
    "    if not isinstance(y_true, pd.Series) or not isinstance(transactions_df, pd.DataFrame):\n",
    "         print(\"Warning (CP@k scorer): y_true must be a pandas Series and transactions_df a DataFrame.\")\n",
    "         return 0.0\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(\"Warning (CP@k scorer): y_pred and y_true have different lengths.\")\n",
    "        return 0.0\n",
    "    if transactions_df.empty:\n",
    "         print(\"Warning (CP@k scorer): transactions_df is empty.\")\n",
    "         return 0.0\n",
    "         \n",
    "    # Let us create a predictions_df DataFrame, that contains all transactions matching the indices of the current fold\n",
    "    # (indices of the y_true vector)\n",
    "    current_fold_indices = y_true.index\n",
    "    # Ensure indices are present in the main transaction df\n",
    "    valid_indices = current_fold_indices.intersection(transactions_df.index)\n",
    "    if valid_indices.empty:\n",
    "        print(f\"Warning (CP@k scorer): No matching indices found in transactions_df for the current fold ({len(current_fold_indices)} indices).\")\n",
    "        return 0.0\n",
    "        \n",
    "    predictions_df=transactions_df.loc[valid_indices].copy()\n",
    "    \n",
    "    # Add predictions ensuring alignment with potentially filtered valid_indices\n",
    "    # Create a Series from y_pred with the original fold indices\n",
    "    y_pred_series = pd.Series(y_pred, index=current_fold_indices)\n",
    "    # Select only the predictions corresponding to valid_indices\n",
    "    predictions_df['predictions'] = y_pred_series.loc[valid_indices]\n",
    "\n",
    "    # Compute the CP@k using the function implemented in Chapter 4, Section 4.2\n",
    "    nb_compromised_cards_per_day,card_precision_top_k_per_day_list,mean_card_precision_top_k= \\\n",
    "        card_precision_top_k(predictions_df, top_k)\n",
    "\n",
    "    # Return the mean_card_precision_top_k\n",
    "    return mean_card_precision_top_k\n",
    "\n",
    "\n",
    "# ### performance_assessment\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def performance_assessment(predictions_df, output_feature='TX_FRAUD',\n",
    "                           prediction_feature='predictions', top_k_list=[100],\n",
    "                           rounded=True):\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if output_feature not in predictions_df.columns or prediction_feature not in predictions_df.columns:\n",
    "        raise ValueError(f\"Missing required columns ('{output_feature}', '{prediction_feature}') in predictions_df for performance_assessment\")\n",
    "\n",
    "    y_true = predictions_df[output_feature]\n",
    "    y_pred_proba = predictions_df[prediction_feature]\n",
    "    \n",
    "    AUC_ROC = np.nan\n",
    "    AP = np.nan\n",
    "    # Check if y_true contains multiple classes before calculating ROC AUC and AP\n",
    "    if len(y_true.unique()) > 1:\n",
    "        try:\n",
    "            AUC_ROC = metrics.roc_auc_score(y_true, y_pred_proba)\n",
    "            AP = metrics.average_precision_score(y_true, y_pred_proba)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning (performance_assessment): ValueError calculating AUC/AP: {e}\")\n",
    "    else:\n",
    "        print(\"Warning (performance_assessment): Only one class present in y_true. AUC ROC and Average Precision are not defined.\")\n",
    "\n",
    "    performances = pd.DataFrame([[AUC_ROC, AP]],\n",
    "                           columns=['AUC ROC','Average precision'])\n",
    "\n",
    "    # Add CP@k metric\n",
    "    for top_k in top_k_list:\n",
    "        # Check if columns required by card_precision_top_k are present\n",
    "        cpk_cols_present = all(col in predictions_df.columns for col in ['TX_TIME_DAYS', 'CUSTOMER_ID'])\n",
    "        if cpk_cols_present:\n",
    "            try:\n",
    "                _, _, mean_card_precision_top_k = card_precision_top_k(predictions_df, top_k)\n",
    "                performances['Card Precision@'+str(top_k)]=mean_card_precision_top_k\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning (performance_assessment): Error calculating CP@{top_k}: {e}\")\n",
    "                 performances['Card Precision@'+str(top_k)] = np.nan\n",
    "        else:\n",
    "             print(f\"Warning (performance_assessment): Skipping CP@{top_k} calculation due to missing columns (TX_TIME_DAYS, CUSTOMER_ID).\")\n",
    "             performances['Card Precision@'+str(top_k)] = np.nan\n",
    "             \n",
    "    if rounded:\n",
    "        performances = performances.round(3)\n",
    "\n",
    "    return performances\n",
    "\n",
    "\n",
    "# ### performance_assessment_model_collection\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "# NOTE: Less relevant with MLflow logging, kept for original function adherence.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def performance_assessment_model_collection(fitted_models_and_predictions_dictionary,\n",
    "                                            transactions_df,\n",
    "                                            type_set='test',\n",
    "                                            top_k_list=[100]):\n",
    "\n",
    "    performances=pd.DataFrame()\n",
    "\n",
    "    for classifier_name, model_and_predictions in fitted_models_and_predictions_dictionary.items():\n",
    "\n",
    "        # Make a copy to avoid modifying original df\n",
    "        predictions_df=transactions_df.copy()\n",
    "        pred_key = 'predictions_'+type_set\n",
    "        \n",
    "        if pred_key not in model_and_predictions:\n",
    "             print(f\"Warning (perf_assess_collection): Predictions '{pred_key}' not found for '{classifier_name}'. Skipping.\")\n",
    "             continue\n",
    "             \n",
    "        predictions_df['predictions']=model_and_predictions[pred_key]\n",
    "\n",
    "        # Call the main performance assessment function\n",
    "        try:\n",
    "            performances_model=performance_assessment(predictions_df, output_feature='TX_FRAUD',\n",
    "                                                    prediction_feature='predictions', top_k_list=top_k_list)\n",
    "            performances_model.index=[classifier_name]\n",
    "            # Use concat instead of append\n",
    "            performances=pd.concat([performances, performances_model])\n",
    "        except ValueError as e:\n",
    "             print(f\"Warning (perf_assess_collection): Error assessing '{classifier_name}' ({type_set}): {e}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Warning (perf_assess_collection): Unexpected error assessing '{classifier_name}' ({type_set}): {e}\")\n",
    "             \n",
    "    return performances\n",
    "\n",
    "\n",
    "# ### execution_times_model_collection\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "# NOTE: Less relevant with MLflow logging, kept for original function adherence.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def execution_times_model_collection(fitted_models_and_predictions_dictionary):\n",
    "\n",
    "    execution_times=pd.DataFrame()\n",
    "\n",
    "    for classifier_name, model_and_predictions in fitted_models_and_predictions_dictionary.items():\n",
    "\n",
    "        execution_times_model=pd.DataFrame()\n",
    "        execution_times_model['Training execution time']=[model_and_predictions.get('training_execution_time', np.nan)]\n",
    "        execution_times_model['Prediction execution time']=[model_and_predictions.get('prediction_execution_time', np.nan)]\n",
    "        execution_times_model.index=[classifier_name]\n",
    "\n",
    "        # Use concat instead of append\n",
    "        execution_times=pd.concat([execution_times, execution_times_model])\n",
    "\n",
    "    return execution_times\n",
    "\n",
    "\n",
    "# ### get_class_from_fraud_probability\n",
    "#\n",
    "# First use in [Chapter 4, Threshold Based Metrics](Threshold_Based_Metrics).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Getting classes from a vector of fraud probabilities and a threshold\n",
    "def get_class_from_fraud_probability(fraud_probabilities, threshold=0.5):\n",
    "\n",
    "    predicted_classes = [0 if fraud_probability<threshold else 1\n",
    "                         for fraud_probability in fraud_probabilities]\n",
    "\n",
    "    return predicted_classes\n",
    "\n",
    "\n",
    "# ### threshold_based_metrics\n",
    "#\n",
    "# First use in [Chapter 4, Threshold Based Metrics](Threshold_Based_Metrics).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def threshold_based_metrics(fraud_probabilities, true_label, thresholds_list):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds_list:\n",
    "\n",
    "        predicted_classes = get_class_from_fraud_probability(fraud_probabilities, threshold=threshold)\n",
    "\n",
    "        try:\n",
    "            # Use labels=[0, 1] to ensure matrix structure even if one class isn't predicted\n",
    "            cm = metrics.confusion_matrix(true_label, predicted_classes, labels=[0, 1])\n",
    "            TN, FP, FN, TP = cm.ravel()\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning (threshold_metrics): Could not calculate confusion matrix for threshold {threshold}: {e}\")\n",
    "            TN, FP, FN, TP = 0, 0, 0, 0 # Default to zeros\n",
    "\n",
    "        # Calculate metrics, handling potential division by zero\n",
    "        total_pop = TN+FP+FN+TP\n",
    "        MME = (FP+FN)/total_pop if total_pop > 0 else 0\n",
    "\n",
    "        TPR = TP/(TP+FN) if (TP+FN) > 0 else 0\n",
    "        TNR = TN/(TN+FP) if (TN+FP) > 0 else 0\n",
    "\n",
    "        FPR = FP/(TN+FP) if (TN+FP) > 0 else 0\n",
    "        FNR = FN/(TP+FN) if (TP+FN) > 0 else 0\n",
    "\n",
    "        BER = 1/2*(FPR+FNR)\n",
    "\n",
    "        Gmean = np.sqrt(TPR*TNR)\n",
    "\n",
    "        precision = TP/(TP+FP) if (TP+FP) > 0 else 0 # Original had 1, changed to 0 for consistency\n",
    "        FDR = FP/(TP+FP) if (TP+FP) > 0 else 0 # Original had 1, changed to 0\n",
    "\n",
    "        NPV = TN/(TN+FN) if (TN+FN) > 0 else 0 # Original had 1, changed to 0\n",
    "        FOR = FN/(TN+FN) if (TN+FN) > 0 else 0 # Original had 1, changed to 0\n",
    "\n",
    "        F1_score = 2*(precision*TPR)/(precision+TPR) if (precision+TPR) > 0 else 0\n",
    "\n",
    "        results.append([threshold, MME, TPR, TNR, FPR, FNR, BER, Gmean, precision, NPV, FDR, FOR, F1_score])\n",
    "\n",
    "    results_df = pd.DataFrame(results,columns=['Threshold' ,'MME', 'TPR', 'TNR', 'FPR', 'FNR', 'BER', 'G-mean', 'Precision', 'NPV', 'FDR', 'FOR', 'F1 Score'])\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# === Summary and Plotting Functions (Copied from Original) ===\n",
    "def get_summary_performances(performances_df, parameter_column_name=\"Parameters summary\"):\n",
    "    metrics_list = ['AUC ROC','Average precision','Card Precision@100'] # Assuming CP@100 was used\n",
    "    performances_results=pd.DataFrame(columns=metrics_list)\n",
    "    if performances_df.empty:\n",
    "         print(\"Warning: Empty performance dataframe passed to get_summary_performances.\")\n",
    "         # Return structure with N/A\n",
    "         na_vals = ['N/A'] * len(metrics_list)\n",
    "         performances_results.loc[\"Best estimated parameters\"]=na_vals\n",
    "         performances_results.loc[\"Validation performance\"]=na_vals\n",
    "         performances_results.loc[\"Test performance\"]=na_vals\n",
    "         performances_results.loc[\"Optimal parameter(s)\"]=na_vals\n",
    "         performances_results.loc[\"Optimal test performance\"]=na_vals\n",
    "         return performances_results\n",
    "\n",
    "    performances_df.reset_index(drop=True,inplace=True)\n",
    "    best_estimated_parameters = []\n",
    "    validation_performance = []\n",
    "    test_performance = []\n",
    "\n",
    "    for metric in metrics_list:\n",
    "        val_metric_col = metric+' Validation'\n",
    "        val_std_col = val_metric_col+' Std'\n",
    "        test_metric_col = metric+' Test'\n",
    "        test_std_col = test_metric_col+' Std'\n",
    "\n",
    "        # Check if columns exist\n",
    "        if val_metric_col not in performances_df.columns or test_metric_col not in performances_df.columns:\n",
    "             print(f\"Warning: Missing columns for metric {metric} in performance dataframe.\")\n",
    "             best_estimated_parameters.append('N/A')\n",
    "             validation_performance.append('N/A')\n",
    "             test_performance.append('N/A')\n",
    "             continue\n",
    "\n",
    "        # Handle potential NaNs from GridSearch failures\n",
    "        valid_performances = pd.to_numeric(performances_df[val_metric_col], errors='coerce')\n",
    "        if valid_performances.isna().all():\n",
    "            print(f\"Warning: All validation scores for metric {metric} are NaN.\")\n",
    "            # Find index with max Test score if validation failed (or default to 0)\n",
    "            test_scores_for_fallback = pd.to_numeric(performances_df[test_metric_col], errors='coerce')\n",
    "            if test_scores_for_fallback.isna().all():\n",
    "                index_best_validation_performance = 0 # Default index if test also NaN\n",
    "            else:\n",
    "                 index_best_validation_performance = test_scores_for_fallback.idxmax()\n",
    "            best_param = performances_df[parameter_column_name].iloc[index_best_validation_performance] if parameter_column_name in performances_df.columns else 'N/A'\n",
    "            val_perf_str = 'NaN'\n",
    "            # Get test perf at this index\n",
    "            test_perf_val = test_scores_for_fallback.iloc[index_best_validation_performance]\n",
    "            test_std_val = pd.to_numeric(performances_df.get(test_std_col, 0.0), errors='coerce').iloc[index_best_validation_performance]\n",
    "            test_perf_str = f\"{test_perf_val:.3f} +/- {test_std_val:.2f}\" if not pd.isna(test_perf_val) else 'NaN'\n",
    "        else:\n",
    "            index_best_validation_performance = valid_performances.idxmax()\n",
    "            best_param = performances_df[parameter_column_name].iloc[index_best_validation_performance] if parameter_column_name in performances_df.columns else 'N/A'\n",
    "            val_perf = valid_performances.iloc[index_best_validation_performance]\n",
    "            val_std = pd.to_numeric(performances_df.get(val_std_col, 0.0), errors='coerce').iloc[index_best_validation_performance]\n",
    "            test_perf = pd.to_numeric(performances_df[test_metric_col], errors='coerce').iloc[index_best_validation_performance]\n",
    "            test_std = pd.to_numeric(performances_df.get(test_std_col, 0.0), errors='coerce').iloc[index_best_validation_performance]\n",
    "\n",
    "            val_perf_str = f\"{val_perf:.3f} +/- {val_std:.2f}\" if not pd.isna(val_perf) else 'NaN'\n",
    "            test_perf_str = f\"{test_perf:.3f} +/- {test_std:.2f}\" if not pd.isna(test_perf) else 'NaN'\n",
    "\n",
    "        best_estimated_parameters.append(best_param)\n",
    "        validation_performance.append(val_perf_str)\n",
    "        test_performance.append(test_perf_str)\n",
    "\n",
    "    performances_results.loc[\"Best estimated parameters\"]=best_estimated_parameters\n",
    "    performances_results.loc[\"Validation performance\"]=validation_performance\n",
    "    performances_results.loc[\"Test performance\"]=test_performance\n",
    "\n",
    "    # Optimal on Test Set (similar logic)\n",
    "    optimal_test_performance = []\n",
    "    optimal_parameters = []\n",
    "    for metric_base in metrics_list:\n",
    "        test_metric_col = metric_base+' Test'\n",
    "        test_std_col = test_metric_col+' Std'\n",
    "\n",
    "        if test_metric_col not in performances_df.columns:\n",
    "            optimal_parameters.append('N/A')\n",
    "            optimal_test_performance.append('N/A')\n",
    "            continue\n",
    "\n",
    "        test_performances = pd.to_numeric(performances_df[test_metric_col], errors='coerce')\n",
    "        if test_performances.isna().all():\n",
    "            print(f\"Warning: All test scores for metric {metric_base} are NaN.\")\n",
    "            index_optimal_test_performance = 0\n",
    "            opt_param = performances_df[parameter_column_name].iloc[index_optimal_test_performance] if parameter_column_name in performances_df.columns else 'N/A'\n",
    "            opt_test_perf_str = 'NaN'\n",
    "        else:\n",
    "            index_optimal_test_performance = test_performances.idxmax()\n",
    "            opt_param = performances_df[parameter_column_name].iloc[index_optimal_test_performance] if parameter_column_name in performances_df.columns else 'N/A'\n",
    "            opt_test_perf = test_performances.iloc[index_optimal_test_performance]\n",
    "            opt_test_std = pd.to_numeric(performances_df.get(test_std_col, 0.0), errors='coerce').iloc[index_optimal_test_performance]\n",
    "            opt_test_perf_str = f\"{opt_test_perf:.3f} +/- {opt_test_std:.2f}\" if not pd.isna(opt_test_perf) else 'NaN'\n",
    "\n",
    "        optimal_parameters.append(opt_param)\n",
    "        optimal_test_performance.append(opt_test_perf_str)\n",
    "\n",
    "    performances_results.loc[\"Optimal parameter(s)\"]=optimal_parameters\n",
    "    performances_results.loc[\"Optimal test performance\"]=optimal_test_performance\n",
    "\n",
    "    return performances_results\n",
    "\n",
    "\n",
    "# ### model_selection_performances\n",
    "#\n",
    "# First use in [Chapter 5, Model Selection](Model_Selection).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def model_selection_performances(performances_df_dictionary,\n",
    "                                 performance_metric='AUC ROC',\n",
    "                                 model_classes=['Decision Tree', 'Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "                                 # ** ALIGNED WITH ORIGINAL NOTEBOOK DEFAULTS USED IN COMPARISON PLOT **\n",
    "                                 default_parameters_dictionary={\n",
    "                                     \"Decision Tree\": 'max_depth=None, random_state=0', # Original compared against max_depth=None\n",
    "                                     \"Logistic Regression\": 'C=1.0, random_state=0', # Original compared against C=1.0\n",
    "                                     \"Random Forest\": \"max_depth=None, n_estimators=100, n_jobs=1, random_state=0\", # Original compared against these\n",
    "                                     \"XGBoost\": \"eval_metric=logloss, learning_rate=0.3, max_depth=6, n_estimators=100, n_jobs=1, random_state=0, use_label_encoder=False, verbosity=0\" # Original compared against these\n",
    "                                 }):\n",
    "    # Helper to create comparable string representation from dict (used internally)\n",
    "    def params_dict_to_str_for_lookup(params_dict):\n",
    "        # Filter out 'clf__' prefix and sort for consistency\n",
    "        # Make sure to include all params defined in default_parameters_dictionary\n",
    "        items = [f\"{k.split('__')[1]}={v}\" for k, v in sorted(params_dict.items())]\n",
    "        return \", \".join(items)\n",
    "\n",
    "    # Convert default parameters dictionary values (which are strings) into a comparable format\n",
    "    # by parsing the string and creating a sorted string, similar to how 'Parameters summary' is created\n",
    "    def parse_default_str_to_comparable_str(param_str):\n",
    "         try:\n",
    "             # Basic parsing, assumes 'key=value' separated by ', '\n",
    "             items = sorted([item.strip() for item in param_str.split(',')])\n",
    "             return \", \".join(items)\n",
    "         except:\n",
    "             return param_str # Fallback if parsing fails\n",
    "             \n",
    "    default_strings_comparable = {model: parse_default_str_to_comparable_str(params)\n",
    "                                for model, params in default_parameters_dictionary.items()}\n",
    "\n",
    "\n",
    "    mean_performances_dictionary={\"Default parameters\": [],\"Best validation parameters\": [],\"Optimal parameters\": []}\n",
    "    std_performances_dictionary={\"Default parameters\": [],\"Best validation parameters\": [],\"Optimal parameters\": []}\n",
    "\n",
    "    for model_class in model_classes:\n",
    "        if model_class not in performances_df_dictionary or performances_df_dictionary[model_class].empty:\n",
    "             print(f\"Warning (model_sel_perf): No performance data for {model_class}. Skipping.\")\n",
    "             for key in mean_performances_dictionary:\n",
    "                 mean_performances_dictionary[key].append(np.nan)\n",
    "                 std_performances_dictionary[key].append(np.nan)\n",
    "             continue\n",
    "\n",
    "        performances_df=performances_df_dictionary[model_class]\n",
    "        if 'Parameters summary' not in performances_df.columns or 'Parameters' not in performances_df.columns:\n",
    "             print(f\"Warning (model_sel_perf): Missing 'Parameters summary' or 'Parameters' column for {model_class}. Skipping default param lookup.\")\n",
    "             mean_performances_dictionary[\"Default parameters\"].append(np.nan)\n",
    "             std_performances_dictionary[\"Default parameters\"].append(np.nan)\n",
    "        else:\n",
    "            # Use the generated string summary for matching defaults\n",
    "            default_param_comparable_str = default_strings_comparable.get(model_class, 'N/A')\n",
    "            # Create comparable string from 'Parameters summary' for matching\n",
    "            performances_df['Parameters summary comparable'] = performances_df['Parameters summary'].apply(parse_default_str_to_comparable_str)\n",
    "            default_performances = performances_df[performances_df['Parameters summary comparable'] == default_param_comparable_str]\n",
    "\n",
    "            if not default_performances.empty:\n",
    "                default_perf = pd.to_numeric(default_performances[performance_metric+\" Test\"], errors='coerce').values[0]\n",
    "                default_std = pd.to_numeric(default_performances[performance_metric+\" Test Std\"], errors='coerce').values[0]\n",
    "                mean_performances_dictionary[\"Default parameters\"].append(default_perf)\n",
    "                std_performances_dictionary[\"Default parameters\"].append(default_std)\n",
    "            else:\n",
    "                print(f\"Warning (model_sel_perf): Default parameters comparable string '{default_param_comparable_str}' not found for {model_class} in 'Parameters summary comparable'. Appending NaN.\")\n",
    "                # Optional: Print available comparable strings for debugging\n",
    "                # print(\"Available comparable summaries:\", performances_df['Parameters summary comparable'].unique())\n",
    "                mean_performances_dictionary[\"Default parameters\"].append(np.nan)\n",
    "                std_performances_dictionary[\"Default parameters\"].append(np.nan)\n",
    "\n",
    "        # Get best validation and optimal parameters using the summary function\n",
    "        performances_summary = get_summary_performances(performances_df, parameter_column_name=\"Parameters summary\")\n",
    "        if performance_metric in performances_summary.columns:\n",
    "            # Best validation parameters -> Test performance\n",
    "            mean_std_test = performances_summary.loc[\"Test performance\", performance_metric]\n",
    "            if isinstance(mean_std_test, str) and '+/-' in mean_std_test:\n",
    "                try:\n",
    "                    mean_val, std_val = map(float, mean_std_test.split(\"+/-\"))\n",
    "                    mean_performances_dictionary[\"Best validation parameters\"].append(mean_val)\n",
    "                    std_performances_dictionary[\"Best validation parameters\"].append(std_val)\n",
    "                except ValueError:\n",
    "                    mean_performances_dictionary[\"Best validation parameters\"].append(np.nan)\n",
    "                    std_performances_dictionary[\"Best validation parameters\"].append(np.nan)\n",
    "            else: # Handle 'N/A' or 'NaN'\n",
    "                mean_performances_dictionary[\"Best validation parameters\"].append(np.nan)\n",
    "                std_performances_dictionary[\"Best validation parameters\"].append(np.nan)\n",
    "\n",
    "            # Optimal test parameters -> Test performance\n",
    "            mean_std_opt = performances_summary.loc[\"Optimal test performance\", performance_metric]\n",
    "            if isinstance(mean_std_opt, str) and '+/-' in mean_std_opt:\n",
    "                try:\n",
    "                    mean_opt, std_opt = map(float, mean_std_opt.split(\"+/-\"))\n",
    "                    mean_performances_dictionary[\"Optimal parameters\"].append(mean_opt)\n",
    "                    std_performances_dictionary[\"Optimal parameters\"].append(std_opt)\n",
    "                except ValueError:\n",
    "                    mean_performances_dictionary[\"Optimal parameters\"].append(np.nan)\n",
    "                    std_performances_dictionary[\"Optimal parameters\"].append(np.nan)\n",
    "            else: # Handle 'N/A' or 'NaN'\n",
    "                mean_performances_dictionary[\"Optimal parameters\"].append(np.nan)\n",
    "                std_performances_dictionary[\"Optimal parameters\"].append(np.nan)\n",
    "        else:\n",
    "             print(f\"Warning (model_sel_perf): Metric '{performance_metric}' not found in summary columns for {model_class}.\")\n",
    "             mean_performances_dictionary[\"Best validation parameters\"].append(np.nan)\n",
    "             std_performances_dictionary[\"Best validation parameters\"].append(np.nan)\n",
    "             mean_performances_dictionary[\"Optimal parameters\"].append(np.nan)\n",
    "             std_performances_dictionary[\"Optimal parameters\"].append(np.nan)\n",
    "\n",
    "    return (mean_performances_dictionary,std_performances_dictionary)\n",
    "\n",
    "\n",
    "# ## Model selection\n",
    "\n",
    "# ### prequential_grid_search\n",
    "#\n",
    "# First use in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# ### prequential_grid_search\n",
    "#\n",
    "# First use in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "\n",
    "# In[ ]:\n",
    "def prequentialSplit_with_dates(transactions_df,\n",
    "                                start_date_training,\n",
    "                                n_folds=4,\n",
    "                                delta_train=7,\n",
    "                                delta_delay=7,\n",
    "                                delta_assessment=7):\n",
    "    \"\"\"\n",
    "    Generates prequential splits, returning indices and printing date ranges for each fold.\n",
    "\n",
    "    Args:\n",
    "        transactions_df (pd.DataFrame): DataFrame with transaction data (must have index and date info).\n",
    "        start_date_training (datetime.datetime): The *latest* training start date\n",
    "                                                 (used for fold 0). Folds go back in time.\n",
    "        n_folds (int): Number of folds.\n",
    "        delta_train (int): Duration of the training period in days.\n",
    "        delta_delay (int): Duration of the delay period in days.\n",
    "        delta_assessment (int): Duration of the assessment (test) period in days.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains (indices_train, indices_test)\n",
    "              for a fold. Matches the original return type for compatibility with GridSearchCV.\n",
    "              Returns an empty list if no valid folds are generated.\n",
    "        Prints: Detailed date ranges for each fold's train, delay, and test periods.\n",
    "    \"\"\"\n",
    "    prequential_split_indices = []\n",
    "    print(f\"\\n--- Generating Prequential Folds (n_folds={n_folds}) ---\")\n",
    "    print(f\"Base Start Date (Fold 0 Train Start): {start_date_training.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Deltas: Train={delta_train}, Delay={delta_delay}, Assessment={delta_assessment}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # For each fold\n",
    "    for fold in range(n_folds):\n",
    "        # Shift back start date for training by the fold index times the assessment period\n",
    "        start_date_training_fold = start_date_training - datetime.timedelta(days=fold * delta_assessment)\n",
    "\n",
    "        # Calculate all date boundaries for this fold\n",
    "        # End dates represent the start of the *next* period (exclusive end)\n",
    "        end_date_training_fold = start_date_training_fold + datetime.timedelta(days=delta_train)\n",
    "        start_date_delay_fold = end_date_training_fold\n",
    "        end_date_delay_fold = start_date_delay_fold + datetime.timedelta(days=delta_delay)\n",
    "        start_date_test_fold = end_date_delay_fold\n",
    "        end_date_test_fold = start_date_test_fold + datetime.timedelta(days=delta_assessment)\n",
    "\n",
    "        # Calculate inclusive end dates for printing clarity\n",
    "        inclusive_end_train = end_date_training_fold - datetime.timedelta(days=1)\n",
    "        inclusive_end_delay = end_date_delay_fold - datetime.timedelta(days=1)\n",
    "        inclusive_end_test = end_date_test_fold - datetime.timedelta(days=1)\n",
    "\n",
    "        print(f\"Fold {fold}:\")\n",
    "        print(f\"  Train Period: {start_date_training_fold.strftime('%Y-%m-%d')} to {inclusive_end_train.strftime('%Y-%m-%d')} ({delta_train} days)\")\n",
    "        print(f\"  Delay Period: {start_date_delay_fold.strftime('%Y-%m-%d')} to {inclusive_end_delay.strftime('%Y-%m-%d')} ({delta_delay} days)\")\n",
    "        print(f\"  Test Period:  {start_date_test_fold.strftime('%Y-%m-%d')} to {inclusive_end_test.strftime('%Y-%m-%d')} ({delta_assessment} days)\")\n",
    "\n",
    "        # Get the training and test (assessment) sets using the original function logic\n",
    "        # This function uses the start dates and deltas to select the correct data slices\n",
    "        try:\n",
    "            (train_df, test_df) = get_train_test_set(transactions_df,\n",
    "                                                   start_date_training=start_date_training_fold,\n",
    "                                                   delta_train=delta_train,\n",
    "                                                   delta_delay=delta_delay,\n",
    "                                                   delta_test=delta_assessment)\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERROR calling get_train_test_set for fold {fold}: {e}\")\n",
    "            print(f\"     Skipping fold {fold}.\")\n",
    "            print(\"-\" * 10)\n",
    "            continue # Skip to next fold\n",
    "\n",
    "        # Get the indices from the two sets, and add them to the list of prequential splits\n",
    "        # Check if sets are empty before getting indices\n",
    "        if not train_df.empty and not test_df.empty:\n",
    "            indices_train = list(train_df.index)\n",
    "            indices_test = list(test_df.index)\n",
    "            prequential_split_indices.append((indices_train, indices_test))\n",
    "            print(f\"  -> Train size: {len(indices_train)}, Test size: {len(indices_test)}. Added fold indices.\")\n",
    "        else:\n",
    "             # Use the warning from the original user code\n",
    "             print(f\"  -> Warning (prequentialSplit): Fold {fold} generated empty train ({train_df.shape}) or test ({test_df.shape}) set for start date {start_date_training_fold.strftime('%Y-%m-%d')}. Skipping fold.\")\n",
    "        print(\"-\" * 10) # Separator between folds\n",
    "\n",
    "    if not prequential_split_indices:\n",
    "        # Use the warning from the original user code\n",
    "        print(f\"Warning (prequentialSplit): No valid folds generated for start date {start_date_training.strftime('%Y-%m-%d')} and {n_folds} folds.\")\n",
    "\n",
    "    print(\"--- Finished Generating Prequential Folds ---\")\n",
    "    # Return the original format (list of tuples of indices) for compatibility\n",
    "    return prequential_split_indices\n",
    "def prequential_grid_search(transactions_df,\n",
    "                            classifier,\n",
    "                            input_features, output_feature,\n",
    "                            parameters, scoring,\n",
    "                            start_date_training,\n",
    "                            n_folds=4,\n",
    "                            expe_type='Test',\n",
    "                            delta_train=7,\n",
    "                            delta_delay=7,\n",
    "                            delta_assessment=7,\n",
    "                            performance_metrics_list_grid=['roc_auc'],\n",
    "                            performance_metrics_list=['AUC ROC'],\n",
    "                            n_jobs=-1):\n",
    "\n",
    "    # Input validation\n",
    "    if transactions_df.empty:\n",
    "         print(f\"ERROR (prequential_grid_search): Input transactions_df is empty for {expe_type}.\")\n",
    "         # Return empty DataFrame matching expected structure\n",
    "         cols = [f'{m} {expe_type}' for m in performance_metrics_list] + \\\n",
    "                [f'{m} {expe_type} Std' for m in performance_metrics_list] + \\\n",
    "                ['Parameters', 'Parameters summary', 'Execution time']\n",
    "         return pd.DataFrame(columns=cols)\n",
    "    if not scoring:\n",
    "         print(f\"ERROR (prequential_grid_search): scoring dictionary is empty for {expe_type}.\")\n",
    "         return pd.DataFrame(columns=cols) # Return empty using same structure\n",
    "\n",
    "    estimators = [('scaler', sklearn.preprocessing.StandardScaler()), ('clf', classifier)]\n",
    "    pipe = sklearn.pipeline.Pipeline(estimators)\n",
    "\n",
    "    # prequential_split_indices=prequentialSplit(transactions_df,\n",
    "    #                                            start_date_training=start_date_training,\n",
    "    #                                            n_folds=n_folds,\n",
    "    #                                            delta_train=delta_train,\n",
    "    #                                            delta_delay=delta_delay,\n",
    "    #                                            delta_assessment=delta_assessment)\n",
    "    prequential_split_indices = prequentialSplit_with_dates(transactions_df,\n",
    "                                                        start_date_training=start_date_training,\n",
    "                                                        n_folds=n_folds,\n",
    "                                                        delta_train=delta_train,\n",
    "                                                        delta_delay=delta_delay,\n",
    "                                                        delta_assessment=delta_assessment)\n",
    "\n",
    "    # If no valid splits, return empty df\n",
    "    if not prequential_split_indices:\n",
    "         print(f\"ERROR (prequential_grid_search): No valid prequential splits for {expe_type} starting {start_date_training.strftime('%Y-%m-%d')}. Cannot run GridSearchCV.\")\n",
    "         # Return empty DataFrame matching expected structure\n",
    "         cols = [f'{m} {expe_type}' for m in performance_metrics_list] + \\\n",
    "                [f'{m} {expe_type} Std' for m in performance_metrics_list] + \\\n",
    "                ['Parameters', 'Parameters summary', 'Execution time']\n",
    "         return pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Use refit=False as per original, we only care about CV results here\n",
    "    grid_search = sklearn.model_selection.GridSearchCV(pipe, parameters, scoring=scoring, cv=prequential_split_indices, refit=False, n_jobs=n_jobs, return_train_score=False)\n",
    "\n",
    "    X=transactions_df[input_features]\n",
    "    y=transactions_df[output_feature]\n",
    "\n",
    "    # >>> REMOVED THE EXPLICIT NaN CHECK BLOCK FOR X_fit <<<\n",
    "    # The original notebook relied on the pipeline's scaler to handle this implicitly (or fail).\n",
    "\n",
    "    print(f\"Starting GridSearchCV for {expe_type} set (Classifier: {classifier.__class__.__name__})...\")\n",
    "    try:\n",
    "        # Fit directly on X, pipeline handles scaling.\n",
    "        # StandardScaler will raise ValueError if NaNs are present during fit.\n",
    "        grid_search.fit(X, y)\n",
    "    except ValueError as ve:\n",
    "         if 'Input contains NaN' in str(ve):\n",
    "              print(f\"ERROR (prequential_grid_search): GridSearchCV fit failed for {expe_type} due to NaNs in input data. StandardScaler cannot fit NaNs. Original notebook might have implicitly handled this earlier or used NaN-free data for this step.\")\n",
    "              # You might want to add imputation here *if* you know the original data was clean or if you want to proceed despite NaNs\n",
    "              # Example imputation (use cautiously, deviates from strict original logic):\n",
    "              # print(\"Attempting imputation with mean...\")\n",
    "              # X_imputed = sklearn.impute.SimpleImputer(strategy='mean').fit_transform(X)\n",
    "              # grid_search.fit(X_imputed, y)\n",
    "              return pd.DataFrame(columns=cols) # Return empty if fit fails\n",
    "         else:\n",
    "              print(f\"ERROR (prequential_grid_search): GridSearchCV fit failed for {expe_type}: {ve}\")\n",
    "              import traceback\n",
    "              traceback.print_exc()\n",
    "              return pd.DataFrame(columns=cols) # Return empty if fit fails\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR (prequential_grid_search): GridSearchCV fit failed unexpectedly for {expe_type}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Return empty df if fit fails\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    print(f\"Finished GridSearchCV for {expe_type} set.\")\n",
    "\n",
    "    performances_df=pd.DataFrame()\n",
    "\n",
    "    # Extract results using grid keys, checking existence\n",
    "    cv_results = grid_search.cv_results_\n",
    "    for i in range(len(performance_metrics_list_grid)):\n",
    "        grid_key = performance_metrics_list_grid[i]\n",
    "        display_name = performance_metrics_list[i] # Assumes lists align\n",
    "        mean_score_key = f'mean_test_{grid_key}'\n",
    "        std_score_key = f'std_test_{grid_key}'\n",
    "\n",
    "        if mean_score_key in cv_results:\n",
    "            performances_df[f'{display_name} {expe_type}'] = cv_results[mean_score_key]\n",
    "        else:\n",
    "            print(f\"Warning: Mean score key '{mean_score_key}' not found in cv_results_ for {expe_type}.\")\n",
    "            performances_df[f'{display_name} {expe_type}'] = np.nan\n",
    "\n",
    "        if std_score_key in cv_results:\n",
    "            performances_df[f'{display_name} {expe_type} Std'] = cv_results[std_score_key]\n",
    "        else:\n",
    "            print(f\"Warning: Std score key '{std_score_key}' not found in cv_results_ for {expe_type}.\")\n",
    "            performances_df[f'{display_name} {expe_type} Std'] = np.nan\n",
    "\n",
    "    if 'params' in cv_results:\n",
    "        performances_df['Parameters']=cv_results['params']\n",
    "        # Helper function to convert parameter dict to a readable string summary\n",
    "        def params_to_str(params):\n",
    "            # Ensure params is a dict\n",
    "            if not isinstance(params, dict):\n",
    "                return str(params)\n",
    "            try:\n",
    "                 # Filter out 'clf__' prefix and sort for consistency\n",
    "                 items = [f\"{k.split('__')[1]}={v}\" for k, v in sorted(params.items())]\n",
    "                 return \", \".join(items)\n",
    "            except Exception:\n",
    "                 return str(params) # Fallback\n",
    "        performances_df['Parameters summary'] = performances_df['Parameters'].apply(params_to_str)\n",
    "    else:\n",
    "        print(\"Warning: 'params' key not found in cv_results_. Cannot add parameter columns.\")\n",
    "        # Add empty/NA columns to maintain structure\n",
    "        performances_df['Parameters'] = [{} for _ in range(len(performances_df))] if not performances_df.empty else []\n",
    "        performances_df['Parameters summary'] = 'N/A'\n",
    "\n",
    "    if 'mean_fit_time' in cv_results:\n",
    "        performances_df['Execution time']=cv_results['mean_fit_time']\n",
    "    else:\n",
    "        print(\"Warning: 'mean_fit_time' key not found in cv_results_.\")\n",
    "        performances_df['Execution time'] = np.nan\n",
    "\n",
    "    return performances_df\n",
    "\n",
    "\n",
    "\n",
    "# ### model_selection_wrapper\n",
    "#\n",
    "# First use in [Chapter 5, Model Selection](Model_Selection).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def model_selection_wrapper(transactions_df,\n",
    "                            classifier,\n",
    "                            input_features, output_feature,\n",
    "                            parameters,\n",
    "                            scoring,\n",
    "                            start_date_training_for_valid,\n",
    "                            start_date_training_for_test,\n",
    "                            n_folds=4,\n",
    "                            delta_train=7,\n",
    "                            delta_delay=7,\n",
    "                            delta_assessment=7,\n",
    "                            performance_metrics_list_grid=['roc_auc'],\n",
    "                            performance_metrics_list=['AUC ROC'],\n",
    "                            n_jobs=-1):\n",
    "\n",
    "    # Get performances on the validation set using prequential validation\n",
    "    print(\"--- Running Prequential Grid Search for Validation Set ---\")\n",
    "    performances_df_validation=prequential_grid_search(transactions_df, classifier,\n",
    "                            input_features, output_feature,\n",
    "                            parameters, scoring,\n",
    "                            start_date_training=start_date_training_for_valid,\n",
    "                            n_folds=n_folds,\n",
    "                            expe_type='Validation',\n",
    "                            delta_train=delta_train,\n",
    "                            delta_delay=delta_delay,\n",
    "                            delta_assessment=delta_assessment,\n",
    "                            performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "                            performance_metrics_list=performance_metrics_list,\n",
    "                            n_jobs=n_jobs)\n",
    "\n",
    "    # Get performances on the test set using prequential validation\n",
    "    print(\"--- Running Prequential Grid Search for Test Set Estimation ---\")\n",
    "    performances_df_test=prequential_grid_search(transactions_df, classifier,\n",
    "                            input_features, output_feature,\n",
    "                            parameters, scoring,\n",
    "                            start_date_training=start_date_training_for_test,\n",
    "                            n_folds=n_folds,\n",
    "                            expe_type='Test',\n",
    "                            delta_train=delta_train,\n",
    "                            delta_delay=delta_delay,\n",
    "                            delta_assessment=delta_assessment,\n",
    "                            performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "                            performance_metrics_list=performance_metrics_list,\n",
    "                            n_jobs=n_jobs)\n",
    "\n",
    "    # Bind the two resulting DataFrames\n",
    "    # Merge based on 'Parameters summary' to ensure rows align correctly\n",
    "    if performances_df_test.empty and performances_df_validation.empty:\n",
    "        print(\"Warning (model_selection_wrapper): Both Test and Validation results are empty.\")\n",
    "        return pd.DataFrame()\n",
    "    elif performances_df_test.empty:\n",
    "         print(\"Warning (model_selection_wrapper): Test results are empty. Returning only Validation results.\")\n",
    "         # Return validation results, maybe rename columns for consistency?\n",
    "         return performances_df_validation \n",
    "    elif performances_df_validation.empty:\n",
    "         print(\"Warning (model_selection_wrapper): Validation results are empty. Returning only Test results.\")\n",
    "         return performances_df_test\n",
    "    else:\n",
    "        # Both have results, merge them\n",
    "        # Check if the crucial 'Parameters summary' column exists in both\n",
    "        if 'Parameters summary' not in performances_df_test.columns or 'Parameters summary' not in performances_df_validation.columns:\n",
    "             print(\"ERROR (model_selection_wrapper): 'Parameters summary' column missing. Cannot merge results. Returning Test results only.\")\n",
    "             return performances_df_test\n",
    "             \n",
    "        performances_df_validation_subset = performances_df_validation.drop(columns=['Parameters','Execution time'], errors='ignore') \n",
    "        # Outer merge to keep all parameter sets tested, even if one failed\n",
    "        performances_df=pd.merge(performances_df_test, performances_df_validation_subset, on='Parameters summary', how='outer')\n",
    "\n",
    "    # And return as a single DataFrame\n",
    "    return performances_df\n",
    "\n",
    "\n",
    "# ### kfold_cv_with_classifier\n",
    "#\n",
    "# First use in [Chapter 6, Cost-sensitive learning](Cost_Sensitive_Learning).\n",
    "# NOTE: Not used in this notebook's main flow, kept for original function adherence.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def kfold_cv_with_classifier(classifier,\n",
    "                             X,\n",
    "                             y,\n",
    "                             n_splits=5,\n",
    "                             strategy_name=\"Basline classifier\"):\n",
    "\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    cv_results_=sklearn.model_selection.cross_validate(classifier,X,y,cv=cv,\n",
    "                                                       scoring=['roc_auc',\n",
    "                                                                'average_precision',\n",
    "                                                                'balanced_accuracy'],\n",
    "                                                       return_estimator=True)\n",
    "\n",
    "    results=round(pd.DataFrame(cv_results_),3)\n",
    "    results_mean=list(results.mean().values)\n",
    "    results_std=list(results.std().values)\n",
    "    # Ensure correct indexing for results_mean/std\n",
    "    num_metrics = len(results.columns) - 1 # Exclude 'estimator' column if present\n",
    "    results_df=pd.DataFrame([[str(round(results_mean[i],3))+'+/-'+\n",
    "                              str(round(results_std[i],3)) for i in range(num_metrics)]],\n",
    "                            columns=['Fit time (s)','Score time (s)',\n",
    "                                     'AUC ROC','Average Precision','Balanced accuracy'])\n",
    "    results_df.rename(index={0:strategy_name}, inplace=True)\n",
    "\n",
    "    classifier_0=cv_results_['estimator'][0]\n",
    "\n",
    "    (train_index, test_index) = next(cv.split(X, y))\n",
    "    \n",
    "    # Handle X,y being numpy or pandas\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        train_df=pd.DataFrame({'X1':X.iloc[train_index,0],'X2':X.iloc[train_index,1], 'Y':y.iloc[train_index]})\n",
    "        test_df=pd.DataFrame({'X1':X.iloc[test_index,0],'X2':X.iloc[test_index,1], 'Y':y.iloc[test_index]})\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        train_df=pd.DataFrame({'X1':X[train_index,0],'X2':X[train_index,1], 'Y':y[train_index]})\n",
    "        test_df=pd.DataFrame({'X1':X[test_index,0],'X2':X[test_index,1], 'Y':y[test_index]})\n",
    "    else:\n",
    "        raise TypeError(\"Input X must be a pandas DataFrame or numpy array\")\n",
    "\n",
    "    return (results_df, classifier_0, train_df, test_df)\n",
    "\n",
    "\n",
    "# ## Plotting (Copied from Original)\n",
    "\n",
    "# ### get_tx_stats\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Compute the number of transactions per day, fraudulent transactions per day and fraudulent cards per day\n",
    "\n",
    "def get_tx_stats(transactions_df, start_date_df=\"2018-04-01\"):\n",
    "    \n",
    "    if transactions_df.empty or 'TX_TIME_DAYS' not in transactions_df.columns:\n",
    "        print(\"Warning (get_tx_stats): Input DataFrame is empty or missing TX_TIME_DAYS.\")\n",
    "        return pd.DataFrame(columns=[\"tx_date\", \"nb_tx_per_day\", \"nb_fraudulent_transactions_per_day\", \"nb_compromised_cards_per_day\"])\n",
    "        \n",
    "    #Number of transactions per day\n",
    "    nb_tx_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['CUSTOMER_ID'].count()\n",
    "    #Number of fraudulent transactions per day\n",
    "    nb_fraudulent_transactions_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['TX_FRAUD'].sum()\n",
    "    #Number of fraudulent cards per day\n",
    "    nb_compromised_card_per_day=transactions_df[transactions_df['TX_FRAUD']==1].groupby(['TX_TIME_DAYS']).CUSTOMER_ID.nunique()\n",
    "\n",
    "    tx_stats=pd.DataFrame({\"nb_tx_per_day\":nb_tx_per_day,\n",
    "                           \"nb_fraudulent_transactions_per_day\":nb_fraudulent_transactions_per_day,\n",
    "                           \"nb_compromised_cards_per_day\":nb_compromised_card_per_day})\n",
    "\n",
    "    tx_stats=tx_stats.reset_index()\n",
    "    \n",
    "    # Fill NaN for days where no frauds/compromised cards occurred\n",
    "    tx_stats.fillna(0, inplace=True)\n",
    "\n",
    "    try:\n",
    "        start_date = datetime.datetime.strptime(start_date_df, \"%Y-%m-%d\")\n",
    "        # Use timedelta with days=... for clarity\n",
    "        tx_stats['tx_date'] = tx_stats['TX_TIME_DAYS'].apply(lambda x: start_date + datetime.timedelta(days=x))\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning (get_tx_stats): Error parsing start_date_df '{start_date_df}': {e}. Cannot create 'tx_date' column.\")\n",
    "        tx_stats['tx_date'] = pd.NaT\n",
    "\n",
    "    return tx_stats\n",
    "\n",
    "\n",
    "# ### get_template_tx_stats\n",
    "#\n",
    "# First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Plot the number of transactions per day, fraudulent transactions per day and fraudulent cards per day\n",
    "\n",
    "def get_template_tx_stats(ax ,fs,\n",
    "                          start_date_training,\n",
    "                          title='',\n",
    "                          delta_train=7,\n",
    "                          delta_delay=7,\n",
    "                          delta_test=7,\n",
    "                          ylim=300):\n",
    "\n",
    "    ax.set_title(title, fontsize=fs*1.5)\n",
    "    ax.set_ylim([0, ylim])\n",
    "\n",
    "    ax.set_xlabel('Date', fontsize=fs)\n",
    "    ax.set_ylabel('Number', fontsize=fs)\n",
    "\n",
    "    plt.yticks(fontsize=fs*0.7)\n",
    "    plt.xticks(fontsize=fs*0.7, rotation=45, ha='right') # Added rotation for readability\n",
    "\n",
    "    # Ensure dates are datetime objects\n",
    "    train_end_date = start_date_training + datetime.timedelta(days=delta_train)\n",
    "    delay_end_date = start_date_training + datetime.timedelta(days=delta_train + delta_delay)\n",
    "    test_end_date = delay_end_date + datetime.timedelta(days=delta_test)\n",
    "    \n",
    "    ax.axvline(train_end_date, 0, ylim, color=\"black\", linestyle='--') # Made lines dashed\n",
    "    ax.axvline(delay_end_date, 0, ylim, color=\"black\", linestyle='--')\n",
    "\n",
    "    # Adjust text placement to avoid overlap with lines/data\n",
    "    text_y_pos = ylim * 0.95\n",
    "    ax.text(start_date_training + datetime.timedelta(days=delta_train/2), text_y_pos, 'Training', fontsize=fs, ha='center', va='top')\n",
    "    ax.text(train_end_date + datetime.timedelta(days=delta_delay/2), text_y_pos, 'Delay', fontsize=fs, ha='center', va='top')\n",
    "    ax.text(delay_end_date + datetime.timedelta(days=delta_test/2), text_y_pos, 'Test', fontsize=fs, ha='center', va='top')\n",
    "    \n",
    "    # Improve date formatting on x-axis if possible\n",
    "    try:\n",
    "      ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%Y-%m-%d'))\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Could not set date formatter: {e}\")\n",
    "\n",
    "\n",
    "# ### get_template_roc_curve\n",
    "#\n",
    "# First use in [Chapter 4, Threshold Free Metrics](Threshold_Free_Metrics).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def get_template_roc_curve(ax, title,fs,random=True):\n",
    "\n",
    "    ax.set_title(title, fontsize=fs)\n",
    "    ax.set_xlim([-0.01, 1.01])\n",
    "    ax.set_ylim([-0.01, 1.01])\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=fs)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=fs)\n",
    "\n",
    "    if random:\n",
    "        ax.plot([0, 1], [0, 1],'r--',label=\"AUC ROC Random = 0.5\")\n",
    "    ax.grid(True) # Add grid\n",
    "\n",
    "\n",
    "# ### get_template_pr_curve\n",
    "#\n",
    "# First use in [Chapter 4, Threshold Free Metrics](Threshold_Free_Metrics).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def get_template_pr_curve(ax, title,fs, baseline=0.5):\n",
    "    ax.set_title(title, fontsize=fs)\n",
    "    ax.set_xlim([-0.01, 1.01])\n",
    "    ax.set_ylim([-0.01, 1.01])\n",
    "\n",
    "    ax.set_xlabel('Recall (True Positive Rate)', fontsize=fs)\n",
    "    ax.set_ylabel('Precision', fontsize=fs)\n",
    "\n",
    "    # Baseline calculation should ideally use the actual fraud rate of the dataset being plotted\n",
    "    ax.plot([0, 1], [baseline, baseline],'r--',label='AP Random = {0:0.3f}'.format(baseline))\n",
    "    ax.grid(True) # Add grid\n",
    "\n",
    "\n",
    "# ### get_performance_plot\n",
    "#\n",
    "# First use in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Get the performance plot for a single performance metric\n",
    "def get_performance_plot(performances_df,\n",
    "                         ax,\n",
    "                         performance_metric,\n",
    "                         expe_type_list=['Test','Validation'],\n",
    "                         expe_type_color_list=['#008000','#FF0000'], # Green for Test, Red for Validation\n",
    "                         parameter_name=\"Parameter summary\",\n",
    "                         summary_performances=None):\n",
    "    if performances_df.empty:\n",
    "        print(f\"Skipping plot for {performance_metric}: No data.\")\n",
    "        ax.text(0.5, 0.5, 'No performance data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(performance_metric+'\\n', fontsize=14)\n",
    "        return\n",
    "\n",
    "    # Check for parameter column\n",
    "    if parameter_name not in performances_df.columns:\n",
    "         print(f\"Warning (get_performance_plot): Parameter column '{parameter_name}' not found. Cannot plot.\")\n",
    "         ax.text(0.5, 0.5, f'Missing column:\\n{parameter_name}', ha='center', va='center', transform=ax.transAxes)\n",
    "         ax.set_title(performance_metric+'\\n', fontsize=14)\n",
    "         return\n",
    "         \n",
    "    parameter_summary_col_raw = performances_df[parameter_name]\n",
    "    # Convert parameters to string if they are dicts or lists for plotting\n",
    "    # Use apply(str) for robustness\n",
    "    parameter_summary_col = parameter_summary_col_raw.apply(str)\n",
    "\n",
    "    parameter_ticks = np.arange(len(parameter_summary_col))\n",
    "\n",
    "    # Plot lines and confidence intervals\n",
    "    for i in range(len(expe_type_list)):\n",
    "        expe_type = expe_type_list[i]\n",
    "        performance_metric_expe_type=performance_metric+' '+expe_type\n",
    "        if performance_metric_expe_type not in performances_df.columns:\n",
    "            print(f\"Note: Metric column '{performance_metric_expe_type}' not found. Skipping plot for {expe_type}.\")\n",
    "            continue # Skip if column doesn't exist\n",
    "\n",
    "        # Ensure data is numeric, replace non-numeric with NaN for plotting\n",
    "        perf_data = pd.to_numeric(performances_df[performance_metric_expe_type], errors='coerce')\n",
    "\n",
    "        # Only plot if there is some non-NaN data\n",
    "        if not perf_data.isna().all():\n",
    "            ax.plot(parameter_ticks, perf_data,\n",
    "                    color=expe_type_color_list[i], label = expe_type, marker='o')\n",
    "\n",
    "            std_col = performance_metric_expe_type+' Std'\n",
    "            if std_col in performances_df.columns:\n",
    "                std_data = pd.to_numeric(performances_df[std_col], errors='coerce')\n",
    "                # Check std_data is not all NaN before calculating bounds\n",
    "                if not std_data.isna().all():\n",
    "                    conf_min = perf_data - 2*std_data\n",
    "                    conf_max = perf_data + 2*std_data\n",
    "                    ax.fill_between(parameter_ticks, conf_min, conf_max, color=expe_type_color_list[i], alpha=.1)\n",
    "            else:\n",
    "                 print(f\"Note: Std Dev column '{std_col}' not found for {expe_type}.\")\n",
    "\n",
    "    # Add vertical line for best validation parameter\n",
    "    if summary_performances is not None and not summary_performances.empty and performance_metric in summary_performances.columns:\n",
    "        best_estimated_parameter_str = str(summary_performances.loc[\"Best estimated parameters\", performance_metric])\n",
    "        val_perf_str = summary_performances.loc[\"Validation performance\", performance_metric]\n",
    "        if val_perf_str != 'N/A' and val_perf_str != 'NaN' and best_estimated_parameter_str != 'N/A':\n",
    "            try:\n",
    "                best_estimated_performance=float(val_perf_str.split(\"+/-\")[0])\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                # Find the position of the parameter string for vlines\n",
    "                param_indices = parameter_summary_col[parameter_summary_col == best_estimated_parameter_str].index\n",
    "                if not param_indices.empty:\n",
    "                    param_pos_index = param_indices[0]\n",
    "                    # Check if index is within bounds of parameter_ticks\n",
    "                    if param_pos_index < len(parameter_ticks):\n",
    "                        param_pos = parameter_ticks[param_pos_index]\n",
    "                        ax.vlines(param_pos, ymin, best_estimated_performance, linestyles=\"dashed\", color='red', label='Best Validation Param')\n",
    "                    else:\n",
    "                        print(f\"Warning (get_performance_plot): Index {param_pos_index} out of bounds for parameter ticks.\")\n",
    "                else:\n",
    "                    print(f\"Warning (get_performance_plot): Best parameter '{best_estimated_parameter_str}' not found for plotting vline.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning (get_performance_plot): Error plotting vline: {e}\")\n",
    "\n",
    "    ax.set_title(performance_metric+'\\n', fontsize=14)\n",
    "    ax.set_xlabel(parameter_name, fontsize=12)\n",
    "    ax.set_ylabel(performance_metric, fontsize=12)\n",
    "    ax.set_xticks(parameter_ticks) # Use integer positions for ticks\n",
    "    ax.set_xticklabels(parameter_summary_col, rotation=45, ha='right') # Use original labels\n",
    "    ax.grid(True, axis='y', linestyle=':') # Add y-axis grid\n",
    "\n",
    "    # Handle legend: Collect unique labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles: # Only add legend if there are items to show\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        ax.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "\n",
    "# ### get_performances_plots\n",
    "#\n",
    "# First use in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Get the performance plots for a set of performance metric\n",
    "def get_performances_plots(performances_df,\n",
    "                           performance_metrics_list=['AUC ROC', 'Average precision', 'Card Precision@100'],\n",
    "                           expe_type_list=['Test','Validation'], expe_type_color_list=['#008000','#FF0000'],\n",
    "                           parameter_name=\"Parameter summary\",\n",
    "                           summary_performances=None):\n",
    "    if performances_df.empty:\n",
    "        print(\"Cannot generate plots: Performance dataframe is empty.\")\n",
    "        return None # Return None if no plots generated\n",
    "        \n",
    "    n_performance_metrics = len(performance_metrics_list)\n",
    "    fig, axes = plt.subplots(1, n_performance_metrics, figsize=(6*n_performance_metrics, 6)) # Increased height slightly\n",
    "    if n_performance_metrics == 1:\n",
    "        axes = [axes] # Ensure axes is iterable\n",
    "        \n",
    "    for i in range(n_performance_metrics):\n",
    "        get_performance_plot(performances_df, axes[i], performance_metric=performance_metrics_list[i],\n",
    "                             expe_type_list=expe_type_list,\n",
    "                             expe_type_color_list=expe_type_color_list,\n",
    "                             parameter_name=parameter_name,\n",
    "                             summary_performances=summary_performances)\n",
    "\n",
    "    # Consolidate legends outside the plot area\n",
    "    handles, labels = [], []\n",
    "    for ax in axes:\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        # Filter out potential duplicates if vline label added multiple times\n",
    "        for label, handle in zip(l, h):\n",
    "            if label not in labels:\n",
    "                labels.append(label)\n",
    "                handles.append(handle)\n",
    "                \n",
    "    if handles: # Only add legend if there are items\n",
    "        fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(labels), title=\"Set Type / Marker\")\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95]) # Adjust layout and add padding\n",
    "    # plt.show() # Don't show automatically in MLOps context, return fig\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ### get_execution_times_plot\n",
    "#\n",
    "# First use in [Chapter 5, Validation Strategies](Validation_Strategies).\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Get the performance plot for a single performance metric\n",
    "def get_execution_times_plot(performances_df,\n",
    "                             title=\"Mean Fit Time per Parameter Set\", # Adjusted title\n",
    "                             parameter_name=\"Parameter summary\"):\n",
    "    \n",
    "    if performances_df.empty or 'Execution time' not in performances_df.columns or parameter_name not in performances_df.columns:\n",
    "        print(\"Cannot generate execution time plot: Required data missing.\")\n",
    "        return None\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8, 5)) # Adjusted size slightly\n",
    "\n",
    "    # Ensure data is numeric\n",
    "    exec_time_data = pd.to_numeric(performances_df[\"Execution time\"], errors='coerce')\n",
    "    parameter_summary_col = performances_df[parameter_name].apply(str)\n",
    "    parameter_ticks = np.arange(len(parameter_summary_col))\n",
    "\n",
    "    # Plot data on graph only if not all NaN\n",
    "    if not exec_time_data.isna().all():\n",
    "        ax.plot(parameter_ticks, exec_time_data, color=\"black\", marker='o')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No execution time data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "    # Set title, and x and y axes labels\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set(xlabel = parameter_name, ylabel=\"Mean Fit Time (seconds)\")\n",
    "    # Use integer positions for ticks and original labels\n",
    "    ax.set_xticks(parameter_ticks)\n",
    "    ax.set_xticklabels(parameter_summary_col, rotation=45, ha='right')\n",
    "    ax.grid(True, axis='y', linestyle=':') # Add y-axis grid\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ### get_model_selection_performances_plots (Plotting function)\n",
    "#\n",
    "# First use in [Chapter 5, Model Selection](Model_Selection).\n",
    "# Renamed from original to avoid conflict\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# ### get_model_selection_performances_plots (Plotting function)\n",
    "#\n",
    "# First use in [Chapter 5, Model Selection](Model_Selection).\n",
    "# Renamed from original to avoid conflict\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Get the performance plot for a single performance metric comparison\n",
    "def plot_model_selection_performance_bar(performances_df_dictionary,\n",
    "                                         ax,\n",
    "                                         performance_metric,\n",
    "                                         ylim=[0,1],\n",
    "                                         model_classes=['Decision Tree', 'Logistic Regression', 'Random Forest', 'XGBoost']):\n",
    "\n",
    "    # ** ALIGNED WITH ORIGINAL NOTEBOOK DEFAULTS USED IN COMPARISON PLOT **\n",
    "    default_parameters_dict_for_lookup={\n",
    "         \"Decision Tree\": 'max_depth=None, random_state=0',\n",
    "         \"Logistic Regression\": 'C=1.0, random_state=0',\n",
    "         \"Random Forest\": \"max_depth=None, n_estimators=100, n_jobs=1, random_state=0\",\n",
    "         \"XGBoost\": \"eval_metric=logloss, learning_rate=0.3, max_depth=6, n_estimators=100, n_jobs=1, random_state=0, use_label_encoder=False, verbosity=0\"\n",
    "     }\n",
    "\n",
    "    # Get the mean/std performance data using the helper function\n",
    "    (mean_performances_dictionary,std_performances_dictionary) = model_selection_performances(\n",
    "                                     performances_df_dictionary=performances_df_dictionary,\n",
    "                                     performance_metric=performance_metric,\n",
    "                                     model_classes=model_classes,\n",
    "                                     default_parameters_dictionary=default_parameters_dict_for_lookup\n",
    "                                     )\n",
    "\n",
    "    barWidth = 0.25\n",
    "    r = np.arange(len(model_classes))\n",
    "    r1 = r - barWidth\n",
    "    r2 = r\n",
    "    r3 = r + barWidth\n",
    "\n",
    "    # Handle potential NaNs when plotting and getting errors\n",
    "    # pd.to_numeric might return numpy arrays\n",
    "    default_means = pd.to_numeric(mean_performances_dictionary.get('Default parameters', [np.nan]*len(model_classes)), errors='coerce')\n",
    "    default_stds = pd.to_numeric(std_performances_dictionary.get('Default parameters', [np.nan]*len(model_classes)), errors='coerce')\n",
    "    best_val_means = pd.to_numeric(mean_performances_dictionary.get('Best validation parameters', [np.nan]*len(model_classes)), errors='coerce')\n",
    "    best_val_stds = pd.to_numeric(std_performances_dictionary.get('Best validation parameters', [np.nan]*len(model_classes)), errors='coerce')\n",
    "    optimal_means = pd.to_numeric(mean_performances_dictionary.get('Optimal parameters', [np.nan]*len(model_classes)), errors='coerce')\n",
    "    optimal_stds = pd.to_numeric(std_performances_dictionary.get('Optimal parameters', [np.nan]*len(model_classes)), errors='coerce')\n",
    "\n",
    "    # *** CORRECTED LINES ***\n",
    "    # Replace NaN std devs with 0 using np.nan_to_num for plotting error bars\n",
    "    default_stds = np.nan_to_num(default_stds, nan=0.0)\n",
    "    best_val_stds = np.nan_to_num(best_val_stds, nan=0.0)\n",
    "    optimal_stds = np.nan_to_num(optimal_stds, nan=0.0)\n",
    "    # *** END CORRECTION ***\n",
    "\n",
    "    ax.bar(r1, default_means, width=barWidth, color='#CA8035', edgecolor='black',\n",
    "           yerr=default_stds, capsize=5, label='Default parameters')\n",
    "    ax.bar(r2, best_val_means, width=barWidth, color='#008000', edgecolor='black',\n",
    "           yerr=best_val_stds, capsize=5, label='Best validation parameters')\n",
    "    ax.bar(r3, optimal_means, width=barWidth, color='#2F4D7E', edgecolor='black',\n",
    "           yerr=optimal_stds, capsize=5, label='Optimal parameters')\n",
    "\n",
    "    ax.set_ylim(ylim[0],ylim[1])\n",
    "    ax.set_xticks(r) # Center ticks on the groups\n",
    "    ax.set_xticklabels(model_classes, rotation = 45, ha=\"right\", fontsize=12)\n",
    "    ax.set_title(performance_metric+'\\n', fontsize=16)\n",
    "    ax.set_ylabel(performance_metric, fontsize=14)\n",
    "    ax.grid(True, axis='y', linestyle=':') # Add y-axis grid\n",
    "\n",
    "\n",
    "\n",
    "# ### plot_decision_boundary_classifier\n",
    "#\n",
    "# First use in [Chapter 6, Cost-sensitive learning](Cost_Sensitive_Learning).\n",
    "# NOTE: Only works for 2 input features, not used in main flow here.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def plot_decision_boundary_classifier(ax,\n",
    "                                      classifier,\n",
    "                                      train_df,\n",
    "                                      input_features=['X1','X2'],\n",
    "                                      output_feature='Y',\n",
    "                                      title=\"\",\n",
    "                                      fs=14,\n",
    "                                      plot_training_data=True):\n",
    "\n",
    "    # Check if exactly 2 input features are provided\n",
    "    if len(input_features) != 2:\n",
    "        print(\"Warning (plot_decision_boundary): Can only plot decision boundary for exactly 2 input features.\")\n",
    "        ax.text(0.5, 0.5, 'Plot requires 2 features', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(title, fontsize=fs)\n",
    "        return\n",
    "        \n",
    "    plot_colors = [\"tab:blue\",\"tab:orange\"]\n",
    "\n",
    "    x1_min, x1_max = train_df[input_features[0]].min() - 1, train_df[input_features[0]].max() + 1\n",
    "    x2_min, x2_max = train_df[input_features[1]].min() - 1, train_df[input_features[1]].max() + 1\n",
    "\n",
    "    plot_step=0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x1_min, x1_max, plot_step),\n",
    "                         np.arange(x2_min, x2_max, plot_step))\n",
    "\n",
    "    # Predict probabilities if possible\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        Z = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "    else:\n",
    "        Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu_r, alpha=0.3)\n",
    "\n",
    "    if plot_training_data:\n",
    "        # Plot the training points\n",
    "        groups = train_df.groupby(output_feature)\n",
    "        for name, group in groups:\n",
    "            ax.scatter(group[input_features[0]], group[input_features[1]], edgecolors='black', label=str(name), s=20) # Smaller points\n",
    "\n",
    "    ax.set_title(title, fontsize=fs)\n",
    "    ax.set_xlabel(input_features[0], fontsize=fs)\n",
    "    ax.set_ylabel(input_features[1], fontsize=fs)\n",
    "    ax.grid(True, linestyle=':')\n",
    "    \n",
    "    # Add color bar if predict_proba was used\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        cbar = plt.colorbar(cs, ax=ax)\n",
    "        cbar.set_label('Predicted Fraud Probability', rotation=270, labelpad=15)\n",
    "\n",
    "\n",
    "# ### plot_decision_boundary\n",
    "#\n",
    "# First use in [Chapter 6, Cost-sensitive learning](Cost_Sensitive_Learning).\n",
    "# NOTE: Only works for 2 input features, not used in main flow here.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def plot_decision_boundary(classifier_0,\n",
    "                           train_df,\n",
    "                           test_df):\n",
    "                           \n",
    "    # Assume input features are the first two columns if not specified\n",
    "    input_features_plot = list(train_df.columns[:2])\n",
    "    output_feature_plot = train_df.columns[2] if len(train_df.columns) > 2 else 'Y'\n",
    "    \n",
    "    if len(input_features_plot) != 2:\n",
    "         print(\"Warning (plot_decision_boundary): Cannot plot, requires exactly 2 features.\")\n",
    "         return None\n",
    "         \n",
    "    fig_decision_boundary, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    plot_decision_boundary_classifier(ax[0], classifier_0,\n",
    "                                      train_df,\n",
    "                                      input_features=input_features_plot,\n",
    "                                      output_feature=output_feature_plot,\n",
    "                                      title=\"Decision surface\\n With training data\",\n",
    "                                      plot_training_data=True)\n",
    "\n",
    "    plot_decision_boundary_classifier(ax[1], classifier_0,\n",
    "                                      train_df, # Use train_df for boundary calculation\n",
    "                                      input_features=input_features_plot,\n",
    "                                      output_feature=output_feature_plot,\n",
    "                                      title=\"Decision surface\\n\",\n",
    "                                      plot_training_data=False)\n",
    "\n",
    "\n",
    "    plot_decision_boundary_classifier(ax[2], classifier_0,\n",
    "                                      test_df,\n",
    "                                      input_features=input_features_plot,\n",
    "                                      output_feature=output_feature_plot,\n",
    "                                      title=\"Decision surface\\n With test data\",\n",
    "                                      plot_training_data=True)\n",
    "\n",
    "    # Consolidate legend based on the plot with data points\n",
    "    handles, labels = ax[2].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig_decision_boundary.legend(handles, labels, loc='center right',\n",
    "                                   bbox_to_anchor=(0.98, 0.5), # Adjust position\n",
    "                                   title=\"Class\")\n",
    "\n",
    "    # Color bar - associated with the contourf plot (cs)\n",
    "    # We need to retrieve 'cs' from one of the plots, e.g., the middle one\n",
    "    # Re-run the middle plot to capture the contour set 'cs'\n",
    "    x1_min, x1_max = train_df[input_features_plot[0]].min() - 1, train_df[input_features_plot[0]].max() + 1\n",
    "    x2_min, x2_max = train_df[input_features_plot[1]].min() - 1, train_df[input_features_plot[1]].max() + 1\n",
    "    plot_step=0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x1_min, x1_max, plot_step), np.arange(x2_min, x2_max, plot_step))\n",
    "    if hasattr(classifier_0, \"predict_proba\"):\n",
    "        Z = classifier_0.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        cs = ax[1].contourf(xx, yy, Z, cmap=plt.cm.RdYlBu_r, alpha=0.3) # Re-plot to get cs\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlBu_r, norm=cs.norm)\n",
    "        sm.set_array([]) # Needed for colorbar\n",
    "        # Place colorbar relative to the figure, not a specific axis\n",
    "        cax = fig_decision_boundary.add_axes([0.93, 0.15, 0.02, 0.7]) # Position: [left, bottom, width, height]\n",
    "        fig_decision_boundary.colorbar(sm, cax=cax, label='Predicted Fraud Probability')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to prevent overlap with colorbar/legend\n",
    "\n",
    "    return fig_decision_boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlops_start_run_markdown_aligned"
   },
   "source": [
    "## 1. Start MLflow Run & Define Parameters (Aligned with Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlops_start_run_code_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Start Parent MLflow Run for the entire notebook execution ---\n",
    "mlflow_run = mlflow.start_run(run_name=\"final_MLOps_Model_Building_Selection_Run\") # Adjusted name\n",
    "run_id = mlflow_run.info.run_id\n",
    "print(f\"Started MLflow Run ID: {run_id}\")\n",
    "if mlflow.get_tracking_uri().startswith(\"azureml\"): # Check if tracking Azure ML\n",
    "    # Construct Azure ML portal URL (replace with your specific details if needed)\n",
    "    # Assuming standard Azure public cloud\n",
    "    azure_portal_url = f\"https://ml.azure.com/experiments/guid/{mlflow_run.info.experiment_id}/runs/{run_id}?wsid=/subscriptions/{ws.subscription_id}/resourcegroups/{ws.resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws.name}\"\n",
    "    print(f\"MLflow Run Azure ML UI: {azure_portal_url}\")\n",
    "else:\n",
    "    print(f\"MLflow Run local UI: http://localhost:5000/#/experiments/{mlflow_run.info.experiment_id}/runs/{run_id}\") # Assuming default local port\n",
    "\n",
    "# --- Log Tags and Environment Info ---\n",
    "mlflow.set_tag(\"Workflow Step\", \"Model Building & Selection (Aligned)\")\n",
    "# mlflow.set_tag(\"Git Commit\", GIT_COMMIT)\n",
    "# mlflow.set_tag(\"Git Repo URL\", GIT_REPO_URL)\n",
    "mlflow.set_tag(\"Python Version\", sys.version.split('|')[0].strip())\n",
    "mlflow.set_tag(\"MLflow Version\", mlflow.__version__)\n",
    "mlflow.set_tag(\"Scikit-learn Version\", sklearn.__version__)\n",
    "mlflow.set_tag(\"XGBoost Version\", xgboost.__version__)\n",
    "if ws:\n",
    "    mlflow.set_tag(\"Azure ML Workspace\", ws.name)\n",
    "\n",
    "# --- Log Input Data Information ---\n",
    "if input_dataset:\n",
    "    mlflow.set_tag(\"Input Dataset Name\", input_dataset.name)\n",
    "    mlflow.set_tag(\"Input Dataset Version\", input_dataset.version)\n",
    "    mlflow.set_tag(\"Input Dataset ID\", input_dataset.id)\n",
    "else:\n",
    "    mlflow.set_tag(\"Input Dataset Name\", \"ERROR - Not Loaded\")\n",
    "    print(\"WARNING: Input dataset object not available for MLflow logging.\")\n",
    "\n",
    "# --- Define and Log Core Parameters (ALIGNED WITH ORIGINAL NOTEBOOK cell 86e7ebefca3940668a4b03082ff390e8) ---\n",
    "\n",
    "# Define Data Loading Range (Matches original notebook)\n",
    "BEGIN_DATE_LOAD = \"2025-06-11\"\n",
    "END_DATE_LOAD = \"2025-08-14\"\n",
    "\n",
    "# Define Training / Validation / Test Periods Parameters (Matches original notebook)\n",
    "DELTA_TRAIN = 7\n",
    "DELTA_DELAY = 7\n",
    "DELTA_ASSESSMENT = 7 # Used for validation and test period lengths\n",
    "DELTA_VALID = DELTA_ASSESSMENT # Original used this implicitly\n",
    "DELTA_TEST = DELTA_ASSESSMENT # Original used this implicitly\n",
    "N_FOLDS = 4 # Number of folds for prequential validation\n",
    "TOP_K_VALUE = 100 # For Card Precision@k metric (used in original)\n",
    "\n",
    "# Define the _anchor_ training start date (Matches original notebook)\n",
    "START_DATE_TRAINING_ANCHOR_STR = \"2025-07-25\"\n",
    "start_date_training_anchor = datetime.datetime.strptime(START_DATE_TRAINING_ANCHOR_STR, \"%Y-%m-%d\")\n",
    "\n",
    "# Calculate start dates relative to the anchor (Matches original notebook calculations)\n",
    "start_date_training_for_valid = start_date_training_anchor - datetime.timedelta(days=(DELTA_DELAY + DELTA_VALID)) # Used for model_selection_wrapper Validation\n",
    "start_date_training_for_test_estimation = start_date_training_anchor # Used for model_selection_wrapper Test\n",
    "\n",
    "\n",
    "\n",
    "# **REVISED Final Split Dates based on original notebook's Step 3/4 usage:**\n",
    "start_date_training_final = start_date_training_anchor # Use anchor date as start for final training\n",
    "start_date_test_final = start_date_training_final + datetime.timedelta(days=(DELTA_TRAIN + DELTA_DELAY))\n",
    "\n",
    "# Log parameters to MLflow\n",
    "mlflow.log_param(\"data_load_begin_date\", BEGIN_DATE_LOAD)\n",
    "mlflow.log_param(\"data_load_end_date\", END_DATE_LOAD)\n",
    "mlflow.log_param(\"delta_train\", DELTA_TRAIN)\n",
    "mlflow.log_param(\"delta_delay\", DELTA_DELAY)\n",
    "mlflow.log_param(\"delta_assessment\", DELTA_ASSESSMENT)\n",
    "mlflow.log_param(\"prequential_n_folds\", N_FOLDS)\n",
    "mlflow.log_param(\"card_precision_top_k\", TOP_K_VALUE)\n",
    "mlflow.log_param(\"anchor_date_train_start\", START_DATE_TRAINING_ANCHOR_STR)\n",
    "# Log the actual start dates used for validation/test estimation loops\n",
    "mlflow.log_param(\"validation_gridsearch_start_date\", start_date_training_for_valid.strftime('%Y-%m-%d'))\n",
    "mlflow.log_param(\"test_estimation_gridsearch_start_date\", start_date_training_for_test_estimation.strftime('%Y-%m-%d'))\n",
    "# Log the final split dates\n",
    "mlflow.log_param(\"final_train_start_date\", start_date_training_final.strftime('%Y-%m-%d'))\n",
    "mlflow.log_param(\"final_test_start_date\", start_date_test_final.strftime('%Y-%m-%d'))\n",
    "\n",
    "print(f\"Anchor Training Start Date: {start_date_training_anchor.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Validation GridSearch Start (Fold 0 Train Start): {start_date_training_for_valid.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Test Estimation GridSearch Start (Fold 0 Train Start): {start_date_training_for_test_estimation.strftime('%Y-%m-%d')}\")\n",
    "print(f\"--- FINAL SPLIT DATES ---\")\n",
    "print(f\"Final Training Set Start Date (for Step 5): {start_date_training_final.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Final Test Set Start Date (for Step 6): {start_date_test_final.strftime('%Y-%m-%d')}\")\n",
    "print(f\"-------------------------\")\n",
    "\n",
    "# Define Features (Matches original notebook cell 86e7ebefca3940668a4b03082ff390e8)\n",
    "OUTPUT_FEATURE = \"TX_FRAUD\"\n",
    "INPUT_FEATURES = ['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "                  'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "                  'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "                  'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
    "                  'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
    "                  'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
    "                  'TERMINAL_ID_RISK_30DAY_WINDOW']\n",
    "\n",
    "mlflow.log_param(\"output_feature\", OUTPUT_FEATURE)\n",
    "# Log input features as a list (log as JSON string for MLflow)\n",
    "mlflow.log_param(\"input_features\", json.dumps(INPUT_FEATURES))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mlflow.end_run()\n",
    "# print(f\"\\nFinished and closed MLflow Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "64fd24fa4f8b4394b5c71bf2b32338b3_aligned",
    "deepnote_cell_type": "markdown",
    "id": "KQZClzM6DSZD_aligned"
   },
   "source": [
    "## 2. Load Versioned Data (Transformed Features)\n",
    "\n",
    "Load the pre-processed data generated by the feature engineering notebook. Assumes `dvc pull` has been run if necessary to ensure the correct version (tracked by `simulated-data-transformed.dvc`) is present locally in the path defined by `TRANSFORMED_DATA_LOCAL_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "86e7ebefca3940668a4b03082ff390e8_aligned_load",
    "deepnote_cell_type": "code",
    "id": "Su7y-4JWDSZE_aligned_load",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "transactions_df = pd.DataFrame() # Initialize\n",
    "load_start_time = time.time()\n",
    "\n",
    "DIR_INPUT = None # Will be set inside the 'with' block\n",
    "\n",
    "try:\n",
    "    # Mount the dataset to a temporary local path\n",
    "    with input_dataset.mount() as mount_context:\n",
    "        DIR_INPUT = mount_context.mount_point # Set DIR_INPUT to the mounted path\n",
    "        print(f\"Dataset mounted temporarily at: {DIR_INPUT}\")\n",
    "        print(f\"\\nLoading data from {DIR_INPUT} between {BEGIN_DATE_LOAD} and {END_DATE_LOAD}...\")\n",
    "        # --- YOUR EXACT CODE BLOCK STARTS HERE ---\n",
    "        # Use the read_from_files function copied from the original notebook\n",
    "        transactions_df = read_from_files(DIR_INPUT, BEGIN_DATE_LOAD, END_DATE_LOAD)\n",
    "        load_exec_time = time.time() - load_start_time\n",
    "\n",
    "        if transactions_df.empty:\n",
    "            print(\"ERROR: Loaded DataFrame is empty. Check input path, date range, and file contents.\")\n",
    "            mlflow.set_tag(\"Data Loading Status\", \"Failed - Empty DataFrame\")\n",
    "            # Stop execution if data loading fails\n",
    "            raise SystemExit(\"Stopping execution: Failed to load transaction data.\")\n",
    "        else:\n",
    "            print(f\"{len(transactions_df)} transactions loaded in {load_exec_time:.2f}s, containing {transactions_df.TX_FRAUD.sum()} fraudulent transactions.\")\n",
    "            mlflow.log_metric(\"loaded_data_rows\", len(transactions_df))\n",
    "            mlflow.log_metric(\"loaded_data_fraud_count\", transactions_df.TX_FRAUD.sum())\n",
    "            mlflow.log_metric(\"loaded_data_load_time_sec\", load_exec_time)\n",
    "            mlflow.set_tag(\"Data Loading Status\", \"Completed - Success\")\n",
    "            # Basic data validation check (using required columns from original functions)\n",
    "            required_cols = INPUT_FEATURES + [OUTPUT_FEATURE, 'TX_DATETIME', 'TX_TIME_DAYS', 'CUSTOMER_ID', 'TRANSACTION_ID']\n",
    "            missing_cols = [col for col in required_cols if col not in transactions_df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"ERROR: Loaded data missing required columns: {missing_cols}\")\n",
    "                mlflow.set_tag(\"Data Validation\", f\"Failed - Missing columns: {missing_cols}\")\n",
    "                raise SystemExit(f\"Stopping execution: Missing required columns {missing_cols}\")\n",
    "            else:\n",
    "                 mlflow.set_tag(\"Data Validation\", \"Passed - Required columns present\")\n",
    "                 # Check data types (optional but good practice)\n",
    "                 if not pd.api.types.is_datetime64_any_dtype(transactions_df['TX_DATETIME']):\n",
    "                      print(\"Warning: TX_DATETIME column is not datetime type after loading.\")\n",
    "                      mlflow.set_tag(\"Data Validation Warning\", \"TX_DATETIME type incorrect\")\n",
    "        # --- YOUR EXACT CODE BLOCK ENDS HERE ---\n",
    "\n",
    "# --- Exception Handling (kept outside the 'with' but catches errors from inside) ---\n",
    "except SystemExit as e:\n",
    "    print(e)\n",
    "    # Optionally end MLflow run if stopping and active\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run(status=\"FAILED\")\n",
    "    # Re-raise to ensure notebook stops\n",
    "    raise\n",
    "except Exception as e:\n",
    "    # This will catch errors from input_dataset.mount() OR from your code block inside the 'with'\n",
    "    print(f\"ERROR during data loading or mounting: {e}\")\n",
    "    mlflow.set_tag(\"Data Loading Status\", f\"Failed - Exception: {type(e).__name__}\")\n",
    "    mlflow.log_param(\"Error Message\", str(e)) # Log the error message\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    # Stop execution and mark run as failed\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run(status=\"FAILED\")\n",
    "    raise SystemExit(f\"Stopping execution: Error during data loading: {e}\")\n",
    "\n",
    "\n",
    "# --- Display head if loaded successfully ---\n",
    "if not transactions_df.empty:\n",
    "    print(\"\\nLoaded Data Head:\")\n",
    "    # Use display if available (Jupyter/IPython), otherwise print\n",
    "    try:\n",
    "        display(transactions_df.head())\n",
    "    except NameError:\n",
    "        print(transactions_df.head())\n",
    "\n",
    "# --- Optional: End MLflow run if this is the *only* step in the run ---\n",
    "# if mlflow.active_run():\n",
    "#     mlflow.end_run(status=\"FINISHED\") # Mark as finished if successful to this point\n",
    "\n",
    "# --- The 'transactions_df' is now ready for subsequent steps (splitting, training) ---\n",
    "print(\"\\nData loading and validation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "define_final_split_markdown_aligned",
    "deepnote_cell_type": "markdown",
    "id": "define_final_split_markdown_aligned"
   },
   "source": [
    "## 3. Define Final Training and Test Sets (Aligned with Original)\n",
    "\n",
    "We create the specific final train/test split based on the **exact dates and logic** from the original notebook (`start_date_training_final`, `start_date_test_final`). This split will be used to train the _final selected_ model (Step 5) and evaluate its performance on unseen data (Step 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_final_split_code_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_final = pd.DataFrame()\n",
    "test_df_final = pd.DataFrame()\n",
    "\n",
    "if not transactions_df.empty:\n",
    "    print(f\"\\nCreating final train/test split (using function from original notebook)...\")\n",
    "    print(f\"Final Train Start: {start_date_training_final.strftime('%Y-%m-%d')}, Delta: {DELTA_TRAIN} days\")\n",
    "    # Note: DELTA_TEST here refers to the duration of the final test period, which is DELTA_ASSESSMENT\n",
    "    print(f\"Final Test Start: {start_date_test_final.strftime('%Y-%m-%d')}, Delta: {DELTA_TEST} days, Delay: {DELTA_DELAY} days\")\n",
    "    \n",
    "    try:\n",
    "        # Use the get_train_test_set function copied from the original notebook\n",
    "        (train_df_final, test_df_final) = get_train_test_set(transactions_df,\n",
    "                                                            start_date_training=start_date_training_final,\n",
    "                                                            delta_train=DELTA_TRAIN,\n",
    "                                                            delta_delay=DELTA_DELAY,\n",
    "                                                            delta_test=DELTA_TEST) # delta_test is same as delta_assessment\n",
    "        \n",
    "        print(f\"\\nFinal training set shape: {train_df_final.shape}\")\n",
    "        print(f\"Final test set shape: {test_df_final.shape}\")\n",
    "        mlflow.log_metric(\"final_train_set_rows\", train_df_final.shape[0] if not train_df_final.empty else 0)\n",
    "        mlflow.log_metric(\"final_test_set_rows\", test_df_final.shape[0] if not test_df_final.empty else 0)\n",
    "        \n",
    "        if not train_df_final.empty:\n",
    "            final_train_fraud_rate = train_df_final[OUTPUT_FEATURE].mean()\n",
    "            print(f\"Final training set fraud rate: {final_train_fraud_rate:.4f}\")\n",
    "            mlflow.log_metric(\"final_train_set_fraud_rate\", final_train_fraud_rate)\n",
    "        else:\n",
    "             print(\"ERROR: Final training set is empty.\")\n",
    "             mlflow.set_tag(\"Final Split Status\", \"Failed - Empty Train Set\")\n",
    "             raise SystemExit(\"Stopping execution: Final training set is empty.\")\n",
    "             \n",
    "        if not test_df_final.empty:\n",
    "            final_test_fraud_rate = test_df_final[OUTPUT_FEATURE].mean()\n",
    "            print(f\"Final test set fraud rate: {final_test_fraud_rate:.4f}\")\n",
    "            mlflow.log_metric(\"final_test_set_fraud_rate\", final_test_fraud_rate)\n",
    "        else:\n",
    "            # An empty test set might be acceptable depending on the period, but flag it.\n",
    "            print(\"Warning: Final test set is empty.\")\n",
    "            mlflow.set_tag(\"Final Split Status\", \"Warning - Empty Test Set\")\n",
    "        \n",
    "        if not train_df_final.empty and not test_df_final.empty:\n",
    "             mlflow.set_tag(\"Final Split Status\", \"Success\")\n",
    "             \n",
    "    except SystemExit as e:\n",
    "        print(e)\n",
    "        raise # Re-raise to stop notebook\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating final train/test split: {e}\")\n",
    "        mlflow.set_tag(\"Final Split Status\", f\"Failed - Exception: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise SystemExit(f\"Stopping execution: Error creating final split: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping final train/test split creation as loaded data is empty.\")\n",
    "    raise SystemExit(\"Stopping execution: Cannot proceed without loaded data.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3b4caebe1cd149c49d625522251af738_aligned",
    "deepnote_cell_type": "markdown",
    "id": "Z1o7WM3QDSZG_aligned"
   },
   "source": [
    "## 4. Model Selection Process (Aligned with Original)\n",
    "\n",
    "### 4a. Define Candidate Models & Hyperparameters (Copied from Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e53d98b98f3844519b3f24ef04220d6c_aligned",
    "deepnote_cell_type": "code",
    "id": "R0pw_LTTDSZG_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define candidate classifiers (Copied EXACTLY from original notebook cell e53d98b98f3844519b3f24ef04220d6c)\n",
    "classifiers_dictionary={\n",
    "    'Logistic Regression':sklearn.linear_model.LogisticRegression(solver='liblinear', random_state=0),\n",
    "    'Decision Tree':sklearn.tree.DecisionTreeClassifier(random_state=0),\n",
    "    'Random Forest':sklearn.ensemble.RandomForestClassifier(random_state=0, n_jobs=1), # Original used n_jobs=1\n",
    "    'XGBoost':xgboost.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss', n_jobs=1) # Original used n_jobs=1\n",
    "}\n",
    "\n",
    "# Define hyperparameter grids (Copied EXACTLY from original notebook cell e53d98b98f3844519b3f24ef04220d6c)\n",
    "# Note the correction made in the original cell for XGBoost verbosity was `verbosity=0` (no comma)\n",
    "parameters_dictionary={\n",
    "    'Logistic Regression': {\n",
    "        'clf__C':[0.1, 1, 10, 100],\n",
    "        'clf__random_state':[0] # Keep for reproducibility if needed by solver\n",
    "        },\n",
    "    'Decision Tree': {\n",
    "        'clf__max_depth':[2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50],\n",
    "        'clf__random_state':[0]\n",
    "        },\n",
    "    'Random Forest': {\n",
    "        'clf__n_estimators': [25, 50, 100],\n",
    "        'clf__max_depth': [5, 10, 20, 50],\n",
    "        'clf__random_state':[0],\n",
    "        'clf__n_jobs':[1] # Explicitly set as in original\n",
    "        },\n",
    "    'XGBoost': {\n",
    "        'clf__max_depth': [3, 6, 9],\n",
    "        'clf__n_estimators': [25, 50, 100],\n",
    "        'clf__learning_rate': [0.1, 0.3],\n",
    "        'clf__random_state':[0],\n",
    "        'clf__n_jobs':[1], # Explicitly set as in original\n",
    "        'clf__verbosity':[0], # Original definition\n",
    "        'clf__use_label_encoder':[False], # Explicitly set as in original definition\n",
    "        'clf__eval_metric':['logloss']     # Explicitly set as in original definition\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Parameter Grids (Aligned with Original Notebook):\")\n",
    "for model, params in parameters_dictionary.items():\n",
    "    # Print relevant tuning parameters only for clarity\n",
    "    print_params = {k:v for k,v in params.items() if k not in ['clf__random_state', 'clf__n_jobs', 'clf__verbosity', 'clf__use_label_encoder', 'clf__eval_metric']}\n",
    "    print(f\"  {model}: {print_params}\")\n",
    "\n",
    "# Log the parameter grids as a JSON artifact for reference\n",
    "params_log_path = \"parameter_grids_aligned.json\"\n",
    "try:\n",
    "    # Convert non-serializable items (like None type in DT/RF max_depth if it were used) to strings for JSON\n",
    "    # Here, the grids only contain basic types, but this is safer general practice\n",
    "    def make_serializable(val):\n",
    "        if isinstance(val, list):\n",
    "            return [make_serializable(item) for item in val]\n",
    "        if val is None:\n",
    "            return 'None'\n",
    "        # Add other types if needed (e.g., np.nan)\n",
    "        return val\n",
    "        \n",
    "    serializable_params = {model: {k: make_serializable(v) for k, v in p.items()} \n",
    "                           for model, p in parameters_dictionary.items()}\n",
    "    with open(params_log_path, 'w') as f:\n",
    "        json.dump(serializable_params, f, indent=4)\n",
    "    mlflow.log_artifact(params_log_path)\n",
    "    print(f\"\\nAligned parameter grids logged to MLflow artifact: {params_log_path}\")\n",
    "except Exception as e:\n",
    "     print(f\"Warning: Could not log parameter grids artifact: {e}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e3a6fd66f5cb41b1bb4ca895fe90f45f_aligned",
    "deepnote_cell_type": "markdown",
    "id": "hsA7kj3NDSZG_aligned"
   },
   "source": [
    "### 4b & 4c. Define Validation Strategy (Prequential) & Metrics (Aligned with Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "515762f3dad6410da1375f633164bd06_aligned",
    "deepnote_cell_type": "code",
    "id": "GzixLk-_DSZH_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prequential split parameters (N_FOLDS, DELTA_TRAIN, etc.) are defined and logged above\n",
    "\n",
    "# Create the scorer dataframe subset _only if_ data was loaded\n",
    "transactions_df_scorer = pd.DataFrame() # Initialize\n",
    "if not transactions_df.empty:\n",
    "    # Only keep columns needed as argument to the custom scoring function\n",
    "    # (to reduce the serialization time of transaction dataset in GridSearchCV)\n",
    "    try:\n",
    "        # Use columns required by card_precision_top_k_custom -> card_precision_top_k\n",
    "        scorer_cols = ['CUSTOMER_ID', 'TX_FRAUD','TX_TIME_DAYS']\n",
    "        missing_scorer_cols = [col for col in scorer_cols if col not in transactions_df.columns]\n",
    "        if missing_scorer_cols:\n",
    "            print(f\"ERROR: Missing columns required for scorer DF: {missing_scorer_cols}\")\n",
    "            mlflow.set_tag(\"Scorer Setup\", f\"Failed - Missing columns: {missing_scorer_cols}\")\n",
    "        else:\n",
    "            transactions_df_scorer = transactions_df[scorer_cols].copy()\n",
    "            print(f\"Created scorer helper DataFrame with shape: {transactions_df_scorer.shape}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: Missing required columns for scorer DataFrame: {e}\")\n",
    "        mlflow.set_tag(\"Scorer Setup\", f\"Failed - Missing columns: {e}\")\n",
    "else:\n",
    "    print(\"Error: transactions_df is empty, cannot create scorer helper DataFrame.\")\n",
    "    mlflow.set_tag(\"Scorer Setup\", \"Failed - Empty Input Data\")\n",
    "    # Cannot proceed without scorer df if CP@k is used\n",
    "    raise SystemExit(\"Stopping: Cannot create scorer dataframe.\")\n",
    "\n",
    "# Define the custom scorer for Card Precision@k using the function from original notebook\n",
    "card_precision_top_k_scorer = None\n",
    "if not transactions_df_scorer.empty:\n",
    "    try:\n",
    "        # The make_scorer call uses the card_precision_top_k_custom function defined in the shared functions cell\n",
    "        card_precision_top_k_scorer = sklearn.metrics.make_scorer(card_precision_top_k_custom,\n",
    "                                                                  needs_proba=True,\n",
    "                                                                  top_k=TOP_K_VALUE,\n",
    "                                                                  transactions_df=transactions_df_scorer)\n",
    "        print(f\"Custom scorer 'card_precision@{TOP_K_VALUE}' created successfully.\")\n",
    "        mlflow.set_tag(\"Scorer Setup\", \"Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating custom scorer: {e}\")\n",
    "        mlflow.set_tag(\"Scorer Setup\", f\"Failed - Scorer Creation Error: {e}\")\n",
    "        # Optionally raise an error if this scorer is essential\n",
    "        # raise SystemExit(\"Stopping: Failed to create CP@k scorer.\")\n",
    "\n",
    "# Define the scoring dictionary for GridSearchCV (Aligned EXACTLY with original cell 515762f3dad6410da1375f633164bd06)\n",
    "scoring = {'roc_auc':'roc_auc',\n",
    "           'average_precision': 'average_precision',\n",
    "           # Only add the custom scorer if it was created successfully\n",
    "           **({f'card_precision@{TOP_K_VALUE}': card_precision_top_k_scorer} if card_precision_top_k_scorer else {}) \n",
    "           }\n",
    "\n",
    "# Check if the custom scorer is actually in the dict\n",
    "if f'card_precision@{TOP_K_VALUE}' not in scoring:\n",
    "     print(f\"Warning: Custom scorer 'card_precision@{TOP_K_VALUE}' was not added to the scoring dict. Check for creation errors.\")\n",
    "     # You might want to stop if this metric is critical\n",
    "     # raise SystemExit(\"Stopping: CP@k scorer is missing but required.\")\n",
    "\n",
    "# List versions for accessing results later and for logging (Aligned EXACTLY with original cell 515762f3dad6410da1375f633164bd06)\n",
    "performance_metrics_list_grid = list(scoring.keys()) # Keys used in GridSearchCV\n",
    "performance_metrics_list = ['AUC ROC', 'Average precision'] # Display names corresponding to keys\n",
    "if f'card_precision@{TOP_K_VALUE}' in scoring:\n",
    "    performance_metrics_list.append(f'Card Precision@{TOP_K_VALUE}')\n",
    "\n",
    "# Define primary metric for model selection (Aligned with original choice in cell 515762f3dad6410da1375f633164bd06)\n",
    "primary_metric = 'Average precision' # Original notebook used Average Precision\n",
    "primary_metric_grid_key = 'average_precision' # Key used in scoring dict\n",
    "\n",
    "print(f\"Validation Strategy: Prequential with {N_FOLDS} folds\")\n",
    "print(f\"Performance Metrics for GridSearch: {performance_metrics_list_grid}\")\n",
    "print(f\"Performance Metrics for Reporting: {performance_metrics_list}\")\n",
    "print(f\"Primary Metric for Selection: {primary_metric}\")\n",
    "\n",
    "# Log scoring setup\n",
    "mlflow.log_param(\"scoring_metrics_grid\", json.dumps(performance_metrics_list_grid))\n",
    "mlflow.log_param(\"scoring_metrics_report\", json.dumps(performance_metrics_list))\n",
    "mlflow.log_param(\"primary_selection_metric\", primary_metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a33f6e7a7431419380609f5fe03c2369_aligned",
    "deepnote_cell_type": "markdown",
    "id": "y2QCTiQCDSZI_aligned"
   },
   "source": [
    "### 4d. Train & Assess Candidates via Prequential Validation (Aligned with Original)\n",
    "\n",
    "*(MLOps Note: This section iterates through each classifier type. For each, it runs the `model_selection_wrapper` (copied from original) which performs `prequential_grid_search` (copied from original) for both validation and test estimation periods using the original dates and parameters. Results are stored and logged to MLflow within nested runs.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ba86b8ad0b674311b0c155c1ce8f09e9_aligned",
    "deepnote_cell_type": "code",
    "id": "Z_hNgWoIDSZI_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell ID: Z_hNgWoIDSZI_aligned\n",
    "# Section 4d: Train & Assess Candidates via Prequential Validation (Aligned with Original)\n",
    "\n",
    "performances_df_dictionary = {}\n",
    "model_selection_times = {}\n",
    "total_selection_start_time = time.time()\n",
    "\n",
    "if transactions_df.empty or not scoring:\n",
    "    print(\"ERROR: Skipping model selection as transactions_df is empty or scoring dictionary is not defined.\")\n",
    "    mlflow.set_tag(\"Model Selection Status\", \"Skipped - No Data or Scoring\")\n",
    "    # Stop execution if this step is critical\n",
    "    raise SystemExit(\"Stopping: Cannot perform model selection.\")\n",
    "else:\n",
    "    for classifier_name in classifiers_dictionary:\n",
    "        print(f\"\\n===== Running Model Selection for: {classifier_name} ====\")\n",
    "\n",
    "        # Start a nested run for this classifier's grid search\n",
    "        with mlflow.start_run(run_name=f\"GridSearch_{classifier_name}_Aligned\", nested=True) as nested_run:\n",
    "            nested_run_id = nested_run.info.run_id\n",
    "            print(f\"  Starting Nested MLflow Run ID: {nested_run_id}\")\n",
    "            mlflow.set_tag(\"Classifier\", classifier_name)\n",
    "            # Log the specific parameter grid being searched\n",
    "            try:\n",
    "                 # Use the make_serializable helper if defined earlier, otherwise basic conversion\n",
    "                 def make_serializable(val):\n",
    "                     if isinstance(val, list): return [make_serializable(item) for item in val]\n",
    "                     if val is None: return 'None'\n",
    "                     return val\n",
    "                 loggable_params = {k: make_serializable(v) for k, v in parameters_dictionary[classifier_name].items()}\n",
    "                 mlflow.log_param(\"parameter_grid\", json.dumps(loggable_params))\n",
    "            except Exception as log_e:\n",
    "                 print(f\"  Warning: Could not log parameter grid for {classifier_name}: {log_e}\")\n",
    "\n",
    "            classifier = classifiers_dictionary[classifier_name]\n",
    "            parameters = parameters_dictionary[classifier_name]\n",
    "\n",
    "            start_time_clf = time.time()\n",
    "\n",
    "            try:\n",
    "                # Call the wrapper function COPIED from the original notebook\n",
    "                # Ensure model_selection_wrapper uses the corrected prequential_grid_search\n",
    "                performances_df = model_selection_wrapper(transactions_df, classifier,\n",
    "                                                          INPUT_FEATURES, OUTPUT_FEATURE,\n",
    "                                                          parameters, scoring, # Use the EXACT grids/scoring from original\n",
    "                                                          start_date_training_for_valid, # Use the EXACT start date from original\n",
    "                                                          start_date_training_for_test_estimation, # Use the EXACT start date from original\n",
    "                                                          n_folds=N_FOLDS,\n",
    "                                                          delta_train=DELTA_TRAIN,\n",
    "                                                          delta_delay=DELTA_DELAY,\n",
    "                                                          delta_assessment=DELTA_ASSESSMENT,\n",
    "                                                          performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "                                                          performance_metrics_list=performance_metrics_list,\n",
    "                                                          n_jobs=5 # Use multiple cores; original used 5\n",
    "                                                         )\n",
    "\n",
    "                clf_execution_time = time.time()-start_time_clf\n",
    "                model_selection_times[classifier_name] = clf_execution_time\n",
    "                print(f\"  Finished {classifier_name} selection in {clf_execution_time:.2f} seconds\")\n",
    "                mlflow.log_metric(\"model_selection_time_sec\", clf_execution_time)\n",
    "\n",
    "                if performances_df.empty:\n",
    "                    print(f\"  Warning: No performance results generated for {classifier_name} by model_selection_wrapper.\")\n",
    "                    mlflow.set_tag(\"Grid Search Status\", \"Completed - No Results\")\n",
    "                    performances_df_dictionary[classifier_name] = pd.DataFrame() # Store empty df\n",
    "                else:\n",
    "                    # Ensure 'Parameters summary' column exists after wrapper call\n",
    "                    if 'Parameters summary' not in performances_df.columns:\n",
    "                        print(\"  Warning: 'Parameters summary' column missing after model_selection_wrapper. Cannot log detailed results correctly.\")\n",
    "                        # Attempt to create it if 'Parameters' column exists\n",
    "                        if 'Parameters' in performances_df.columns:\n",
    "                             def params_to_str_fallback(params):\n",
    "                                 try:\n",
    "                                     # Ensure params is a dict before processing\n",
    "                                     if isinstance(params, dict):\n",
    "                                         items = [f\"{k.split('__')[1]}={v}\" for k, v in sorted(params.items())]\n",
    "                                         return \", \".join(items)\n",
    "                                     else:\n",
    "                                         return str(params)\n",
    "                                 except Exception: # Catch potential errors during string formatting\n",
    "                                     return str(params)\n",
    "                             performances_df['Parameters summary'] = performances_df['Parameters'].apply(params_to_str_fallback)\n",
    "                             print(\"  Created fallback 'Parameters summary' column.\")\n",
    "                        else:\n",
    "                             mlflow.set_tag(\"Grid Search Status\", \"Completed - Missing Param Summary\")\n",
    "                    else:\n",
    "                        mlflow.set_tag(\"Grid Search Status\", \"Completed - Success\")\n",
    "\n",
    "                    # Log detailed results per hyperparameter combination as artifact\n",
    "                    perf_artifact_path = f\"{classifier_name}_grid_search_results_aligned.csv\"\n",
    "                    try:\n",
    "                        performances_df.to_csv(perf_artifact_path, index=False)\n",
    "                        mlflow.log_artifact(perf_artifact_path)\n",
    "                        print(f\"  Logged detailed grid search results to {perf_artifact_path}\")\n",
    "                    except Exception as art_e:\n",
    "                         print(f\"  Warning: Failed to log artifact {perf_artifact_path}: {art_e}\")\n",
    "\n",
    "                # Store potentially modified df (with fallback summary col)\n",
    "                performances_df_dictionary[classifier_name]=performances_df\n",
    "\n",
    "            except SystemExit:\n",
    "                print(f\"SystemExit occurred during model selection for {classifier_name}. Stopping.\")\n",
    "                raise # Re-raise to stop the notebook\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during model selection wrapper for {classifier_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                mlflow.set_tag(\"Grid Search Status\", f\"Failed - Exception: {e}\")\n",
    "                # Store empty df to avoid errors later\n",
    "                performances_df_dictionary[classifier_name] = pd.DataFrame()\n",
    "\n",
    "# Cell ID: Z_hNgWoIDSZI_aligned (End of the cell)\n",
    "\n",
    "print(f\"\\nTotal Model Selection Time: {(time.time()-total_selection_start_time):.2f} seconds\")\n",
    "mlflow.log_metric(\"total_model_selection_time_sec\", time.time()-total_selection_start_time)\n",
    "\n",
    "# Check if any grid search failed before marking as completed\n",
    "print(\"Checking status of nested grid search runs...\")\n",
    "all_statuses = []\n",
    "try:\n",
    "    # Fetch ALL runs under the parent first\n",
    "    nested_runs = mlflow.search_runs(\n",
    "        experiment_ids=[mlflow.active_run().info.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.parentRunId = '{run_id}'\", # Only filter by parent ID\n",
    "        output_format=\"list\"\n",
    "    )\n",
    "\n",
    "    # Filter in Python for the correct run names and get their status\n",
    "    all_statuses = [\n",
    "        run.data.tags.get(\"Grid Search Status\", \"Unknown\")\n",
    "        for run in nested_runs\n",
    "        if run.data.tags.get(\"mlflow.runName\", \"\").startswith(\"GridSearch_\") # Filter names here\n",
    "    ]\n",
    "    print(f\"Found {len(all_statuses)} relevant nested runs with statuses: {all_statuses}\")\n",
    "\n",
    "except Exception as search_e:\n",
    "    print(f\"ERROR searching for nested runs: {search_e}\")\n",
    "    # Mark parent status as unknown or failed due to inability to check children\n",
    "    mlflow.set_tag(\"Model Selection Status\", \"Unknown - Failed to Check Nested Runs\")\n",
    "\n",
    "# Set parent status based on children (only if statuses were retrieved)\n",
    "if all_statuses:\n",
    "    if any(\"Failed\" in status for status in all_statuses):\n",
    "        mlflow.set_tag(\"Model Selection Status\", \"Completed with Failures\")\n",
    "    elif all(status == \"Completed - Success\" or status.startswith(\"Completed - \") for status in all_statuses): # Be more lenient for success variations\n",
    "        mlflow.set_tag(\"Model Selection Status\", \"Completed Successfully\")\n",
    "    else: # Handle cases like \"No Results\" or missing summaries\n",
    "         mlflow.set_tag(\"Model Selection Status\", \"Completed with Issues/Warnings\")\n",
    "elif 'nested_runs' in locals() and not nested_runs: # Search succeeded but found no runs\n",
    "    mlflow.set_tag(\"Model Selection Status\", \"Completed - No Nested Runs Found\")\n",
    "# Else: Status already set if search failed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f4c018f9a11b48da9efd8d10e80d7f5b_aligned",
    "deepnote_cell_type": "markdown",
    "id": "UVeKgomgDSZI_aligned"
   },
   "source": [
    "### 4d.1 Review Execution Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7d295985b8034b60acdedcc935022419_aligned",
    "deepnote_cell_type": "code",
    "id": "p2OXmPN9DSZJ_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_selection_times:\n",
    "    exec_times_df = pd.DataFrame.from_dict(model_selection_times, orient='index', columns=['Total Selection Time (s)'])\n",
    "    exec_times_df = exec_times_df.reset_index().rename(columns={'index': 'Model'})\n",
    "    print(\"\\nModel Selection Execution Times:\")\n",
    "    display(exec_times_df)\n",
    "    # Log this summary table as an artifact\n",
    "    exec_times_path = \"model_selection_times_aligned.csv\"\n",
    "    try:\n",
    "        exec_times_df.to_csv(exec_times_path, index=False)\n",
    "        mlflow.log_artifact(exec_times_path)\n",
    "        print(f\"Execution times summary logged to {exec_times_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Warning: Failed to log execution times artifact: {e}\")\n",
    "else:\n",
    "    print(\"No execution times recorded (model selection likely skipped or failed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6bb7852358f9413791b9e6eb8daf8644_aligned",
    "deepnote_cell_type": "markdown",
    "id": "2XE3Kge2DSZJ_aligned"
   },
   "source": [
    "### 4d.2 Review Performance Summaries and Plots (Aligned with Original)\n",
    "\n",
    "*(MLOps Note: Generate summaries and plots using functions copied from original. Plots are logged to MLflow.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4b38c37135fb486c898b690be1998bfd_aligned",
    "deepnote_cell_type": "code",
    "id": "fFE6IDMQDSZK_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_summaries = {}\n",
    "for model_name, perf_df in performances_df_dictionary.items():\n",
    "    print(f\"\\n===== Summary for {model_name} (Aligned) ====\")\n",
    "    if not perf_df.empty:\n",
    "        # Ensure 'Parameters summary' column exists before calling summary function\n",
    "        if 'Parameters summary' not in perf_df.columns:\n",
    "             print(f\"  Error: 'Parameters summary' column missing for {model_name}. Cannot generate summary or plots.\")\n",
    "             continue\n",
    "             \n",
    "        # Use the get_summary_performances function COPIED from the original notebook\n",
    "        summary = get_summary_performances(perf_df, parameter_column_name=\"Parameters summary\")\n",
    "        display(summary)\n",
    "        all_summaries[model_name] = summary\n",
    "        \n",
    "        # Generate and log performance plots using function COPIED from original\n",
    "        print(\"\\n  Performance Plots:\")\n",
    "        fig_perf = get_performances_plots(perf_df,\n",
    "                                          performance_metrics_list=performance_metrics_list, # Use the aligned list\n",
    "                                          parameter_name=\"Parameters summary\", # Use the correct column name\n",
    "                                          summary_performances=summary # Pass summary for vline\n",
    "                                         )\n",
    "        if fig_perf:\n",
    "            try:\n",
    "                plt.show() # Show plot in notebook\n",
    "                mlflow.log_figure(fig_perf, f\"plots/{model_name}_performance_curves_aligned.png\")\n",
    "                plt.close(fig_perf) # Close figure to free memory\n",
    "                print(f\"  Logged performance plots for {model_name}.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"  Warning: Failed to log performance plot for {model_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"  Could not generate performance plots for {model_name}.\")\n",
    "            \n",
    "        # Generate and log execution time plot using function COPIED from original\n",
    "        fig_time = get_execution_times_plot(perf_df, \n",
    "                                            title=f\"{model_name} Mean Fit Time (Aligned)\", \n",
    "                                            parameter_name=\"Parameters summary\") # Use correct column name\n",
    "        if fig_time:\n",
    "            try:\n",
    "                plt.show()\n",
    "                mlflow.log_figure(fig_time, f\"plots/{model_name}_execution_time_aligned.png\")\n",
    "                plt.close(fig_time)\n",
    "                print(f\"  Logged execution time plot for {model_name}.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"  Warning: Failed to log execution time plot for {model_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"  Could not generate execution time plot for {model_name}.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"  No performance data available for {model_name} to display or plot.\")\n",
    "\n",
    "# Log all summaries combined as an artifact (optional)\n",
    "if all_summaries:\n",
    "    combined_summary_path = \"all_model_summaries_aligned.json\"\n",
    "    try:\n",
    "        # Convert DataFrames to dicts for JSON serialization\n",
    "        serializable_summaries = {k: v.to_dict() for k, v in all_summaries.items()}\n",
    "        with open(combined_summary_path, 'w') as f:\n",
    "            json.dump(serializable_summaries, f, indent=4)\n",
    "        mlflow.log_artifact(combined_summary_path)\n",
    "        print(f\"\\nCombined model summaries logged to {combined_summary_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Warning: Failed to log combined summaries artifact: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "66be5ac210164e2888b1ac310aecf22a_aligned",
    "deepnote_cell_type": "markdown",
    "id": "lVb0PTBBDSZK_aligned"
   },
   "source": [
    "### 4d.3 Compare Model Performances (Aligned with Original)\n",
    "\n",
    "*(MLOps Note: Generate comparison plot using function copied from original and log it.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f0bc0572d2a44ee99a5e858cbc835e61_aligned",
    "deepnote_cell_type": "markdown",
    "id": "exFhgDGGDSZK_aligned"
   },
   "source": [
    "### 4e. Select Best Model and Hyperparameters (Aligned with Original)\n",
    "\n",
    "_(MLOps Note: Select the best model based on the primary validation metric (`Average precision`) using summaries generated by the original function. Log the chosen model name, its parameters, and the corresponding validation score.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "51d0a201b6dd420e99975a9adfa76d17_aligned",
    "deepnote_cell_type": "code",
    "id": "pKTGWxgLDSZK_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_name = None\n",
    "best_model_params_dict = None # The actual dictionary for set_params\n",
    "best_model_params_summary_str = None # The string summary for reference\n",
    "best_validation_score = -np.inf\n",
    "\n",
    "# Use the primary metric defined earlier (aligned with original)\n",
    "print(f\"\\n===== Selecting Best Model based on Validation {primary_metric} (Aligned) ====\")\n",
    "\n",
    "if not all_summaries:\n",
    "    print(\"ERROR: Cannot select best model: No summary data available from get_summary_performances.\")\n",
    "    mlflow.set_tag(\"Best Model Selection Status\", \"Failed - No Summaries\")\n",
    "else:\n",
    "    for model_name, summary_df in all_summaries.items():\n",
    "        # Check if the primary metric column exists in the summary generated by the original function\n",
    "        if primary_metric in summary_df.columns:\n",
    "            # Extract the validation score and parameter string from the summary\n",
    "            validation_perf_str = summary_df.loc['Validation performance', primary_metric]\n",
    "            best_param_summary_str_candidate = summary_df.loc['Best estimated parameters', primary_metric]\n",
    "\n",
    "            # Check if validation performance is valid ('N/A' or 'NaN' indicate issues)\n",
    "            if isinstance(validation_perf_str, str) and validation_perf_str not in ['N/A', 'NaN']:\n",
    "                try:\n",
    "                    # Extract the numeric score part\n",
    "                    current_validation_score = float(validation_perf_str.split('+/-')[0])\n",
    "                    print(f\"  {model_name}: Validation {primary_metric} = {current_validation_score:.4f} (Params Summary: {best_param_summary_str_candidate})\")\n",
    "\n",
    "                    # Compare with the current best score\n",
    "                    if current_validation_score > best_validation_score:\n",
    "                        best_validation_score = current_validation_score\n",
    "                        best_model_name = model_name\n",
    "                        best_model_params_summary_str = best_param_summary_str_candidate\n",
    "                        \n",
    "                        # Find the full parameter dict corresponding to the best summary string\n",
    "                        # Need the original performances_df for this model\n",
    "                        perf_df = performances_df_dictionary.get(model_name)\n",
    "                        if perf_df is not None and 'Parameters summary' in perf_df.columns and 'Parameters' in perf_df.columns:\n",
    "                            best_row = perf_df[perf_df['Parameters summary'] == best_param_summary_str_candidate]\n",
    "                            if not best_row.empty:\n",
    "                                best_model_params_dict = best_row['Parameters'].iloc[0]\n",
    "                                if not isinstance(best_model_params_dict, dict):\n",
    "                                     print(f\"  Warning: Retrieved best parameters for {model_name} is not a dictionary: {best_model_params_dict}\")\n",
    "                                     best_model_params_dict = None # Reset if not dict\n",
    "                            else:\n",
    "                                print(f\"  Warning: Could not find parameter dictionary for best summary string '{best_param_summary_str_candidate}' in {model_name}'s performance df.\")\n",
    "                                best_model_params_dict = None # Reset if dict not found\n",
    "                        else:\n",
    "                             print(f\"  Warning: Could not retrieve performance df or required columns for {model_name} to find best params dict.\")\n",
    "                             best_model_params_dict = None # Reset if cannot find dict\n",
    "                             \n",
    "                except ValueError as ve:\n",
    "                    print(f\"  {model_name}: Could not parse validation score '{validation_perf_str}': {ve}\")\n",
    "                except Exception as e:\n",
    "                     print(f\"  Error processing {model_name} during selection: {e}\")\n",
    "            else:\n",
    "                # Handles 'N/A' or 'NaN'\n",
    "                print(f\"  {model_name}: Validation {primary_metric} = {validation_perf_str} (Skipped for best model comparison)\")\n",
    "        else:\n",
    "            print(f\"  {model_name}: Primary metric '{primary_metric}' not found in summary columns.\")\n",
    "\n",
    "    # Log results of selection\n",
    "    if best_model_name and best_model_params_dict is not None:\n",
    "        print(f\"\\nSelected Best Model: {best_model_name}\")\n",
    "        print(f\"Best Validation {primary_metric}: {best_validation_score:.4f}\")\n",
    "        print(f\"Best Hyperparameters Dict: {best_model_params_dict}\")\n",
    "        print(f\"Best Hyperparameters Summary Str: {best_model_params_summary_str}\")\n",
    "        mlflow.set_tag(\"best_model_name\", best_model_name)\n",
    "        mlflow.log_metric(f\"best_validation_{primary_metric_grid_key}\", best_validation_score)\n",
    "        # Log the best parameters (remove 'clf__' prefix for clarity in MLflow UI)\n",
    "        try:\n",
    "            final_params_to_log = {k.split('__', 1)[1]: make_serializable(v) for k, v in best_model_params_dict.items()}\n",
    "            mlflow.log_params(final_params_to_log)\n",
    "            mlflow.set_tag(\"Best Model Selection Status\", \"Success\")\n",
    "        except Exception as log_e:\n",
    "             print(f\"Warning: Could not log best parameters: {log_e}\")\n",
    "             mlflow.set_tag(\"Best Model Selection Status\", \"Success - Param Logging Failed\")\n",
    "             \n",
    "    else:\n",
    "        # Fallback logic similar to original if no clear winner\n",
    "        print(f\"\\nCould not determine the best model based on validation {primary_metric}.\")\n",
    "        # Original defaulted to XGBoost\n",
    "        print(\"Defaulting to XGBoost with its best validated parameters (if available) or library defaults.\")\n",
    "        mlflow.set_tag(\"Best Model Selection Status\", \"Failed - Defaulting to XGBoost\")\n",
    "        best_model_name = 'XGBoost'\n",
    "        best_model_params_dict = None # Reset\n",
    "        best_model_params_summary_str = 'Default Fallback'\n",
    "        \n",
    "        # Try to get XGBoost's best params from its summary\n",
    "        if 'XGBoost' in all_summaries and primary_metric in all_summaries['XGBoost'].columns:\n",
    "             xgb_summary = all_summaries['XGBoost']\n",
    "             best_param_summary_str_xgb = xgb_summary.loc['Best estimated parameters', primary_metric]\n",
    "             xgb_perf_df = performances_df_dictionary.get('XGBoost')\n",
    "             if xgb_perf_df is not None and 'Parameters summary' in xgb_perf_df.columns and 'Parameters' in xgb_perf_df.columns:\n",
    "                  best_row_xgb = xgb_perf_df[xgb_perf_df['Parameters summary'] == best_param_summary_str_xgb]\n",
    "                  if not best_row_xgb.empty:\n",
    "                       best_model_params_dict = best_row_xgb['Parameters'].iloc[0]\n",
    "                       best_model_params_summary_str = best_param_summary_str_xgb\n",
    "                       print(f\"Using best validated XGBoost params: {best_model_params_summary_str}\")\n",
    "                  \n",
    "        # If still no params, use the library defaults defined earlier\n",
    "        if best_model_params_dict is None:\n",
    "            print(\"Warning: Could not find best validated XGBoost params. Using library defaults defined in parameters_dictionary.\")\n",
    "            best_model_params_dict = parameters_dictionary['XGBoost']\n",
    "            # Create summary string for defaults\n",
    "            try:\n",
    "                 best_model_params_summary_str = \", \".join([f\"{k.split('__')[1]}={v}\" for k, v in sorted(best_model_params_dict.items())])\n",
    "            except:\n",
    "                 best_model_params_summary_str = str(best_model_params_dict)\n",
    "                 \n",
    "        # Log the fallback choice\n",
    "        mlflow.set_tag(\"best_model_name\", best_model_name)\n",
    "        if best_model_params_dict:\n",
    "            try:\n",
    "                final_params_to_log = {k.split('__', 1)[1]: make_serializable(v) for k, v in best_model_params_dict.items()}\n",
    "                mlflow.log_params(final_params_to_log)\n",
    "            except Exception as log_e:\n",
    "                 print(f\"Warning: Could not log default XGBoost parameters: {log_e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9057b661492f40a29e99ee7937cfcedc_aligned",
    "deepnote_cell_type": "markdown",
    "id": "UAgEUR6WDSZL_aligned"
   },
   "source": [
    "## 5. Train Final Model (Aligned with Original)\n",
    "\n",
    "Train the selected model (`best_model_name`) with its `best_model_params_dict` on the full final training set (`train_df_final`) using the **exact dates and data split** from the original notebook.\n",
    "\n",
    "*(MLOps Note: The fitted pipeline (including scaler and classifier with best parameters) will be logged to MLflow and registered in Azure ML.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3526a7ea5f614a1b8d87d1e48318a2c5_aligned",
    "deepnote_cell_type": "code",
    "id": "l7jnHHsmDSZL_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell ID: l7jnHHsmDSZL_aligned\n",
    "# Section 5: Train Final Model (Aligned with Original)\n",
    "\n",
    "final_pipeline = None # Initialize\n",
    "\n",
    "if best_model_name and best_model_params_dict is not None:\n",
    "    print(f\"\\n===== Training Final Model: {best_model_name} (Aligned) ====\")\n",
    "\n",
    "    if train_df_final.empty:\n",
    "        print(\"ERROR: Final training data (train_df_final) is empty. Cannot train final model.\")\n",
    "        mlflow.set_tag(\"Final Model Training Status\", \"Failed - Empty Train Data\")\n",
    "        # Stop execution\n",
    "        raise SystemExit(\"Stopping: Cannot train final model on empty data.\")\n",
    "    else:\n",
    "        # 1. Get the base classifier instance - Clone to avoid modifying the dictionary\n",
    "        try:\n",
    "             final_classifier_base = sklearn.base.clone(classifiers_dictionary[best_model_name])\n",
    "        except Exception as clone_e:\n",
    "             print(f\"ERROR: Could not clone classifier '{best_model_name}': {clone_e}\")\n",
    "             raise SystemExit(\"Stopping: Classifier cloning failed.\")\n",
    "\n",
    "        # 2. Prepare the final parameters (remove 'clf__' prefix)\n",
    "        final_params = {k.split('__', 1)[1]: v for k, v in best_model_params_dict.items() if k.startswith('clf__')}\n",
    "\n",
    "        # 3. Set the parameters on the classifier instance, filtering invalid ones\n",
    "        try:\n",
    "            valid_params_keys = final_classifier_base.get_params().keys()\n",
    "            final_params_filtered = {k: v for k, v in final_params.items() if k in valid_params_keys}\n",
    "            # Handle potential None strings coming from JSON logging/retrieval if applicable\n",
    "            for k, v in final_params_filtered.items():\n",
    "                if isinstance(v, str) and v.lower() == 'none':\n",
    "                    final_params_filtered[k] = None\n",
    "            print(f\"  Using parameters: {final_params_filtered}\")\n",
    "            final_classifier_base.set_params(**final_params_filtered)\n",
    "            mlflow.set_tag(\"Final Model Params Used\", \"Best Validated\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error setting parameters for {best_model_name}: {e}\")\n",
    "            print(\"  Proceeding with default parameters for the final model.\")\n",
    "            # Reset to default if params cause error\n",
    "            final_classifier_base = sklearn.base.clone(classifiers_dictionary[best_model_name])\n",
    "            mlflow.set_tag(\"Final Model Params Used\", \"Defaults due to error\")\n",
    "\n",
    "        # 4. Create the final pipeline (Scaler + Classifier)\n",
    "        final_pipeline = sklearn.pipeline.Pipeline([\n",
    "            ('scaler', sklearn.preprocessing.StandardScaler()),\n",
    "            ('clf', final_classifier_base)\n",
    "        ])\n",
    "\n",
    "        # 5. Fit the final pipeline on the final training data\n",
    "        print(f\"  Fitting final model on training data shape: {train_df_final.shape}\")\n",
    "        start_fit_time = time.time()\n",
    "\n",
    "        # Handle potential NaNs in the final training data BEFORE fitting\n",
    "        # **ALIGNED WITH ORIGINAL NOTEBOOK's Step 3 (cell l7jnHHsmDSZL): Impute with mean**\n",
    "        train_df_final_imputed = train_df_final.copy()\n",
    "\n",
    "        # *** CORRECTED NaN CHECK HERE ***\n",
    "        if train_df_final_imputed[INPUT_FEATURES].isnull().values.any():\n",
    "            print(\"  Warning: NaNs detected in final training data. Imputing with mean (matching original Step 3 logic).\")\n",
    "            # Fit imputer on training data only\n",
    "            imputer_final_train = sklearn.impute.SimpleImputer(strategy='mean')\n",
    "            train_df_final_imputed[INPUT_FEATURES] = imputer_final_train.fit_transform(train_df_final_imputed[INPUT_FEATURES])\n",
    "            mlflow.set_tag(\"Final Train Imputation\", \"Mean Imputed\")\n",
    "            # Save the imputer if needed for consistent test imputation later\n",
    "            # joblib.dump(imputer_final_train, 'final_train_imputer.joblib')\n",
    "        else:\n",
    "             mlflow.set_tag(\"Final Train Imputation\", \"Not Required\")\n",
    "        # *** END CORRECTION ***\n",
    "\n",
    "        try:\n",
    "            final_pipeline.fit(train_df_final_imputed[INPUT_FEATURES], train_df_final_imputed[OUTPUT_FEATURE])\n",
    "            final_fit_time = time.time() - start_fit_time\n",
    "            print(f\"  Final model fitting completed in {final_fit_time:.2f} seconds.\")\n",
    "            mlflow.log_metric(\"final_model_train_time_sec\", final_fit_time)\n",
    "            mlflow.set_tag(\"Final Model Training Status\", \"Success\")\n",
    "\n",
    "            # 6. Log the fitted pipeline (model) to MLflow & Register in AML\n",
    "            print(f\"  Logging final pipeline to MLflow and registering as '{MODEL_NAME_AML}'...\")\n",
    "            # Define conda environment for the model\n",
    "            try:\n",
    "                sklearn_version = sklearn.__version__\n",
    "                xgboost_version = xgboost.__version__\n",
    "                pandas_version = pd.__version__\n",
    "                numpy_version = np.__version__\n",
    "                mlflow_version = mlflow.__version__\n",
    "            except AttributeError:\n",
    "                 print(\"Warning: Could not get all library versions for conda env.\")\n",
    "                 sklearn_version = \"1.1.3\" \n",
    "                 xgboost_version = \"1.7.5\" \n",
    "                 pandas_version = \"1.5.3\" \n",
    "                 numpy_version = \"1.23.5\"\n",
    "                 mlflow_version = \"2.9.2\" \n",
    "\n",
    "            conda_env = {\n",
    "                'channels': ['defaults', 'conda-forge'],\n",
    "                'dependencies': [\n",
    "                    f'python={sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}',\n",
    "                    'pip',\n",
    "                    {\n",
    "                        'pip': [\n",
    "                            f'mlflow=={mlflow_version}',\n",
    "                            f'scikit-learn=={sklearn_version}',\n",
    "                            f'xgboost=={xgboost_version}',\n",
    "                            f'pandas=={pandas_version}',\n",
    "                            f'numpy=={numpy_version}',\n",
    "                            'cloudpickle' \n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "                'name': 'mlflow-env'\n",
    "            }\n",
    "\n",
    "            # Log the model\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=final_pipeline,\n",
    "                artifact_path=\"model\", # Standard path within the run artifacts\n",
    "                conda_env=conda_env,\n",
    "                registered_model_name=MODEL_NAME_AML # Register in AML Model Registry\n",
    "            )\n",
    "            print(f\"  Model logged and registered as '{MODEL_NAME_AML}' in Azure ML.\")\n",
    "            mlflow.set_tag(\"Final Model Logging Status\", \"Success\")\n",
    "\n",
    "        except SystemExit: # Catch potential exits from within fit/log\n",
    "             print(\"SystemExit during final model training/logging.\")\n",
    "             raise\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR fitting or logging the final model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            mlflow.set_tag(\"Final Model Training Status\", f\"Failed - Exception: {e}\")\n",
    "            mlflow.set_tag(\"Final Model Logging Status\", \"Failed\")\n",
    "            final_pipeline = None # Ensure pipeline is None if fitting failed\n",
    "            raise SystemExit(\"Stopping: Final model training failed.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nERROR: Skipping final model training as no best model was selected or parameters are missing.\")\n",
    "    mlflow.set_tag(\"Final Model Training Status\", \"Skipped - No Best Model Selected\")\n",
    "    raise SystemExit(\"Stopping: Cannot proceed without a selected model.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_final_markdown_aligned"
   },
   "source": [
    "## 6. Evaluate Final Model (Aligned with Original)\n",
    "\n",
    "Evaluate the performance of the final trained model on the unseen test set (`test_df_final`) using the **exact data split and evaluation functions** from the original notebook.\n",
    "\n",
    "*(MLOps Note: Log the evaluation metrics calculated on the test set to MLflow.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_final_code_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"\\n===== Evaluating Final Model on Unseen Test Set (Aligned) ====\")\n",
    "\n",
    "if final_pipeline is None:\n",
    "    print(\"ERROR: Skipping evaluation: Final pipeline was not trained successfully.\")\n",
    "    mlflow.set_tag(\"Final Model Evaluation Status\", \"Skipped - No Trained Pipeline\")\n",
    "    # If training failed, we likely should stop\n",
    "    raise SystemExit(\"Stopping: Cannot evaluate as final pipeline is not available.\")\n",
    "\n",
    "elif test_df_final.empty:\n",
    "    print(\"Warning: Skipping evaluation: Final test set (test_df_final) is empty.\")\n",
    "    mlflow.set_tag(\"Final Model Evaluation Status\", \"Skipped - Empty Test Set\")\n",
    "    # Depending on requirements, an empty test set might be ok or an error.\n",
    "    # For now, we allow proceeding but flag it.\n",
    "\n",
    "else:\n",
    "    print(f\"  Evaluating on test data shape: {test_df_final.shape}\")\n",
    "    start_pred_time = time.time()\n",
    "\n",
    "    # Handle potential NaNs in the final test data BEFORE predicting\n",
    "    # **ALIGNED WITH ORIGINAL NOTEBOOK's Step 3 logic (cell l7jnHHsmDSZL) for imputation, applied to TEST set**\n",
    "    # Ideally, use the imputer fitted on the training data. For strict alignment with original's simpler logic, refit on test.\n",
    "    test_df_final_imputed = test_df_final.copy()\n",
    "\n",
    "    # *** CORRECTED NaN CHECK HERE ***\n",
    "    if test_df_final_imputed[INPUT_FEATURES].isnull().values.any():\n",
    "        print(\"  Warning: NaNs detected in final test data. Imputing with mean (matching original Step 3 logic applied to test).\")\n",
    "        # Refit imputer on test data - less ideal but matches original implicit handling\n",
    "        imputer_final_test = sklearn.impute.SimpleImputer(strategy='mean')\n",
    "        test_df_final_imputed[INPUT_FEATURES] = imputer_final_test.fit_transform(test_df_final_imputed[INPUT_FEATURES])\n",
    "        mlflow.set_tag(\"Final Test Imputation\", \"Mean Imputed (Test Fit)\")\n",
    "        # If final_train_imputer was saved:\n",
    "        # try:\n",
    "        #    imputer_final_train = joblib.load('final_train_imputer.joblib')\n",
    "        #    test_df_final_imputed[INPUT_FEATURES] = imputer_final_train.transform(test_df_final_imputed[INPUT_FEATURES])\n",
    "        #    mlflow.set_tag(\"Final Test Imputation\", \"Mean Imputed (Train Fit)\")\n",
    "        # except Exception as imp_e:\n",
    "        #    print(f\"Could not load train imputer ({imp_e}), refitting on test.\")\n",
    "        #    # ... refit logic ...\n",
    "    else:\n",
    "        mlflow.set_tag(\"Final Test Imputation\", \"Not Required\")\n",
    "    # *** END CORRECTION ***\n",
    "\n",
    "    # Predict probabilities on the (potentially imputed) final test set\n",
    "    try:\n",
    "        final_predictions_test = final_pipeline.predict_proba(test_df_final_imputed[INPUT_FEATURES])[:, 1]\n",
    "        final_pred_time = time.time() - start_pred_time\n",
    "        print(f\"  Final model prediction completed in {final_pred_time:.2f} seconds.\")\n",
    "        mlflow.log_metric(\"final_model_predict_time_sec\", final_pred_time)\n",
    "\n",
    "        # Add predictions to the ORIGINAL final test dataframe for evaluation using the original function\n",
    "        test_df_final_eval = test_df_final.copy()\n",
    "        test_df_final_eval['predictions'] = final_predictions_test\n",
    "\n",
    "        # Assess performance on the final test set using function COPIED from original\n",
    "        final_performance_metrics = performance_assessment(test_df_final_eval,\n",
    "                                                           output_feature=OUTPUT_FEATURE,\n",
    "                                                           prediction_feature='predictions',\n",
    "                                                           top_k_list=[TOP_K_VALUE], # Use the same k as original\n",
    "                                                           rounded=False) # Get raw values for logging\n",
    "\n",
    "        print(\"\\n  Final Model Performance on Unseen Test Set (Aligned):\")\n",
    "        display(final_performance_metrics.round(4)) # Display rounded for readability\n",
    "\n",
    "        # Log final metrics to MLflow\n",
    "        auc_roc_test = final_performance_metrics['AUC ROC'].iloc[0]\n",
    "        ap_test = final_performance_metrics['Average precision'].iloc[0]\n",
    "        # Use .get with a default Series containing NaN to safely access CP@k\n",
    "        cp_at_k_test = final_performance_metrics.get(f'Card Precision@{TOP_K_VALUE}', pd.Series([np.nan])).iloc[0]\n",
    "\n",
    "        # Log only if metric is not NaN\n",
    "        if not pd.isna(auc_roc_test):\n",
    "            mlflow.log_metric(\"final_test_auc_roc\", auc_roc_test)\n",
    "        if not pd.isna(ap_test):\n",
    "            mlflow.log_metric(\"final_test_average_precision\", ap_test)\n",
    "        if not pd.isna(cp_at_k_test):\n",
    "             mlflow.log_metric(f\"final_test_card_precision_at_{TOP_K_VALUE}\", cp_at_k_test)\n",
    "\n",
    "        mlflow.set_tag(\"Final Model Evaluation Status\", \"Success\")\n",
    "\n",
    "        # Log the performance summary dataframe as an artifact\n",
    "        final_perf_path = \"final_model_test_performance_aligned.csv\"\n",
    "        try:\n",
    "            final_performance_metrics.round(5).to_csv(final_perf_path, index=False)\n",
    "            mlflow.log_artifact(final_perf_path)\n",
    "            print(f\"  Final performance metrics logged to MLflow and saved to {final_perf_path}\")\n",
    "        except Exception as art_e:\n",
    "            print(f\"  Warning: Failed to log performance artifact {final_perf_path}: {art_e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during final model prediction or evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        mlflow.set_tag(\"Final Model Evaluation Status\", f\"Failed - Exception: {e}\")\n",
    "        # Raise error to stop notebook if evaluation fails critically\n",
    "        raise SystemExit(f\"Stopping: Error during final model evaluation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "save_local_markdown_aligned",
    "deepnote_cell_type": "markdown",
    "id": "save_local_markdown_aligned"
   },
   "source": [
    "## 7. Optional: Save Final Model Locally\n",
    "\n",
    "*(MLOps Note: `mlflow.sklearn.log_model` already saved the model in MLflow format and registered it. This step saves a local copy using joblib, similar to the original notebook's potential last step, mainly for local verification or backup.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "save_local_code_aligned",
    "deepnote_cell_type": "code",
    "id": "save_local_code_aligned"
   },
   "outputs": [],
   "source": [
    "# Define path for local saving (within notebook context or defined output dir)\n",
    "# Use the best model name determined earlier\n",
    "local_model_filename = f\"{best_model_name or 'default'}_final_pipeline_aligned_local.pkl\"\n",
    "local_model_save_path = os.path.join(MODEL_OUTPUT_DIR, local_model_filename)\n",
    "\n",
    "if final_pipeline is not None:\n",
    "    try:\n",
    "        print(f\"\\nAttempting to save final pipeline locally to: {local_model_save_path}\")\n",
    "        joblib.dump(final_pipeline, local_model_save_path)\n",
    "        print(f\"  Final pipeline saved locally successfully.\")\n",
    "        # Optionally log this local file as well, though the MLflow format is preferred\n",
    "        # mlflow.log_artifact(local_model_save_path, artifact_path=\"local_model_backup\")\n",
    "        mlflow.set_tag(\"Local Model Save Status\", \"Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving the final model locally: {e}\")\n",
    "        mlflow.set_tag(\"Local Model Save Status\", f\"Failed: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping local model saving as final pipeline is not available.\")\n",
    "    mlflow.set_tag(\"Local Model Save Status\", \"Skipped - No Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "end_run_markdown_aligned"
   },
   "source": [
    "## 8. End MLflow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "end_run_code_aligned",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- End Parent MLflow Run ---\n",
    "mlflow.end_run()\n",
    "print(f\"\\nFinished and closed MLflow Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_local_markdown_aligned"
   },
   "source": [
    "## 9. Optional: Verify Locally Saved Model\n",
    "\n",
    "Load the locally saved `.pkl` file and make predictions on the test set to ensure it matches the results obtained before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "verify_local_code_aligned",
    "deepnote_cell_type": "code",
    "id": "verify_local_code_aligned"
   },
   "outputs": [],
   "source": [
    "# Optional: Code to load back the locally saved model and test (for verification)\n",
    "print(\"\\n===== Verifying Locally Saved Model (Optional) ====\")\n",
    "if 'local_model_save_path' in locals() and os.path.exists(local_model_save_path):\n",
    "    try:\n",
    "        loaded_local_model = joblib.load(local_model_save_path)\n",
    "        print(f\"  Model loaded successfully from: {local_model_save_path}\")\n",
    "        \n",
    "        # Ensure test data is available and handle imputation consistently\n",
    "        if not test_df_final.empty:\n",
    "            print(\"  Testing the loaded local model on unseen test data...\")\n",
    "            test_df_verify_imputed = test_df_final.copy()\n",
    "            if test_df_verify_imputed[INPUT_FEATURES].isNone().values.any():\n",
    "                 print(\"  Warning: NaNs detected in verification test data. Imputing with mean (Test Fit).\")\n",
    "                 imputer_verify = sklearn.impute.SimpleImputer(strategy='mean')\n",
    "                 test_df_verify_imputed[INPUT_FEATURES] = imputer_verify.fit_transform(test_df_verify_imputed[INPUT_FEATURES])\n",
    "            \n",
    "            # Predict using loaded model\n",
    "            local_predictions = loaded_local_model.predict_proba(test_df_verify_imputed[INPUT_FEATURES])[:, 1]\n",
    "            \n",
    "            # Create a DataFrame for evaluation\n",
    "            test_df_verify_eval = test_df_final.copy()\n",
    "            test_df_verify_eval['predictions'] = local_predictions\n",
    "            \n",
    "            # Assess the performance using the original function\n",
    "            local_model_performance = performance_assessment(test_df_verify_eval,\n",
    "                                                             output_feature=OUTPUT_FEATURE,\n",
    "                                                             prediction_feature='predictions',\n",
    "                                                             top_k_list=[TOP_K_VALUE],\n",
    "                                                             rounded=True)\n",
    "            print(\"\\n  Loaded Local Model Performance on Test Set:\")\n",
    "            display(local_model_performance)\n",
    "            \n",
    "            # Compare with previously calculated final_performance_metrics if available\n",
    "            if 'final_performance_metrics' in locals() and not final_performance_metrics.empty:\n",
    "                 print(\"\\n  Comparison with original final evaluation:\")\n",
    "                 comparison_df = pd.concat([\n",
    "                     final_performance_metrics.round(4).rename(index={0:'Original Final Eval'}),\n",
    "                     local_model_performance.round(4).rename(index={0:'Loaded Local Model Eval'})\n",
    "                 ])\n",
    "                 display(comparison_df)\n",
    "            \n",
    "        else:\n",
    "            print(\"  Test dataset is empty. Cannot test the loaded local model.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading or testing the local model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"  Local model file not found at '{local_model_save_path}' or path not defined. Cannot verify.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": "true",
    "deepnote_cell_type": "markdown",
    "id": "deepnote_link_aligned"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=68fe131d-9402-41f5-ab34-383fc691ce88' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "fd6507f9b4c9448383ad18f4c0c0df89",
  "deepnote_persisted_session": {
   "createdAt": "2025-04-08T15:15:03.011Z"
  },
  "kernelspec": {
   "display_name": "Custom Env (YAML)",
   "language": "python",
   "name": "custom-notebook-env-conda-adapted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
