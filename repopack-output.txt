This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-04-21T11:24:33.709Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.github/
  workflows/
    deploy-batch-endpoint-pipeline.yml
    deploy-model-training-pipeline.yml
    deploy-online-endpoint-pipeline.yml
    tf-gha-deploy-infra.yml
data-science/
  environment/
    train-conda.yml
  src/
    evaluate.py
    prep.py
    register.py
    train.py
    utils.py
infrastructure/
  modules/
    aml-workspace/
      main.tf
      outputs.tf
      variables.tf
    application-insights/
      main.tf
      outputs.tf
      variables.tf
    container-registry/
      main.tf
      outputs.tf
      variables.tf
    data-explorer/
      main.tf
      outputs.tf
      variables.tf
    key-vault/
      main.tf
      outputs.tf
      variables.tf
    resource-group/
      main.tf
      outputs.tf
      variables.tf
    storage-account/
      main.tf
      outputs.tf
      variables.tf
  aml_deploy.tf
  locals.tf
  main.tf
  variables.tf
mlops/
  azureml/
    deploy/
      batch/
        batch-deployment.yml
        batch-endpoint.yml
      online/
        online-deployment.yml
        online-endpoint.yml
        score.py
    train/
      data.yml
      environment.yml
      pipeline.yml
.gitignore
.pre-commit-config.yaml
CODE_OF_CONDUCT.md
config-infra-prod.yml
LICENSE
README.md
requirements.txt
SECURITY.md
SUPPORT.md

================================================================
Repository Files
================================================================

================
File: .github/workflows/deploy-batch-endpoint-pipeline.yml
================
name: deploy-batch-endpoint-pipeline
on:
  workflow_dispatch:
jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@main
      with:
        file_name: config-infra-prod.yml
  create-compute:
      needs: get-config
      uses: Azure/mlops-templates/.github/workflows/create-compute.yml@main
      with:
        cluster_name: batch-cluster # Compute cluster name for batch endpoint
        size: STANDARD_DS3_V2 # Adjust VM size if needed
        min_instances: 0
        max_instances: 5
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
      secrets:
          creds: ${{secrets.AZURE_CREDENTIALS}}
  create-endpoint:
      needs: [get-config,create-compute]
      uses: Azure/mlops-templates/.github/workflows/create-endpoint.yml@main
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/batch/batch-endpoint.yml
        endpoint_name: ${{ needs.get-config.outputs.bep }} # Use name from config
        endpoint_type: batch
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}
  create-deployment:
      uses: Azure/mlops-templates/.github/workflows/create-deployment.yml@main
      needs: [get-config,create-endpoint]
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/batch/batch-deployment.yml
        endpoint_name: ${{ needs.get-config.outputs.bep }} # Use name from config
        endpoint_type: batch
        deployment_name: default-batch-deploy # Name your deployment
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

================
File: .github/workflows/deploy-model-training-pipeline.yml
================
name: deploy-model-training-pipeline
on:
  workflow_dispatch:

jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@v1.1.0
      with:
        file_name: config-infra-prod.yml

  # register-environment:
  #     needs: get-config
  #     uses: Azure/mlops-templates/.github/workflows/register-environment.yml@v1.1.0
  #     with:
  #       resource_group: ${{ needs.get-config.outputs.resource_group }}
  #       workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
  #       environment_file: mlops/azureml/train/environment.yml # Points to AML Env definition
  #       conda_file: data-science/environment/train-conda.yml # Points to conda file
  #     secrets:
  #         creds: ${{secrets.AZURE_CREDENTIALS}}

  # # Optional: Register raw data if not already registered
  # register-raw-data:
  #   needs: get-config
  #   uses: Azure/mlops-templates/.github/workflows/register-dataset.yml@v1.1.0
  #   with:
  #     resource_group: ${{ needs.get-config.outputs.resource_group }}
  #     workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
  #     name: raw-fraud-data # Name for the raw data asset
  #     data_file: mlops/azureml/train/data.yml # Points to data asset definition
  #   secrets:
  #     creds: ${{secrets.AZURE_CREDENTIALS}}

  # create-compute:
  #     # Depends on environment registration (and optionally data registration)
  #     needs: [get-config, register-environment,register-raw-data] # Add register-raw-data if uncommented above
  #     uses: Azure/mlops-templates/.github/workflows/create-compute.yml@v1.1.0
  #     with:
  #       cluster_name: cpu-cluster # Default compute for pipeline steps
  #       size: Standard_E4ds_v5 # Adjust as needed
  #       min_instances: 0
  #       max_instances: 1
  #       cluster_tier: dedicated # Use low priority to save cost (optional)
  #       resource_group: ${{ needs.get-config.outputs.resource_group }}
  #       workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
  #     secrets:
  #         creds: ${{secrets.AZURE_CREDENTIALS}}

  run-pipeline:
      needs: [get-config] # Add register-raw-data if uncommented above
      uses: Azure/mlops-templates/.github/workflows/run-pipeline.yml@v1.1.0
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        # experiment_name: fraud-detection-training 
        parameters-file: mlops/azureml/train/pipeline.yml
        job-name: fraud-detection-pipeline-run # Display name for the run

      secrets:
          creds: ${{secrets.AZURE_CREDENTIALS}}

================
File: .github/workflows/deploy-online-endpoint-pipeline.yml
================
name: deploy-online-endpoint-pipeline
on:
  workflow_dispatch:

jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@v1.1.0
      with:
        file_name: config-infra-prod.yml

  create-endpoint:
      needs: get-config
      uses: Azure/mlops-templates/.github/workflows/create-endpoint.yml@v1.1.0
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/online/online-endpoint.yml
        endpoint_name: ${{ format('fraud-online-ep-{0}', needs.get-config.outputs.postfix) }} # Use name from config
        endpoint_type: online
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

  create-deployment:
      uses: Azure/mlops-templates/.github/workflows/create-deployment.yml@v1.1.0
      needs: [get-config,create-endpoint]
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/online/online-deployment.yml
        endpoint_name: ${{ format('fraud-online-ep-{0}', needs.get-config.outputs.postfix) }} # Use name from config
        endpoint_type: online
        deployment_name: fraud-online-dp # Default deployment name (can be parameterized)
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

  allocate-traffic:
      uses: Azure/mlops-templates/.github/workflows/allocate-traffic.yml@v1.1.0
      needs: [get-config,create-deployment]
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        traffic_allocation: fraud-online-dp=100 # Allocate all traffic to the 'blue' deployment
        endpoint_name: ${{ format('fraud-online-ep-{0}', needs.get-config.outputs.postfix) }} # Use name from config
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

================
File: .github/workflows/tf-gha-deploy-infra.yml
================
# This file remains largely the same as the template.
# It reads config-infra-prod.yml and uses Terraform to deploy Azure resources.
# No specific changes needed for the fraud detection logic itself,
# assuming the resource requirements (AML workspace, storage, etc.) are standard.

name: tf-gha-deploy-infra.yml

on:
  workflow_dispatch:
env:
    config_env: 'none'
jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@main
      with:
        file_name: config-infra-prod.yml
  test-terraform-state-deployment:
    needs: get-config
    uses: Azure/mlops-templates/.github/workflows/tf-gha-install-terraform.yml@main
    with:
      TFAction: 'apply'
      # Note: The original template had a 'set-env-branch' step which seems missing or simplified here.
      # Assuming config-infra-prod.yml is always used for this workflow dispatch.
      # dply_environment: ${{ needs.set-env-branch.outputs.config-file }} # This would need adjustment if branching logic is added
      dply_environment: 'prod' # Hardcoding for now based on file name
      location: ${{ needs.get-config.outputs.location }}
      namespace: ${{ needs.get-config.outputs.namespace }}
      postfix: ${{ needs.get-config.outputs.postfix }}
      environment: ${{ needs.get-config.outputs.environment }}
      enable_aml_computecluster: ${{ needs.get-config.outputs.enable_aml_computecluster == true }}
      enable_monitoring: ${{ needs.get-config.outputs.enable_monitoring == true  }}
      terraform_version: ${{ needs.get-config.outputs.terraform_version }}
      terraform_workingdir: ${{ needs.get-config.outputs.terraform_workingdir }}
      terraform_st_location: ${{ needs.get-config.outputs.terraform_st_location }}
      terraform_st_storage_account: ${{ needs.get-config.outputs.terraform_st_storage_account }}
      terraform_st_resource_group: ${{ needs.get-config.outputs.terraform_st_resource_group }}
      terraform_st_container_name: ${{ needs.get-config.outputs.terraform_st_container_name }}
      terraform_st_key: ${{ needs.get-config.outputs.terraform_st_key }}
      # terraform_plan_location: ${{ needs.get-config.outputs.location }} # Plan location might not be needed for apply
      # terraform_plan_vnet: "TBD" # VNet planning might be 
      terraform_plan_location: ${{ needs.get-config.outputs.location }} # Use the main location
      terraform_plan_vnet: "none" # Provide a placeholder if no specific VNet in plan
    secrets:
      azure_creds: ${{ secrets.AZURE_CREDENTIALS }}
      clientId: ${{ secrets.ARM_CLIENT_ID }}
      clientSecret: ${{ secrets.ARM_CLIENT_SECRET }}
      subscriptionId: ${{ secrets.ARM_SUBSCRIPTION_ID }}
      tenantId: ${{ secrets.ARM_TENANT_ID }}
  # The deploy-azureml-resources step in the original template seems like a placeholder.
  # The actual deployment happens via the tf-gha-install-terraform.yml reusable workflow.
  # deploy-azureml-resources:
  #   runs-on: ubuntu-latest
  #   steps:
  #   - id: deploy-aml-workspace
  #     name: deploy-aml-workspace
  #     run: echo "OK"

================
File: data-science/environment/train-conda.yml
================
# Conda environment specification for Fraud Detection Training Pipeline
# Based on user's environment.yml and MLOps requirements
name: fraud-detection-train-env
channels:
  - conda-forge # Prioritize conda-forge
  - defaults
dependencies:
  # --- Core Environment ---
  - python=3.9
  - pip # <<< IMPORTANT: Make sure 'pip' itself is listed as a conda dependency

  # --- Conda Dependencies (from requirements.txt) ---
  # Using conda install for these is generally preferred
  - python-graphviz=0.19.1   # Use 'python-graphviz' on conda [4, 6, 8]
  - imbalanced-learn=0.8.1
  - matplotlib              # REMOVED version pin ==3.2.2 to allow compatibility with Python 3.9
  - numpy=1.19.5
  - pandas=1.3.5
  - pandarallel=1.5.4
  - scikit-learn=1.0.0
  - seaborn=0.11.2
  - Sphinx=4.2.0
  - sphinxcontrib-bibtex=2.2.1
  - xgboost=1.5.1

  # --- Pip Dependencies ---
  - pip: # <<< Create this nested section for pip packages
      - dvc[azure]   # <<< Moved here - extras like [azure] require pip
      - mlflow       # <<< Moved here - often installed via pip
      - azureml-sdk  # <<< Moved here - v1 SDK is a pip package
      - azureml-mlflow
      - azure-ai-ml 

      # --- IMPORTANT NOTE FOR AZURE ML COMPUTE INSTANCES ---
      # If you are using the CURRENT Azure ML platform (v2),
      # you should likely be using the v2 SDK instead:
      # - azure-ai-ml
      - azure-identity # (often needed for authentication)
      # Comment out azureml-sdk above and uncomment these if using v2.
      - azureml-ai-monitoring
      - azureml-inference-server-http
      - mltable 
      - azureml-dataprep[pandas]
      - pyarrow
      # Cloudpickle often needed for pipeline serialization
      - cloudpickle>=2.0.0,<3.0.0

# Notes:
# - Version ranges are examples; adjust based on exact requirements and compatibility testing.
# - Consider pinning more versions for strict reproducibility.
# - Remove unused libraries (Sphinx?) if not needed for the pipeline steps.
# - If using V1 SDK (azureml-sdk), ensure compatibility with Python 3.9.
# - If using V2 SDK (azure-ai-ml), adjust imports and code in pipeline scripts accordingly.

================
File: data-science/src/evaluate.py
================
# data-science/src/evaluate.py
import os
import argparse
import datetime
from pathlib import Path # Ensure Path is imported
import pickle
import json
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn

# Import shared functions from utils.py
from utils import (
    performance_assessment # Still needed
    # get_train_test_set # NO LONGER NEEDED here
)

def parse_args():
    parser = argparse.ArgumentParser("evaluate")
    parser.add_argument("--model_input", type=str, help="Path to input model directory")
    # --- UPDATED: Argument expects the directory ---
    parser.add_argument("--test_data", type=str, help="Path to the DIRECTORY containing the final test data file (final_test_data.pkl)") # Updated help text
    parser.add_argument("--evaluation_output", type=str, help="Path to save evaluation results")
    parser.add_argument("--model_name", type=str, help="Name of the model for registration comparison", default="fraud-detection-model")

    # Threshold for deploy flag
    parser.add_argument("--deploy_threshold_metric", type=str, default="Average precision", help="Metric to check for deployment threshold")
    parser.add_argument("--deploy_threshold_value", type=float, default=0.65, help="Threshold value for deployment metric")

    args = parser.parse_args()
    return args

def main(args):
    mlflow.start_run()
    print("Evaluation script started")
    print(f"Args: {args}")

    evaluation_output_path = Path(args.evaluation_output)
    evaluation_output_path.mkdir(parents=True, exist_ok=True)

    # --- Load Model ---
    print(f"Loading model from: {args.model_input}")
    try:
        model = mlflow.sklearn.load_model(args.model_input)
        print("Model loaded successfully.")
        mlflow.log_param("model_input_path", args.model_input)
    except Exception as e:
        print(f"ERROR: Failed to load model from {args.model_input}: {e}")
        mlflow.log_param("evaluation_error", f"Model loading failed: {e}")
        mlflow.end_run(status="FAILED")
        return

    # --- Load Pre-split Test Data ---
    # --- UPDATED: Construct path inside the script ---
    print(f"Looking for final test data PKL inside directory: {args.test_data}")
    test_data_dir = Path(args.test_data)
    test_data_file = test_data_dir / "final_test_data.pkl" # Construct the full path
    # --- End Update ---

    test_df_final = pd.DataFrame() # Initialize
    if test_data_file.is_file():
        try:
            test_df_final = pd.read_pickle(test_data_file)
            if test_df_final.empty:
                print("Warning: Loaded test data file is empty.")
            else:
                # Use the specific filename in the log message
                print(f"Loaded final test set '{test_data_file.name}' with shape: {test_df_final.shape}")
            mlflow.log_metric("test_set_rows", test_df_final.shape[0])
            if not test_df_final.empty and 'TX_FRAUD' in test_df_final.columns:
                 mlflow.log_metric("test_set_fraud_rate", test_df_final['TX_FRAUD'].mean())

        except Exception as e:
            print(f"ERROR loading test data file {test_data_file}: {e}")
            mlflow.log_param("evaluation_error", f"Test data loading failed: {e}")
            mlflow.end_run(status="FAILED")
            return
    else:
        # Use the constructed path in the error message
        print(f"ERROR: Test data file not found at expected location: {test_data_file}")
        mlflow.log_param("evaluation_error", "Test data file not found.")
        mlflow.end_run(status="FAILED")
        return

    # --- Evaluate (if test data loaded) ---
    if test_df_final.empty:
        print("Warning: Final test set is empty. Skipping evaluation.")
        performance_metrics = pd.DataFrame(columns=['AUC ROC', 'Average precision', f'Card Precision@{100}'])
        performance_metrics.loc[0] = [np.nan, np.nan, np.nan]
        deploy_flag = 0 # Cannot evaluate, so do not deploy
    else:
        try:
            # --- Get Predictions ---
            print("Making predictions on test set...")
            # Ensure INPUT_FEATURES and OUTPUT_FEATURE are defined correctly
            INPUT_FEATURES = ['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',
                              'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',
                              'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',
                              'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',
                              'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',
                              'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',
                              'TERMINAL_ID_RISK_30DAY_WINDOW']
            OUTPUT_FEATURE = "TX_FRAUD"

            # Validate required columns exist in the loaded test dataframe
            missing_input_features = [col for col in INPUT_FEATURES if col not in test_df_final.columns]
            if missing_input_features:
                raise ValueError(f"Missing input features in test data: {missing_input_features}")
            if OUTPUT_FEATURE not in test_df_final.columns:
                raise ValueError(f"Missing output feature '{OUTPUT_FEATURE}' in test data")


            X_test = test_df_final[INPUT_FEATURES]
            y_test = test_df_final[OUTPUT_FEATURE] # Defined for clarity, used in performance_assessment

            # Note: The pipeline model (including scaler) should handle potential NaNs if trained correctly.
            # Adding an explicit check here for robustness:
            if X_test.isnull().values.any():
                 print("Warning: NaNs detected in test features before prediction. Model's pipeline should handle imputation/scaling.")
                 # Depending on the model's robustness, you might need explicit imputation here if the pipeline doesn't handle it.
                 # Example (use cautiously, assumes imputer was saved during training or refit here):
                 # from sklearn.impute import SimpleImputer
                 # print("Attempting imputation with mean...")
                 # imputer = SimpleImputer(strategy='mean')
                 # X_test = pd.DataFrame(imputer.fit_transform(X_test), columns=X_test.columns, index=X_test.index)


            predictions = model.predict_proba(X_test)[:, 1]
            test_df_final['predictions'] = predictions # Add predictions column for assessment function
            print("Predictions generated.")

            # --- Evaluate Performance ---
            print("Calculating performance metrics...")
            performance_metrics = performance_assessment(
                test_df_final, # Pass the dataframe with the predictions column
                output_feature=OUTPUT_FEATURE,
                prediction_feature='predictions',
                top_k_list=[100], # Example k value, adjust if needed or parameterize
                rounded=False # Get raw values for logging
            )
            print("Performance Metrics on Test Set:")
            print(performance_metrics)

            # --- Log Metrics ---
            print("Logging metrics to MLflow...")
            for metric in performance_metrics.columns:
                value = performance_metrics[metric].iloc[0]
                if not pd.isna(value):
                    # Sanitize metric name for MLflow
                    mlflow_metric_name = metric.lower().replace(" ", "_").replace("@", "_at_").replace("%","perc")
                    mlflow.log_metric(f"test_{mlflow_metric_name}", value)
            print("Metrics logged.")

            # --- Determine Deploy Flag ---
            print("Determining deploy flag...")
            deploy_flag = 0
            # Check if threshold is positive and metric exists
            if args.deploy_threshold_value >= 0 and args.deploy_threshold_metric in performance_metrics.columns:
                metric_value = performance_metrics[args.deploy_threshold_metric].iloc[0]
                # Check if metric value is valid and meets threshold
                if not pd.isna(metric_value) and metric_value >= args.deploy_threshold_value:
                    deploy_flag = 1
                    print(f"Deploy flag set to 1 ({args.deploy_threshold_metric} {metric_value:.4f} >= {args.deploy_threshold_value})")
                else:
                    # Reason for not deploying
                    reason = "NaN" if pd.isna(metric_value) else f"{metric_value:.4f} < {args.deploy_threshold_value}"
                    print(f"Deploy flag remains 0 ({args.deploy_threshold_metric} is {reason})")
            # Handle case where threshold is negative (always deploy)
            elif args.deploy_threshold_value < 0:
                 deploy_flag = 1
                 print("Deploy flag set to 1 (deploy_threshold_value is negative).")
            # Handle case where metric is missing or threshold is 0
            else:
                 reason = f"metric '{args.deploy_threshold_metric}' not found" if args.deploy_threshold_metric not in performance_metrics.columns else f"threshold is {args.deploy_threshold_value}"
                 print(f"Deploy flag remains 0 ({reason})")

            mlflow.log_metric("deploy_flag", deploy_flag)
            mlflow.log_param("deploy_threshold_metric", args.deploy_threshold_metric)
            mlflow.log_param("deploy_threshold_value", args.deploy_threshold_value)

        except Exception as e:
            print(f"ERROR during prediction or evaluation: {e}")
            import traceback; traceback.print_exc()
            mlflow.log_param("evaluation_error", f"Prediction/eval failed: {e}")
            # Create an empty dataframe with expected columns on error
            performance_metrics = pd.DataFrame(columns=['AUC ROC', 'Average precision', f'Card Precision@{100}'])
            performance_metrics.loc[0] = [np.nan, np.nan, np.nan]
            deploy_flag = 0 # Ensure deploy flag is 0 on error
            mlflow.end_run(status="FAILED")
            # Still try to save empty metrics and flag file

    # --- Save Evaluation Results ---
    # Always save results, even if empty/NaN, and the deploy flag
    print(f"Saving evaluation results to {evaluation_output_path}")
    metrics_file = evaluation_output_path / "test_performance_metrics.csv"
    try:
        performance_metrics.round(5).to_csv(metrics_file, index=False)
        mlflow.log_artifact(str(metrics_file))
        print(f"Metrics saved to {metrics_file}")
    except Exception as e: print(f"Error saving metrics file: {e}")

    deploy_flag_file = evaluation_output_path / "deploy_flag"
    try:
        with open(deploy_flag_file, 'w') as f: f.write(str(deploy_flag))
        mlflow.log_artifact(str(deploy_flag_file))
        print(f"Deploy flag ({deploy_flag}) saved to {deploy_flag_file}")
    except Exception as e: print(f"Error saving deploy flag file: {e}")

    mlflow.end_run()
    print("Evaluation script finished.")

if __name__ == "__main__":
    args = parse_args()
    main(args)

================
File: data-science/src/prep.py
================
# data-science/src/prep.py
import os
import argparse
import datetime
import time
from pathlib import Path
import pandas as pd
import numpy as np
import mlflow
import mltable # Import mltable
import traceback # For detailed error printing

# Import shared functions from utils.py
from utils import (
    is_weekend,
    is_night,
    get_customer_spending_behaviour_features,
    get_count_risk_rolling_window
)

def parse_args():
    parser = argparse.ArgumentParser("prep_mltable")
    parser.add_argument("--input_tabular_data", type=str, required=True, help="Path to input MLTable directory (mounted)")
    # FIX 1: Make output path required
    parser.add_argument("--output_mltable_path", type=str, help="Path to output directory for the final MLTable")

    # FIX 2: Add default values matching pipeline.yml defaults
    parser.add_argument("--output_start_date", type=str, default="2025-06-11", help="Start date for filtering the transformed data (YYYY-MM-DD)")
    parser.add_argument("--output_end_date", type=str, default="2025-08-14", help="End date for filtering the transformed data (YYYY-MM-DD)")
    parser.add_argument("--baseline_date_str", type=str, default="2025-04-01", help="Optional fixed baseline date for TX_TIME_DAYS calculation if not already present (YYYY-MM-DD)")

    args = parser.parse_args()
    print(f"Parsed Arguments: {args}") # Print parsed args to confirm defaults
    return args

def main(args):
    mlflow.start_run()
    print("Preparation script started (MLTable input/output)")
    print(f"Full Arguments Received (before parsing defaults): {args}") # Log raw args object

    # Log parameters AFTER parsing (will show defaults if applied)
    mlflow.log_param("input_mltable_path", args.input_tabular_data)
    mlflow.log_param("output_mltable_path", args.output_mltable_path)
    mlflow.log_param("filter_output_start_date", args.output_start_date)
    mlflow.log_param("filter_output_end_date", args.output_end_date)
    mlflow.log_param("baseline_date_str_used", args.baseline_date_str if args.baseline_date_str else "None")

    # --- Load Input MLTable ---
    print(f"\n[LOAD DATA] Loading input MLTable from: {args.input_tabular_data}")
    try:
        input_path_obj = Path(args.input_tabular_data)
        if not input_path_obj.exists() or not input_path_obj.is_dir() or not (input_path_obj / "MLTable").is_file():
             raise FileNotFoundError(f"Valid MLTable definition not found at directory: {args.input_tabular_data}")

        input_tbl = mltable.load(args.input_tabular_data)
        print("[LOAD DATA] MLTable definition loaded. Loading into DataFrame...")
        transactions_df = input_tbl.to_pandas_dataframe()
        print(f"Loaded MLTable into DataFrame. Shape: {transactions_df.shape}")

        if transactions_df.empty:
             print("ERROR: Loaded DataFrame is empty. Exiting.")
             mlflow.log_metric("input_rows_loaded_prep", 0)
             mlflow.end_run(status="FAILED")
             return
        mlflow.log_metric("input_rows_loaded_prep", len(transactions_df))

        # --- Data Validation and Time Column Handling ---
        required_cols = ['TRANSACTION_ID', 'TX_DATETIME', 'CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT', 'TX_FRAUD'] # Add other required cols if any
        missing_cols = [col for col in required_cols if col not in transactions_df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns in input MLTable data: {missing_cols}")

        if not pd.api.types.is_datetime64_any_dtype(transactions_df['TX_DATETIME']):
            print("Converting TX_DATETIME to datetime objects...")
            transactions_df['TX_DATETIME'] = pd.to_datetime(transactions_df['TX_DATETIME'])

        if args.baseline_date_str and ('TX_TIME_DAYS' not in transactions_df.columns or 'TX_TIME_SECONDS' not in transactions_df.columns):
            print(f"Calculating time columns relative to fixed baseline: {args.baseline_date_str}")
            try:
                baseline_dt_date = datetime.datetime.strptime(args.baseline_date_str, "%Y-%m-%d").date()
                baseline_dt_datetime = datetime.datetime.strptime(args.baseline_date_str, "%Y-%m-%d")
                transactions_df['TX_TIME_DAYS'] = (transactions_df['TX_DATETIME'].dt.date - baseline_dt_date).apply(lambda x: x.days)
                transactions_df['TX_TIME_SECONDS'] = (transactions_df['TX_DATETIME'] - baseline_dt_datetime).dt.total_seconds()
                mlflow.set_tag("time_columns_calculated", "True")
                print("Time columns TX_TIME_DAYS and TX_TIME_SECONDS calculated.")
            except Exception as e:
                 print(f"ERROR calculating time columns: {e}. Proceeding without them if possible.")
                 mlflow.set_tag("time_columns_calculated", "Error")
        elif 'TX_TIME_DAYS' not in transactions_df.columns or 'TX_TIME_SECONDS' not in transactions_df.columns:
             print("Warning: TX_TIME_DAYS or TX_TIME_SECONDS missing and baseline_date_str not provided or not needed. Some features/splitting might fail if they rely on these.")
             mlflow.set_tag("time_columns_calculated", "Missing/NotNeeded")
        else:
             print("Time columns TX_TIME_DAYS and TX_TIME_SECONDS already present.")
             mlflow.set_tag("time_columns_calculated", "Already Present")


    except Exception as e:
        print(f"ERROR loading or initially processing input MLTable: {e}")
        traceback.print_exc()
        mlflow.log_param("prep_error", f"Input loading failed: {e}")
        mlflow.end_run(status="FAILED")
        return

    # --- Apply Transformations ---
    print("\n[TRANSFORM] Applying feature transformations...")
    start_transform_time = time.time()
    try:
        # Apply existing transformations
        # Note: Ensure groupy().apply() warnings are acceptable or add include_groups=False if appropriate for your logic
        print("[TRANSFORM] Calculating datetime features...")
        transactions_df['TX_DURING_WEEKEND'] = transactions_df.TX_DATETIME.apply(is_weekend)
        transactions_df['TX_DURING_NIGHT'] = transactions_df.TX_DATETIME.apply(is_night)
        mlflow.set_tag("transformation_datetime", "Success")
        print("[TRANSFORM] Calculating customer spending features...")
        transactions_df = transactions_df.groupby('CUSTOMER_ID', group_keys=False).apply(
            lambda x: get_customer_spending_behaviour_features(x, windows_size_in_days=[1, 7, 30])
            #, include_groups=False # Add this if pandas version requires it and logic is correct
        )
        transactions_df = transactions_df.sort_values('TRANSACTION_ID').reset_index(drop=True)
        mlflow.set_tag("transformation_customer", "Success")
        print("[TRANSFORM] Calculating terminal risk features...")
        transactions_df = transactions_df.groupby('TERMINAL_ID', group_keys=False).apply(
            lambda x: get_count_risk_rolling_window(x, delay_period=7, windows_size_in_days=[1, 7, 30], feature="TERMINAL_ID")
            #, include_groups=False # Add this if pandas version requires it and logic is correct
        )
        transactions_df = transactions_df.sort_values('TRANSACTION_ID').reset_index(drop=True)
        mlflow.set_tag("transformation_terminal", "Success")
        print("[TRANSFORM] All transformations applied.")

    except Exception as e:
        print(f"ERROR during transformations: {e}")
        traceback.print_exc()
        mlflow.log_param("prep_error", f"Transformation failed: {e}")
        mlflow.set_tag("Transformation Status", f"Failed: {e}")
        mlflow.end_run(status="FAILED")
        return

    transform_time = time.time() - start_transform_time
    print(f"Transformations applied in {transform_time:.2f} seconds.")
    mlflow.log_metric("prep_transform_time_sec", transform_time)

    # --- Filter Transformed Data ---
    transactions_df_filtered = transactions_df # Start with the full transformed df
    # Check if dates were provided (they will have defaults now unless overridden)
    if args.output_start_date and args.output_end_date:
        print(f"\n[FILTER] Filtering transformed data to output range: {args.output_start_date} to {args.output_end_date}")
        try:
            # Convert string dates from args to datetime objects
            output_start_dt = datetime.datetime.strptime(args.output_start_date, "%Y-%m-%d")
            output_end_dt = datetime.datetime.strptime(args.output_end_date, "%Y-%m-%d")

            # Perform the filtering on the TX_DATETIME column
            transactions_df_filtered = transactions_df[
                (transactions_df['TX_DATETIME'] >= output_start_dt) &
                # End date is inclusive, so check less than day *after* end date
                (transactions_df['TX_DATETIME'] < output_end_dt + datetime.timedelta(days=1))
            ].copy() # Use copy to avoid SettingWithCopyWarning

            print(f"Filtered data shape: {transactions_df_filtered.shape}")
            if transactions_df_filtered.empty:
                print("ERROR: No data remains after filtering for the specified date range. No MLTable will be saved.")
                mlflow.log_metric("final_prepared_rows", 0)
                mlflow.set_tag("Data Saving Status", "Failed - Empty after filter")
                mlflow.end_run(status="FAILED")
                return
            mlflow.set_tag("Data Filtering", "Applied")

        except ValueError as e:
             print(f"ERROR parsing filter dates: {e}. Check YYYY-MM-DD format.")
             mlflow.log_param("prep_error", f"Date parsing failed: {e}")
             mlflow.set_tag("Data Filtering", "Error")
             mlflow.end_run(status="FAILED")
             return
        except Exception as e:
            print(f"ERROR filtering data for output range: {e}")
            traceback.print_exc()
            mlflow.log_param("prep_error", f"Filtering failed: {e}")
            mlflow.set_tag("Data Filtering", "Error")
            mlflow.end_run(status="FAILED")
            return
    else:
        # This case should not happen now with defaults, unless defaults are empty strings
        print("WARNING: Output start or end date missing, using all transformed data.")
        mlflow.set_tag("Data Filtering", "Skipped (Missing Dates)")


    # --- Save Final DataFrame and Create MLTable Definition ---
    # FIX 3: Correct saving logic
    print(f"\n[SAVE MLTABLE] Saving final DataFrame to Parquet and creating MLTable definition in: {args.output_mltable_path}")
    output_dir = Path(args.output_mltable_path)
    # Azure ML creates the output directory structure for the component task.

    # Define the path for the Parquet data file *inside* the output directory
    data_filename = "data.parquet" # A standard name for the data file
    data_file_path = output_dir / data_filename
    # Define the relative path to be used *within* the MLTable definition file
    relative_data_path = f"./{data_filename}"

    try:
        # --- Step 1: Save the Filtered DataFrame to a Parquet file ---
        print(f"Saving DataFrame ({transactions_df_filtered.shape}) to Parquet file: {data_file_path}")
        transactions_df_filtered.to_parquet(data_file_path, index=False, engine='pyarrow')
        print("DataFrame saved successfully to Parquet.")

        # --- Step 2: Create the MLTable definition referencing the saved Parquet file ---
        # The paths list should contain the relative path to the data file(s)
        paths = [{'file': relative_data_path}]

        # Create the MLTable object defining how to read the Parquet file
        final_mltable = mltable.from_parquet_files(paths=paths)
        # You could add transformations here if needed for the *output* table, e.g.,
        # final_mltable = final_mltable.convert_column_types(...)

        # --- Step 3: Save the MLTable definition file (MLTable YAML) ---
        # This saves the `MLTable` YAML file into the specified output directory.
        print(f"Saving MLTable definition file (MLTable YAML) to directory: {output_dir}")
        final_mltable.save(str(output_dir)) # Pass the directory path

        print(f"MLTable definition saved successfully in {output_dir}.")
        print("Azure ML pipeline will handle the registration based on pipeline output definition.")
        mlflow.log_metric("final_prepared_rows", len(transactions_df_filtered))
        mlflow.set_tag("Data Saving Status", "Success (MLTable Definition + Parquet)")

    except Exception as e:
        print(f"ERROR saving final data and creating MLTable: {e}")
        traceback.print_exc()
        mlflow.log_param("prep_error", f"MLTable saving/creation failed: {e}")
        mlflow.set_tag("Data Saving Status", "Failed")
        mlflow.end_run(status="FAILED")
        return

    mlflow.end_run()
    print("\nPreparation script finished successfully.")


if __name__ == "__main__":
    args = parse_args()
    main(args)

================
File: data-science/src/register.py
================
# data-science/src/register.py
import argparse
from pathlib import Path
import pickle # Keep pickle import if needed elsewhere, though not directly used here for registration
import json
import os
import traceback # Import traceback for better error details if needed

import mlflow
import mlflow.sklearn

def parse_args():
    parser = argparse.ArgumentParser("register")
    parser.add_argument('--model_name', type=str, help='Name under which model will be registered', default="fraud-detection-model")
    # Input model_path is the LOCAL path where the artifact was downloaded by AML
    parser.add_argument('--model_path', type=str, help='Path to trained model directory (local path in job)')
    parser.add_argument('--evaluation_output', type=str, help='Path of evaluation results directory (contains deploy_flag)')
    parser.add_argument('--model_info_output_path', type=str, help="Path to write model info JSON")
    args, _ = parser.parse_known_args()
    print(f'Arguments: {args}')
    return args

def main(args):
    # --- Start MLflow Run ---
    # An MLflow run is automatically started when a pipeline step executes.
    # Using mlflow.start_run() is often redundant here but doesn't hurt.
    # We'll get the active run ID later.
    mlflow.start_run()
    active_run = mlflow.active_run()
    if active_run:
        run_id = active_run.info.run_id
        print(f"Register script started (MLflow Run ID: {run_id})")
    else:
        print("Register script started (WARNING: No active MLflow run found!)")
        run_id = None # Handle cases where run might not be active

    # --- Read Deploy Flag ---
    deploy_flag = 0 # Default to not deploying
    deploy_flag_file = Path(args.evaluation_output) / "deploy_flag"
    if deploy_flag_file.exists():
        try:
            with open(deploy_flag_file, 'r') as f:
                deploy_flag = int(f.read().strip())
            print(f"Read deploy_flag: {deploy_flag}")
        except Exception as e:
            print(f"Warning: Could not read deploy flag file at {deploy_flag_file}. Defaulting to 0. Error: {e}")
    else:
        print(f"Warning: Deploy flag file not found at {deploy_flag_file}. Defaulting to 0.")

    mlflow.log_metric("deploy_flag_read", deploy_flag)

    # --- Register Model Condition ---
    # Uncomment below to force registration for testing
    # deploy_flag = 1
    # print(f"Overriding deploy_flag to: {deploy_flag}")

    if deploy_flag == 1:
        print(f"Attempting to register model '{args.model_name}'...")
        print(f"Model artifact downloaded by AML to local path: '{args.model_path}'")

        try:
            # --- FIX: Construct the correct model URI ---
            # Method: Use the artifact logged by the *train_model* step.
            # We assume the train_model step logged the model using mlflow.sklearn.save_model
            # which implicitly logs it to MLflow under args.model_name within *that* step's run.
            # The input args.model_path IS the locally downloaded artifact.
            # We need to register using a valid MLflow URI. The most reliable way
            # in this pattern is often to log it *again* within this step's run,
            # or construct the 'runs:/...' URI relative to the *training* run.
            # Let's try the re-logging approach first, as it's simpler contextually.

            # 1. Load the model from the local path provided by AML input binding
            print(f"Loading model from local path: {args.model_path}")
            # Check if it's a directory and contains MLmodel
            local_model_path = Path(args.model_path)
            if not local_model_path.is_dir() or not (local_model_path / "MLmodel").exists():
                 raise ValueError(f"Provided model_path '{args.model_path}' is not a valid MLflow model directory.")
            # No need to load the model object itself unless validation is desired

            # NOTE: We are NOT re-logging here. The pipeline definition itself
            # implies the input 'model_path' *is* the model. We just need to tell
            # mlflow.register_model the *correct* reference URI format.
            # The value passed via `${{parent.jobs.train_model.outputs.model_output}}`
            # *should* be interpretable by the backend if the pipeline is set up correctly.
            # The error indicates the value being passed *to the script* is the mount path.

            # --- REVISED FIX: Use the input path directly but ensure it's treated correctly ---
            # The error message implies it expects azureml:// format for file sources.
            # However, the standard practice is to use runs:/<run_id>/path
            # Let's try constructing the runs:/ URI based on the CURRENT run_id, assuming
            # the model was logged within the *training* step, and the artifact path is just the model name.
            # This is brittle as it assumes the training step's artifact path.

            # --- SAFEST FIX (adopted from original template logic): Log Within This Step ---
            # This avoids needing info from the parent run.
            print("Loading model object for re-logging...")
            model_object = mlflow.sklearn.load_model(args.model_path)

            print(f"Logging model '{args.model_name}' within registration step's run ({run_id})...")
            mlflow.sklearn.log_model(
                 sk_model=model_object,
                 artifact_path=args.model_name # Log it under the model name within this run's artifacts
            )
            print("Model logged.")

            # Construct the URI based on the *current* run's ID and the artifact path we just used
            model_uri_for_registration = f'runs:/{run_id}/{args.model_name}'
            print(f"Constructed URI for registration: {model_uri_for_registration}")

            # 2. Register the model using the correctly formatted URI
            print(f"Registering model using URI: {model_uri_for_registration}")
            registered_model = mlflow.register_model(
                model_uri=model_uri_for_registration, # Use the constructed URI
                name=args.model_name
            )
            model_version = registered_model.version
            print(f"Successfully registered model '{args.model_name}' version {model_version}")
            mlflow.log_param("registered_model_name", args.model_name)
            mlflow.log_param("registered_model_version", model_version)

            # 3. Write model info JSON output
            print("Writing model info JSON...")
            model_info = {"id": f"{args.model_name}:{model_version}"}
            output_dir = Path(args.model_info_output_path)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / "model_info.json"
            try:
                with open(output_path, "w") as of:
                    json.dump(model_info, fp=of, indent=4)
                print(f"Model info saved to {output_path}")
                # Optionally log this JSON as an artifact of the registration step
                mlflow.log_artifact(str(output_path))
            except Exception as json_e:
                 print(f"Error writing model info JSON: {json_e}")
                 # Don't fail the whole step for this minor error

        except Exception as e:
            print(f"ERROR during model registration: {e}")
            # --- FIX for secondary error: Truncate the error message ---
            error_message = str(e)
            max_len = 499 # Max length for MLflow param value is 500
            truncated_error_message = (error_message[:max_len] + '...') if len(error_message) > max_len else error_message
            print(f"Logging truncated error: {truncated_error_message}")
            try:
                mlflow.log_param("registration_error", truncated_error_message)
            except Exception as log_err:
                print(f"Could not log registration error parameter: {log_err}")
            # Optionally, fail the run explicitly
            mlflow.end_run(status="FAILED")
            print("Register script failed.")
            # Use raise to ensure the step fails in Azure ML
            raise e # Re-raise the original exception to fail the step

    else:
        print("Deploy flag is 0. Model will not be registered.")
        # Create an empty model_info.json to avoid downstream errors if file is expected
        print("Creating empty model info JSON.")
        model_info = {"id": "None:0"} # Indicate no model registered
        output_dir = Path(args.model_info_output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / "model_info.json"
        try:
            with open(output_path, "w") as of:
                json.dump(model_info, fp=of, indent=4)
            print(f"Empty model info saved to {output_path}")
            # mlflow.log_artifact(str(output_path)) # Optionally log the empty file
        except Exception as e:
             print(f"Error writing empty model info JSON: {e}")

    mlflow.end_run()
    print("Register script finished.")

if __name__ == "__main__":
    args = parse_args()
    main(args)

================
File: data-science/src/train.py
================
# data-science/src/train.py
# --- UPDATED to consume MLTable input ---

import os
import argparse
import datetime
import time
import pickle
import json
from pathlib import Path
import pandas as pd
import numpy as np
import traceback # Import traceback for detailed error printing
import sys
import mltable # Import mltable

import sklearn
from sklearn import model_selection, metrics, pipeline, preprocessing, tree, ensemble
import xgboost

import mlflow
import mlflow.sklearn

# Import shared functions from utils.py
from utils import (
    card_precision_top_k_custom,
    get_summary_performances,
    model_selection_wrapper,
    prequentialSplit_with_dates,
    get_train_test_set
)


def parse_args():
    # UPDATED Argument Parser for MLTable input
    parser = argparse.ArgumentParser("train_mltable") # Renamed for clarity
    parser.add_argument("--input_mltable_data", type=str, required=True, help="Path to folder containing the prepared MLTable data (mounted)")
    # --- FIX: Added required=True to output arguments ---
    parser.add_argument("--model_output", type=str, required=True, help="Path to save output model artifact (MLflow format)")
    parser.add_argument("--test_data_output", type=str, required=True, help="Path to save the final test data split (as folder containing pkl)")
    # --- End Fix ---
    parser.add_argument("--anchor_date_str", type=str, required=True, help="Anchor date for deriving training/validation splits (YYYY-MM-DD)")

    # Keep arguments needed for splitting the loaded data (with defaults)
    parser.add_argument("--delta_train", type=int, default=7, help="Duration of training period in days")
    parser.add_argument("--delta_delay", type=int, default=7, help="Duration of delay period in days")
    parser.add_argument("--delta_assessment", type=int, default=7, help="Duration of assessment period in days")
    parser.add_argument("--n_folds", type=int, default=4, help="Number of folds for prequential validation")

    # Other Parameters (with defaults)
    parser.add_argument("--top_k_value", type=int, default=100, help="Value K for Card Precision@k")
    parser.add_argument("--n_jobs", type=int, default=5, help="Number of parallel jobs for GridSearchCV")

    args = parser.parse_args()
    print(f"Parsed Arguments: {args}") # Print args early for debugging
    return args

# REMOVED: load_transformed_data_window function is no longer needed

def main(args):
    mlflow.start_run()
    run_id = mlflow.active_run().info.run_id
    print(f"===== Training Script Started (MLTable Input - MLflow Run ID: {run_id}) =====")
    print(f"Full Arguments Received: {args}") # Log full args object

    # Log parameters (exclude path inputs if desired)
    print("[PARAM LOG] Logging input parameters...")
    loggable_params = {k: v for k, v in vars(args).items() if k not in ['input_mltable_data', 'model_output', 'test_data_output']}
    mlflow.log_params(loggable_params)
    # Log path parameters separately for clarity
    mlflow.log_param("input_mltable_path_param", args.input_mltable_data)
    mlflow.log_param("model_output_path_param", args.model_output)
    mlflow.log_param("test_data_output_path_param", args.test_data_output)



    # --- Load Prepared Data ---
    print(f"\n[LOAD DATA] Reading data from input folder '{args.input_mltable_data}'...")
    load_start_time = time.time()
    transactions_df = pd.DataFrame() # Initialize
    try:
        input_path_obj = Path(args.input_mltable_data)
        if not input_path_obj.is_dir():
             raise ValueError(f"Input data path must be a directory: {args.input_mltable_data}")

        # --- MODIFICATION START: Construct path to Parquet and load directly ---
        # Define the expected name of the parquet file created by prep.py
        parquet_filename = "data.parquet"
        parquet_file_path = input_path_obj / parquet_filename

        if not parquet_file_path.is_file():
             # Check if the MLTable file exists for a more informative error
             mltable_file_path = input_path_obj / "MLTable"
             if mltable_file_path.is_file():
                  error_msg = f"Found MLTable definition file at {mltable_file_path}, but could not find the expected data file: {parquet_file_path}"
             else:
                   error_msg = f"Neither MLTable definition nor the expected data file ({parquet_filename}) found in input directory: {input_path_obj}"
             raise FileNotFoundError(error_msg)

        print(f"[LOAD DATA] Found Parquet data file: {parquet_file_path}")
        print("[LOAD DATA] Attempting to create MLTable object directly from Parquet file...")

        # Create the mltable object directly from the specific Parquet file path
        input_tbl = mltable.from_parquet_files([{'file': str(parquet_file_path)}])

        print("[LOAD DATA] MLTable object created. Attempting to load into DataFrame...")
        # Load the full data into pandas
        transactions_df = input_tbl.to_pandas_dataframe()
        # --- MODIFICATION END ---

        print(f"[LOAD DATA] Successfully loaded data into DataFrame. Shape: {transactions_df.shape}")

        # Validate essential columns after loading
        if transactions_df.empty:
             print("[LOAD DATA] WARNING: Loaded DataFrame is empty. No training possible.")
             mlflow.log_metric("input_rows_loaded_train", 0)
             mlflow.set_tag("Training Status", "Failed - Empty DataFrame Loaded")
             mlflow.end_run(status="FAILED")
             sys.exit(1)

        if 'TX_DATETIME' not in transactions_df.columns:
            raise ValueError("Loaded DataFrame must contain 'TX_DATETIME' column for splitting.")
        # Ensure TX_DATETIME is datetime type
        if not pd.api.types.is_datetime64_any_dtype(transactions_df['TX_DATETIME']):
             print("[LOAD DATA] Converting TX_DATETIME to datetime objects...")
             transactions_df['TX_DATETIME'] = pd.to_datetime(transactions_df['TX_DATETIME'])
             print("[LOAD DATA] TX_DATETIME conversion complete.")

        # Log actual date range loaded
        min_date_loaded = transactions_df['TX_DATETIME'].min()
        max_date_loaded = transactions_df['TX_DATETIME'].max()
        print(f"[LOAD DATA] Actual loaded TX_DATETIME range: {min_date_loaded} to {max_date_loaded}")
        mlflow.log_param("actual_train_data_min_date", min_date_loaded.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(min_date_loaded) else "N/A")
        mlflow.log_param("actual_train_data_max_date", max_date_loaded.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(max_date_loaded) else "N/A")

    except FileNotFoundError as e:
        print(f"[LOAD DATA] ERROR: {e}")
        traceback.print_exc()
        mlflow.set_tag("Training Status", "Failed - Input Path/MLTable Not Found")
        mlflow.end_run(status="FAILED")
        sys.exit(1)
    except Exception as e:
        print(f"[LOAD DATA] ERROR loading input MLTable or converting to DataFrame: {e}")
        traceback.print_exc()
        mlflow.set_tag("Training Status", "Failed - Data Loading Error")
        mlflow.end_run(status="FAILED")
        sys.exit(1) # Exit script if loading failed

    load_time = time.time() - load_start_time
    print(f"[LOAD DATA] Data loading finished in {load_time:.2f} seconds.")
    mlflow.log_metric("input_rows_loaded_train", len(transactions_df))
    mlflow.log_metric("data_load_time_sec", load_time)


    # Define features and output (Assuming these columns exist in the prepared MLTable)
    OUTPUT_FEATURE = "TX_FRAUD"
    INPUT_FEATURES = ['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',
                      'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',
                      'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',
                      'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',
                      'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',
                      'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',
                      'TERMINAL_ID_RISK_30DAY_WINDOW']

    # --- Validate features exist in the loaded DataFrame ---
    print("\n[CONFIG] Validating required features in loaded DataFrame...")
    missing_input_features = [col for col in INPUT_FEATURES if col not in transactions_df.columns]
    if missing_input_features:
        print(f"[CONFIG] ERROR: Missing required input features in loaded data: {missing_input_features}")
        mlflow.set_tag("Training Status", "Failed - Missing Input Features")
        mlflow.end_run(status="FAILED")
        sys.exit(1)
    if OUTPUT_FEATURE not in transactions_df.columns:
        print(f"[CONFIG] ERROR: Missing required output feature '{OUTPUT_FEATURE}' in loaded data.")
        mlflow.set_tag("Training Status", "Failed - Missing Output Feature")
        mlflow.end_run(status="FAILED")
        sys.exit(1)

    print(f"[CONFIG] All required input features found: {INPUT_FEATURES}")
    print(f"[CONFIG] Output Feature found: {OUTPUT_FEATURE}")
    mlflow.log_param("input_features_used", json.dumps(INPUT_FEATURES))
    mlflow.log_param("output_feature_used", OUTPUT_FEATURE)


    # --- Derive Dates (Based on Anchor Date arg) ---
    print("\n[DATE DERIVATION] Calculating split dates based on anchor date...")
    try:
        start_date_training_anchor = datetime.datetime.strptime(args.anchor_date_str, "%Y-%m-%d")
        start_date_validation = start_date_training_anchor - datetime.timedelta(days=(args.delta_delay + args.delta_assessment))
        start_date_test_estimation = start_date_training_anchor
        final_train_start_date = start_date_training_anchor

        # Calculate approximate required range based on derived dates
        earliest_gs_train_start = start_date_validation - datetime.timedelta(days=(args.n_folds - 1) * args.delta_assessment)
        latest_final_test_end = final_train_start_date + datetime.timedelta(days=args.delta_train + args.delta_delay + args.delta_assessment)

        print(f"  Anchor Date: {start_date_training_anchor.strftime('%Y-%m-%d')}")
        print(f"  Derived Validation GridSearch Start: {start_date_validation.strftime('%Y-%m-%d')}")
        print(f"  Derived Test Estimation GridSearch Start: {start_date_test_estimation.strftime('%Y-%m-%d')}")
        print(f"  Derived Final Training Start: {final_train_start_date.strftime('%Y-%m-%d')}")
        print(f"  Approx. Earliest Date Needed for GridSearch Train: {earliest_gs_train_start.strftime('%Y-%m-%d')}")
        print(f"  Approx. Latest Date Needed for Final Test: {latest_final_test_end.strftime('%Y-%m-%d')}")

        mlflow.log_param("derived_validation_start_date", start_date_validation.strftime('%Y-%m-%d'))
        mlflow.log_param("derived_test_estimation_start_date", start_date_test_estimation.strftime('%Y-%m-%d'))
        mlflow.log_param("derived_final_train_start_date", final_train_start_date.strftime('%Y-%m-%d'))

        # --- REVISED DATE VALIDATION ---
        # Check if the derived splitting dates fall within the actual data loaded
        min_date_loaded = transactions_df['TX_DATETIME'].min()
        max_date_loaded = transactions_df['TX_DATETIME'].max()
        if pd.isna(min_date_loaded) or pd.isna(max_date_loaded):
             print("[DATE VALIDATION] WARNING: Could not determine date range from loaded data (contains NaNs?). Cannot validate split coverage.")
             mlflow.set_tag("Data Window Warning", "NaN dates in loaded data, split coverage unknown")
        elif earliest_gs_train_start < min_date_loaded or latest_final_test_end > max_date_loaded:
             print("[DATE VALIDATION] WARNING: Derived split date range extends beyond the actual dates present in the loaded MLTable data!")
             print(f"  Loaded Data Actual Range: {min_date_loaded.strftime('%Y-%m-%d')} to {max_date_loaded.strftime('%Y-%m-%d')}")
             print(f"  Derived Split Range Needed: Approx {earliest_gs_train_start.strftime('%Y-%m-%d')} to {latest_final_test_end.strftime('%Y-%m-%d')}")
             print("  Splitting functions might generate empty or smaller-than-expected sets. Ensure input MLTable covers the required period.")
             mlflow.set_tag("Data Window Warning", "Derived splits might exceed loaded data range")
        else:
             print("[DATE VALIDATION] OK: Loaded MLTable data date range appears to cover the derived split periods.")

    except ValueError as e:
        print(f"[DATE DERIVATION] ERROR parsing anchor date '{args.anchor_date_str}': {e}")
        mlflow.set_tag("Training Status", "Failed - Date Parsing Error")
        mlflow.end_run(status="FAILED")
        sys.exit(1) # Exit on critical date parse error
    except Exception as e:
        print(f"[DATE DERIVATION] ERROR during date calculation or validation: {e}")
        traceback.print_exc()
        mlflow.set_tag("Training Status", "Failed - Date Calculation Error")
        mlflow.end_run(status="FAILED")
        sys.exit(1) # Exit on critical date calc error

    # --- Model Selection (Grid Search for XGBoost ONLY) ---
    print("\n===== Starting Model Selection (XGBoost Only) =====")
    start_selection_time = time.time()

    classifier_xgb = xgboost.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss', n_jobs=1)
    parameters_xgb = {
        'clf__max_depth': [3, 6, 9], 'clf__n_estimators': [25, 50, 100], 'clf__learning_rate': [0.1, 0.3],
        'clf__random_state':[0], 'clf__n_jobs':[1], 'clf__verbosity':[0],
        'clf__use_label_encoder':[False], 'clf__eval_metric':['logloss']
    }
    print(f"[GRID SEARCH] XGBoost Parameter Grid: {parameters_xgb}")
    mlflow.log_param("xgboost_param_grid", json.dumps({k:str(v) for k,v in parameters_xgb.items()}))

    # Prepare scorer dataframe subset (needs TX_TIME_DAYS)
    transactions_df_scorer = pd.DataFrame()
    print("[GRID SEARCH] Preparing scorer helper dataframe...")
    try:
        # Ensure TX_TIME_DAYS exists from loading or calculation in prep
        # This column is crucial for the custom scorer logic
        scorer_cols = ['CUSTOMER_ID', 'TX_FRAUD', 'TX_TIME_DAYS']
        if not all(col in transactions_df.columns for col in scorer_cols):
             missing_scorer_cols = [col for col in scorer_cols if col not in transactions_df.columns]
             raise ValueError(f"Missing columns required for scorer helper DataFrame: {missing_scorer_cols}. Ensure they are present in the prepared MLTable.")

        transactions_df_scorer = transactions_df[scorer_cols].copy()
        print(f"[GRID SEARCH] Scorer helper DataFrame created. Shape: {transactions_df_scorer.shape}")
    except Exception as e:
        print(f"[GRID SEARCH] ERROR creating scorer helper DataFrame: {e}")
        mlflow.set_tag("Training Status", "Failed - Scorer DF Creation Error")
        mlflow.end_run(status="FAILED")
        sys.exit(1) # Exit if scorer cannot be prepared

    # Create custom scorer
    card_precision_top_k_scorer = None
    if not transactions_df_scorer.empty:
        print("[GRID SEARCH] Creating custom scorer...")
        try:
            # Ensure the scorer function and its dependencies are correct
            card_precision_top_k_scorer = sklearn.metrics.make_scorer(
                card_precision_top_k_custom, needs_proba=True,
                top_k=args.top_k_value, transactions_df=transactions_df_scorer)
            print(f"[GRID SEARCH] Custom scorer 'card_precision@{args.top_k_value}' created successfully.")
        except Exception as e:
            print(f"[GRID SEARCH] Warning: Failed to create custom scorer: {e}. Card Precision@k metric will be unavailable in GridSearchCV results.")
            # Continue without the custom scorer if it fails
            card_precision_top_k_scorer = None # Explicitly set to None

    # Define scoring dictionary for GridSearchCV
    scoring = {'roc_auc': 'roc_auc', 'average_precision': 'average_precision'}
    if card_precision_top_k_scorer:
        scoring[f'card_precision@{args.top_k_value}'] = card_precision_top_k_scorer
    # List used later to extract results from cv_results_
    performance_metrics_list_grid = list(scoring.keys())
    # List used for the summary table generation
    performance_metrics_list = ['AUC ROC', 'Average precision']
    # Add CP@k to summary list ONLY if scorer was successfully created
    if card_precision_top_k_scorer:
        performance_metrics_list.append(f'Card Precision@{args.top_k_value}')
    print(f"[GRID SEARCH] Scoring metrics for GridSearchCV: {performance_metrics_list_grid}")
    print(f"[GRID SEARCH] Metrics for summary table: {performance_metrics_list}")


    # Run model selection wrapper FOR XGBOOST ONLY
    performances_df_xgb = pd.DataFrame()
    print("[GRID SEARCH] Starting model_selection_wrapper for XGBoost...")
    try:
        # Pass the main transactions_df loaded from the MLTable
        performances_df_xgb = model_selection_wrapper(
            transactions_df, # Use the DataFrame loaded from MLTable
            classifier_xgb, INPUT_FEATURES, OUTPUT_FEATURE,
            parameters_xgb, scoring, # Pass the potentially modified scoring dict
            start_date_validation, start_date_test_estimation, # Use DERIVED dates
            n_folds=args.n_folds,
            delta_train=args.delta_train, delta_delay=args.delta_delay, delta_assessment=args.delta_assessment,
            performance_metrics_list_grid=performance_metrics_list_grid, # Pass grid keys
            performance_metrics_list=performance_metrics_list, # Pass summary keys
            n_jobs=args.n_jobs
        )
        selection_time = time.time() - start_selection_time
        print(f"[GRID SEARCH] XGBoost GridSearchCV finished in {selection_time:.2f} seconds.")
        mlflow.log_metric("xgb_gridsearch_time_sec", selection_time)

        # Log GridSearch results artifact
        if not performances_df_xgb.empty:
             print(f"[GRID SEARCH] GridSearchCV results obtained. Shape: {performances_df_xgb.shape}")
             perf_artifact_path = "xgboost_grid_search_results.csv"
             performances_df_xgb.round(5).to_csv(perf_artifact_path, index=False) # Round for consistency
             mlflow.log_artifact(perf_artifact_path)
             print(f"[GRID SEARCH] Logged XGBoost grid search results to MLflow artifact: {perf_artifact_path}")
             mlflow.set_tag("XGBoost Grid Search Status", "Completed - Success")
        else:
             print("[GRID SEARCH] Warning: XGBoost grid search returned empty results DataFrame.")
             mlflow.set_tag("XGBoost Grid Search Status", "Completed - No Results")

    except Exception as e:
        print(f"[GRID SEARCH] ERROR during XGBoost model_selection_wrapper execution: {e}")
        traceback.print_exc()
        mlflow.log_metric("xgb_gridsearch_time_sec", time.time() - start_selection_time) # Log time even on failure
        mlflow.set_tag("XGBoost Grid Search Status", f"Failed: {e}")
        # Decide if failure here is critical. For now, continue to default params.
        print("[GRID SEARCH] Proceeding with default parameters due to GridSearchCV failure.")


    # --- Determine Best Parameters for XGBoost ---
    # This logic relies on the structure of performances_df_xgb generated above
    best_params_xgb_dict = None
    best_params_xgb_summary = "Defaults"
    primary_metric = 'Average precision' # Metric to optimize (must match a name in performance_metrics_list)
    print(f"\n[BEST PARAMS] Determining best XGBoost parameters based on Validation '{primary_metric}'...")

    if not performances_df_xgb.empty:
        try:
            # Ensure 'Parameters summary' column exists (fallback if needed)
            if 'Parameters summary' not in performances_df_xgb.columns:
                 if 'Parameters' in performances_df_xgb.columns:
                     # Define fallback for creating summary string if missing
                     def params_to_str_fallback(params):
                         try:
                             if isinstance(params, dict): items = [f"{k.split('__')[1]}={v}" for k, v in sorted(params.items())]; return ", ".join(items)
                             else: return str(params) # Return raw string if not dict
                         except Exception: return str(params) # Final fallback
                     performances_df_xgb['Parameters summary'] = performances_df_xgb['Parameters'].apply(params_to_str_fallback)
                     print("[BEST PARAMS] Created fallback 'Parameters summary' column.")
                 else:
                     # If neither 'Parameters' nor 'Parameters summary' exists, we can't determine best params
                     raise KeyError("Missing 'Parameters' and 'Parameters summary' columns in GridSearchCV results.")

            # Generate summary performance table using the defined metrics list
            summary_xgb = get_summary_performances(performances_df_xgb, parameter_column_name="Parameters summary")
            print("[BEST PARAMS] Generated performance summary table:")
            print(summary_xgb) # Print the summary table
            # Log summary as artifact
            summary_artifact_path = "grid_search_summary.csv"
            summary_xgb.to_csv(summary_artifact_path)
            mlflow.log_artifact(summary_artifact_path)
            print(f"[BEST PARAMS] Logged performance summary to MLflow artifact: {summary_artifact_path}")


            # Find best parameters based on the primary validation metric
            if primary_metric in summary_xgb.columns:
                best_params_xgb_summary = summary_xgb.loc["Best estimated parameters", primary_metric]
                validation_perf_str = summary_xgb.loc["Validation performance", primary_metric]

                # Check if grid search actually found a best parameter set (not 'N/A')
                if best_params_xgb_summary != 'N/A':
                    # Find the original parameter dictionary corresponding to the best summary string
                    best_row = performances_df_xgb[performances_df_xgb['Parameters summary'] == best_params_xgb_summary]
                    if not best_row.empty:
                        best_params_xgb_dict = best_row['Parameters'].iloc[0] # Get the actual dict
                        print(f"[BEST PARAMS] Found Best XGBoost parameters (based on Validation {primary_metric}):")
                        print(f"  Summary String: {best_params_xgb_summary}")
                        print(f"  Parameter Dict: {best_params_xgb_dict}")
                        mlflow.set_tag("best_xgb_params_source", "GridSearch")
                        mlflow.log_param("best_xgb_params_summary", best_params_xgb_summary)
                        # Log the best validation score achieved
                        try:
                             if isinstance(validation_perf_str, str) and validation_perf_str not in ['N/A', 'NaN']:
                                 # Extract numeric part (before potential '+/-')
                                 best_val_score_num = float(validation_perf_str.split('+/-')[0].strip())
                                 mlflow.log_metric(f"best_validation_{primary_metric.lower().replace(' ','_').replace('@','_at_')}", best_val_score_num)
                                 print(f"[BEST PARAMS] Best Validation {primary_metric} Score: {best_val_score_num:.4f}")
                             else:
                                 print(f"[BEST PARAMS] Best Validation {primary_metric} score string is invalid or N/A: '{validation_perf_str}'")
                        except Exception as score_e:
                             print(f"[BEST PARAMS] Warning: Could not parse/log best validation score from '{validation_perf_str}': {score_e}")
                    else:
                        print(f"[BEST PARAMS] Warning: Could not find original parameters row matching best summary string '{best_params_xgb_summary}'. Using defaults.")
                        best_params_xgb_dict = None # Reset to ensure defaults are used
                else:
                    print(f"[BEST PARAMS] Best parameters summary is 'N/A' in the summary table. Using defaults.")
                    best_params_xgb_dict = None # Reset to ensure defaults are used
            else:
                print(f"[BEST PARAMS] Warning: Primary metric '{primary_metric}' not found in performance summary columns. Using defaults.")
                best_params_xgb_dict = None # Reset to ensure defaults are used

        except KeyError as e:
             print(f"[BEST PARAMS] Error: Missing expected column during best parameter determination: {e}. Using defaults.")
             best_params_xgb_dict = None
        except Exception as e:
            print(f"[BEST PARAMS] Error determining best XGBoost parameters from results: {e}. Using defaults.")
            traceback.print_exc()
            best_params_xgb_dict = None # Reset to ensure defaults are used
    else:
        print("[BEST PARAMS] XGBoost grid search results DataFrame is empty. Using default parameters.")
        best_params_xgb_dict = None # Ensure defaults are used

    # Fallback to default parameters if grid search failed or didn't find better params
    if best_params_xgb_dict is None:
        # Define the default parameters explicitly
        best_params_xgb_dict = {
            'clf__max_depth': 6, 'clf__n_estimators': 100, 'clf__learning_rate': 0.3,
            'clf__random_state': 0, 'clf__n_jobs': 1, 'clf__verbosity': 0,
            'clf__use_label_encoder': False, 'clf__eval_metric': 'logloss'
        }
        # Regenerate summary string from defaults for consistency in logging/tags
        try:
             items = [f"{k.split('__')[1]}={v}" for k, v in sorted(best_params_xgb_dict.items())]
             best_params_xgb_summary = ", ".join(items)
        except Exception: best_params_xgb_summary = str(best_params_xgb_dict) # Fallback

        print(f"[BEST PARAMS] Using default XGBoost parameters: {best_params_xgb_summary}")
        mlflow.set_tag("best_xgb_params_source", "Default")
        mlflow.log_param("best_xgb_params_summary", best_params_xgb_summary) # Log the defaults used


    # --- Train Final XGBoost Model ---
    print("\n===== Training Final XGBoost Model =====")

    # Prepare final training data split using the full loaded DataFrame and derived dates
    final_train_df = pd.DataFrame()
    final_test_df = pd.DataFrame()
    print(f"[FINAL SPLIT] Creating final train/test split using derived start date: {final_train_start_date.strftime('%Y-%m-%d')}")
    try:
        # Ensure the split function uses the DataFrame loaded from MLTable
        (final_train_df, final_test_df) = get_train_test_set(
            transactions_df, # Use the DataFrame loaded from MLTable
            start_date_training=final_train_start_date, # Use derived date
            delta_train=args.delta_train, delta_delay=args.delta_delay, delta_test=args.delta_assessment
        )
        print(f"[FINAL SPLIT] Final training set shape: {final_train_df.shape}")
        print(f"[FINAL SPLIT] Final test set shape: {final_test_df.shape}")
        mlflow.log_metric("final_train_rows", final_train_df.shape[0])
        mlflow.log_metric("final_test_rows", final_test_df.shape[0])
        if not final_train_df.empty: mlflow.log_metric("final_train_fraud_rate", final_train_df[OUTPUT_FEATURE].mean())
        if not final_test_df.empty: mlflow.log_metric("final_test_fraud_rate", final_test_df[OUTPUT_FEATURE].mean())

        # --- CRITICAL CHECK: Ensure final training data is not empty ---
        if final_train_df.empty:
            print("[FINAL SPLIT] ERROR: Final training set is empty after splitting. Cannot train model.")
            mlflow.set_tag("Training Status", "Failed - Empty Final Train Split")
            mlflow.end_run(status="FAILED")
            sys.exit(1) # Exit script with failure code
        else:
             print("[FINAL SPLIT] Final training set is not empty. Proceeding with training.")

    except Exception as e:
        print(f"[FINAL SPLIT] ERROR creating final train/test split: {e}")
        traceback.print_exc()
        mlflow.set_tag("Training Status", "Failed - Final Split Error")
        mlflow.end_run(status="FAILED")
        sys.exit(1) # Exit script with failure code

    # Create final pipeline with selected/default parameters
    print("[FINAL TRAIN] Creating final pipeline with selected/default XGBoost parameters...")
    final_classifier_xgb = xgboost.XGBClassifier() # Base instance
    # Prepare parameters dictionary (remove 'clf__' prefix needed for GridSearchCV)
    final_params_xgb_filtered = {}
    try:
        final_params_xgb_filtered = {k.split('__', 1)[1]: v for k, v in best_params_xgb_dict.items() if k.startswith('clf__')}
        # Ensure boolean/int types are correct if they came from strings (less likely now but good practice)
        if 'use_label_encoder' in final_params_xgb_filtered: final_params_xgb_filtered['use_label_encoder'] = bool(str(final_params_xgb_filtered['use_label_encoder']).lower() == 'true')
        if 'verbosity' in final_params_xgb_filtered: final_params_xgb_filtered['verbosity'] = int(final_params_xgb_filtered['verbosity'])
        if 'n_jobs' in final_params_xgb_filtered: final_params_xgb_filtered['n_jobs'] = int(final_params_xgb_filtered['n_jobs'])
        if 'random_state' in final_params_xgb_filtered: final_params_xgb_filtered['random_state'] = int(final_params_xgb_filtered['random_state'])
        # Set parameters on the classifier instance
        final_classifier_xgb.set_params(**final_params_xgb_filtered)
        print(f"[FINAL TRAIN] Set final XGBoost classifier parameters: {final_params_xgb_filtered}")
    except Exception as e:
        print(f"[FINAL TRAIN] Warning: Error processing/setting final XGBoost params: {e}. Using defaults.")
        # Reset to known defaults if setting params failed
        final_classifier_xgb = xgboost.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss', n_jobs=1)

    # Define the final scikit-learn pipeline (Scaler + Classifier)
    final_pipeline = sklearn.pipeline.Pipeline([
        ('scaler', sklearn.preprocessing.StandardScaler()), # Standard scaler remains
        ('clf', final_classifier_xgb) # Use the configured classifier
    ])
    print(f"[FINAL TRAIN] Final scikit-learn pipeline created: {final_pipeline}")

    # Fit final pipeline on the final training data
    print("[FINAL TRAIN] Starting final model fitting...")
    start_fit_time = time.time()
    try:
        X_train_final = final_train_df[INPUT_FEATURES]
        y_train_final = final_train_df[OUTPUT_FEATURE]
        print(f"[FINAL TRAIN] Input features shape for final fit: {X_train_final.shape}")
        print(f"[FINAL TRAIN] Output feature shape for final fit: {y_train_final.shape}")

        # Check for NaNs before fitting (Scaler should handle, but good practice)
        if X_train_final.isnull().values.any():
            nan_cols = X_train_final.columns[X_train_final.isnull().any()].tolist()
            print(f"[FINAL TRAIN] Warning: NaNs detected in final training features before fitting (Columns: {nan_cols}). Scaler should handle or raise error.")
            # Optional: Add explicit imputation here if pipeline doesn't handle it
            # Example: X_train_final = SimpleImputer(strategy='mean').fit_transform(X_train_final)

        # Fit the pipeline
        final_pipeline.fit(X_train_final, y_train_final)
        final_fit_time = time.time() - start_fit_time
        print(f"[FINAL TRAIN] Final XGBoost model pipeline fitting completed successfully in {final_fit_time:.2f} seconds.")
        mlflow.log_metric("final_model_train_time_sec", final_fit_time)
        mlflow.set_tag("Final Model Training Status", "Success")

        # --- Log and Save Final Model ---
        print(f"\n[MODEL LOGGING] Attempting to log final pipeline to MLflow run...")
        # The output path is provided by the Azure ML pipeline runtime
        print(f"[MODEL LOGGING] Target output path variable for model: {args.model_output}")
        try:
            # Save the scikit-learn pipeline using mlflow.sklearn
            mlflow.sklearn.save_model(
                sk_model=final_pipeline,
                path=args.model_output,  # Save to the path specified by the pipeline output
                serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,
                # Optionally include signature and input example for better tracking/deployment
                # signature=infer_signature(X_train_final, y_train_final),
                # input_example=X_train_final.head(5)
            )
            print(f"[MODEL LOGGING] mlflow.sklearn.save_model call completed. Model saved to path: {args.model_output}")
            mlflow.set_tag("Model Logging Status", "Success")
            # Note: Azure ML automatically uploads the content of args.model_output path
            # as the pipeline output artifact named 'model_output'.

        except Exception as log_e:
             print(f"[MODEL LOGGING] ERROR during mlflow.sklearn.save_model: {log_e}")
             traceback.print_exc()
             mlflow.set_tag("Model Logging Status", f"Failed: {log_e}")
             mlflow.set_tag("Training Status", "Failed - Model Logging Error")
             mlflow.end_run(status="FAILED")
             sys.exit(1) # Exit script with failure code

        # --- Save Final Test Data Split to Output Path ---
        # This part remains the same, as evaluate.py still expects a .pkl file
        print(f"\n[TEST DATA SAVE] Saving final test data split to output path: {args.test_data_output}")
        test_data_output_path = Path(args.test_data_output)
        test_data_output_path.mkdir(parents=True, exist_ok=True) # Ensure directory exists
        test_data_file = test_data_output_path / "final_test_data.pkl"
        try:
            if not final_test_df.empty:
                # Save the test split DataFrame as a pickle file
                final_test_df.to_pickle(test_data_file)
                print(f"[TEST DATA SAVE] Final test data DataFrame saved successfully to {test_data_file}")
                mlflow.set_tag("Test Data Save Status", "Success")
            else:
                print("[TEST DATA SAVE] Warning: Final test data split is empty, not saving the pickle file.")
                # Create an empty file perhaps, or just log? Creating empty might be safer downstream.
                test_data_file.touch() # Create empty file marker
                print(f"[TEST DATA SAVE] Created empty marker file at {test_data_file}")
                mlflow.set_tag("Test Data Save Status", "Skipped - Empty Split")
        except Exception as e:
            print(f"[TEST DATA SAVE] ERROR saving final test data pickle file: {e}")
            traceback.print_exc()
            mlflow.set_tag("Test Data Save Status", f"Failed: {e}")
            # Consider if this failure is critical. Maybe not, if eval can handle missing file?
            # For now, log error but don't fail the run.

    except Exception as e:
        print(f"[FINAL TRAIN] ERROR fitting final XGBoost model pipeline: {e}")
        traceback.print_exc()
        mlflow.log_metric("final_model_train_time_sec", time.time() - start_fit_time) # Log time even on failure
        mlflow.set_tag("Final Model Training Status", f"Failed: {e}")
        mlflow.set_tag("Training Status", "Failed - Final Fit Error")
        mlflow.end_run(status="FAILED")
        sys.exit(1) # Exit script with failure code

    # --- End of Script ---
    mlflow.set_tag("Training Status", "Completed Successfully") # Set status if reached here
    mlflow.end_run()
    print(f"===== Training Script Finished Successfully (MLflow Run ID: {run_id}) =====")

if __name__ == "__main__":
    # This block executes when the script is run directly
    print("--- Script execution started ---")
    args = parse_args()
    main(args)
    print("--- Script execution finished ---")

================
File: data-science/src/utils.py
================
# data-science/src/utils.py
# Contains shared functions adapted from the user's notebooks

import os
import datetime
import time
import pickle
import json
import pandas as pd
import numpy as np

import sklearn
from sklearn import metrics, preprocessing, model_selection, pipeline, tree, ensemble, linear_model

# === Data Loading/Saving ===

def save_object(obj, filename):
    """Saves object as pickle file."""
    with open(filename, 'wb') as output:
        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)

# === Data Preprocessing & Transformation ===

def is_weekend(tx_datetime):
    """Checks if a datetime object falls on a weekend."""
    weekday = tx_datetime.weekday() # Monday is 0 and Sunday is 6
    return int(weekday >= 5)

def is_night(tx_datetime):
    """Checks if a datetime object's time is between 00:00 and 06:00."""
    tx_hour = tx_datetime.hour
    return int(tx_hour <= 6)

def get_customer_spending_behaviour_features(customer_transactions, windows_size_in_days=[1,7,30]):
    """Calculates customer spending behavior features over specified windows."""
    # Let us first order transactions chronologically
    # Ensure the input dataframe doesn't already have TRANSACTION_ID as index
    customer_transactions = customer_transactions.reset_index(drop=True)
    customer_transactions=customer_transactions.sort_values('TX_DATETIME')

    # The transaction date and time is set as the index, which will allow the use of the rolling function
    # Make a temporary copy to avoid modifying the original slice's index directly if needed elsewhere
    temp_df = customer_transactions.copy()
    temp_df.index=temp_df.TX_DATETIME

    # For each window size
    for window_size in windows_size_in_days:

        # Compute the sum of the transaction amounts and the number of transactions for the given window size
        SUM_AMOUNT_TX_WINDOW=temp_df['TX_AMOUNT'].rolling(str(window_size)+'d').sum()
        NB_TX_WINDOW=temp_df['TX_AMOUNT'].rolling(str(window_size)+'d').count()

        # Compute the average transaction amount for the given window size
        AVG_AMOUNT_TX_WINDOW=SUM_AMOUNT_TX_WINDOW/NB_TX_WINDOW

        # Save feature values back to the original dataframe slice (using its original index)
        # Ensure the lists have the same length as the original slice's index
        customer_transactions['CUSTOMER_ID_NB_TX_'+str(window_size)+'DAY_WINDOW'] = list(NB_TX_WINDOW.values)
        customer_transactions['CUSTOMER_ID_AVG_AMOUNT_'+str(window_size)+'DAY_WINDOW'] = list(AVG_AMOUNT_TX_WINDOW.values)

    # *** NO LONGER SET INDEX TO TRANSACTION_ID ***
    # customer_transactions.index=customer_transactions.TRANSACTION_ID # <<< REMOVED

    # The dataframe 'customer_transactions' still has its original index (likely integer)
    # The 'apply' function in prep.py will handle combining these results.
    return customer_transactions

# --- CORRECTED FUNCTION 2 ---
def get_count_risk_rolling_window(terminal_transactions, delay_period=7, windows_size_in_days=[1,7,30], feature="TERMINAL_ID"):
    """Calculates terminal risk features over specified windows with a delay."""
    # Ensure the input dataframe doesn't already have TRANSACTION_ID as index
    terminal_transactions = terminal_transactions.reset_index(drop=True)
    terminal_transactions=terminal_transactions.sort_values('TX_DATETIME')

    # Set index for rolling calculation - Use a temporary copy
    temp_df = terminal_transactions.copy()
    temp_df.index=temp_df.TX_DATETIME

    # Calculate frauds and transactions within the delay period immediately preceding the current transaction
    NB_FRAUD_DELAY=temp_df['TX_FRAUD'].rolling(str(delay_period)+'d').sum()
    NB_TX_DELAY=temp_df['TX_FRAUD'].rolling(str(delay_period)+'d').count()

    # Calculate frauds and transactions for the window size plus the delay period
    for window_size in windows_size_in_days:

        NB_FRAUD_DELAY_WINDOW=temp_df['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').sum()
        NB_TX_DELAY_WINDOW=temp_df['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').count()

        # Calculate frauds and transactions for the specific window (shifted back by delay)
        NB_FRAUD_WINDOW=NB_FRAUD_DELAY_WINDOW-NB_FRAUD_DELAY
        NB_TX_WINDOW=NB_TX_DELAY_WINDOW-NB_TX_DELAY

        # Calculate risk score for the window
        RISK_WINDOW = (NB_FRAUD_WINDOW / NB_TX_WINDOW).fillna(0)

        # Save feature values back to the original dataframe slice
        terminal_transactions[feature+'_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW.values)
        terminal_transactions[feature+'_RISK_'+str(window_size)+'DAY_WINDOW']=list(RISK_WINDOW.values)

    # *** NO LONGER SET INDEX TO TRANSACTION_ID ***
    # terminal_transactions.index=terminal_transactions.TRANSACTION_ID # <<< REMOVED

    # Replace any remaining NA values in the original slice
    terminal_transactions.fillna(0,inplace=True)

    # Return the original slice with added columns and original index
    return terminal_transactions

# === Train/Test Splitting ===

def get_train_test_set(transactions_df,
                       start_date_training,
                       delta_train=7,delta_delay=7,delta_test=7,
                       sampling_ratio=1.0,
                       random_state=0):
    
    # Validate inputs
    if 'TX_DATETIME' not in transactions_df.columns or 'TX_TIME_DAYS' not in transactions_df.columns or 'CUSTOMER_ID' not in transactions_df.columns or 'TX_FRAUD' not in transactions_df.columns or 'TRANSACTION_ID' not in transactions_df.columns:
        raise ValueError("Missing required columns in transactions_df for get_train_test_set")
    if not isinstance(start_date_training, datetime.datetime):
         raise ValueError("start_date_training must be a datetime object")
         
    # Get the training set data
    train_df = transactions_df[(transactions_df.TX_DATETIME>=start_date_training) &
                               (transactions_df.TX_DATETIME<start_date_training+datetime.timedelta(days=delta_train))]

    # Get the test set data
    test_df = []

    # Note: Cards known to be compromised after the delay period are removed from the test set
    # That is, for each test day, all frauds known at (test_day-delay_period) are removed

    # First, get known defrauded customers from the training set
    known_defrauded_customers = set(train_df[train_df.TX_FRAUD==1].CUSTOMER_ID)

    # Get the relative starting day of training set (easier than TX_DATETIME to collect test data)
    if train_df.empty:
        print(f"Warning: Training period starting {start_date_training.strftime('%Y-%m-%d')} is empty.")
        # Return empty DataFrames matching expected columns
        return (train_df.copy(), pd.DataFrame(columns=transactions_df.columns))
        
    start_tx_time_days_training = train_df.TX_TIME_DAYS.min()

    # Then, for each day of the test set
    for day in range(delta_test):

        # Get test data for that day
        test_day_date = start_tx_time_days_training + delta_train + delta_delay + day
        test_df_day = transactions_df[transactions_df.TX_TIME_DAYS == test_day_date]

        # Compromised cards from that test day, minus the delay period, are added to the pool of known defrauded customers
        # **Correction:** Original notebook used `day-1` relative to `start_tx_time_days_training+delta_train` for delay period check, NOT `test_day_date - delta_delay`
        delay_period_check_day = start_tx_time_days_training + delta_train + day - 1 
        test_df_day_delay_period = transactions_df[transactions_df.TX_TIME_DAYS == delay_period_check_day]

        new_defrauded_customers = set(test_df_day_delay_period[test_df_day_delay_period.TX_FRAUD==1].CUSTOMER_ID)
        known_defrauded_customers = known_defrauded_customers.union(new_defrauded_customers)

        test_df_day = test_df_day[~test_df_day.CUSTOMER_ID.isin(known_defrauded_customers)]

        test_df.append(test_df_day)

    if not test_df:
        print(f"Warning: Test period for training start {start_date_training.strftime('%Y-%m-%d')} resulted in an empty set after filtering.")
        test_df = pd.DataFrame(columns=transactions_df.columns)
    else:
        test_df = pd.concat(test_df)

    # If subsample
    if sampling_ratio<1:

        train_df_frauds=train_df[train_df.TX_FRAUD==1].sample(frac=sampling_ratio, random_state=random_state)
        train_df_genuine=train_df[train_df.TX_FRAUD==0].sample(frac=sampling_ratio, random_state=random_state)
        train_df=pd.concat([train_df_frauds,train_df_genuine])

    # Sort data sets by ascending order of transaction ID
    train_df=train_df.sort_values('TRANSACTION_ID')
    test_df=test_df.sort_values('TRANSACTION_ID')

    return (train_df, test_df)


def prequentialSplit_with_dates(transactions_df,
                                start_date_training,
                                n_folds=4,
                                delta_train=7,
                                delta_delay=7,
                                delta_assessment=7):
    """
    Generates prequential splits, returning indices and printing date ranges for each fold.

    Args:
        transactions_df (pd.DataFrame): DataFrame with transaction data (must have index and date info).
        start_date_training (datetime.datetime): The *latest* training start date
                                                 (used for fold 0). Folds go back in time.
        n_folds (int): Number of folds.
        delta_train (int): Duration of the training period in days.
        delta_delay (int): Duration of the delay period in days.
        delta_assessment (int): Duration of the assessment (test) period in days.

    Returns:
        list: A list of tuples, where each tuple contains (indices_train, indices_test)
              for a fold. Matches the original return type for compatibility with GridSearchCV.
              Returns an empty list if no valid folds are generated.
        Prints: Detailed date ranges for each fold's train, delay, and test periods.
    """
    prequential_split_indices = []
    print(f"\n--- Generating Prequential Folds (n_folds={n_folds}) ---")
    print(f"Base Start Date (Fold 0 Train Start): {start_date_training.strftime('%Y-%m-%d')}")
    print(f"Deltas: Train={delta_train}, Delay={delta_delay}, Assessment={delta_assessment}")
    print("-" * 60)

    # For each fold
    for fold in range(n_folds):
        # Shift back start date for training by the fold index times the assessment period
        start_date_training_fold = start_date_training - datetime.timedelta(days=fold * delta_assessment)

        # Calculate all date boundaries for this fold
        # End dates represent the start of the *next* period (exclusive end)
        end_date_training_fold = start_date_training_fold + datetime.timedelta(days=delta_train)
        start_date_delay_fold = end_date_training_fold
        end_date_delay_fold = start_date_delay_fold + datetime.timedelta(days=delta_delay)
        start_date_test_fold = end_date_delay_fold
        end_date_test_fold = start_date_test_fold + datetime.timedelta(days=delta_assessment)

        # Calculate inclusive end dates for printing clarity
        inclusive_end_train = end_date_training_fold - datetime.timedelta(days=1)
        inclusive_end_delay = end_date_delay_fold - datetime.timedelta(days=1)
        inclusive_end_test = end_date_test_fold - datetime.timedelta(days=1)

        print(f"Fold {fold}:")
        print(f"  Train Period: {start_date_training_fold.strftime('%Y-%m-%d')} to {inclusive_end_train.strftime('%Y-%m-%d')} ({delta_train} days)")
        print(f"  Delay Period: {start_date_delay_fold.strftime('%Y-%m-%d')} to {inclusive_end_delay.strftime('%Y-%m-%d')} ({delta_delay} days)")
        print(f"  Test Period:  {start_date_test_fold.strftime('%Y-%m-%d')} to {inclusive_end_test.strftime('%Y-%m-%d')} ({delta_assessment} days)")

        # Get the training and test (assessment) sets using the original function logic
        # This function uses the start dates and deltas to select the correct data slices
        try:
            (train_df, test_df) = get_train_test_set(transactions_df,
                                                   start_date_training=start_date_training_fold,
                                                   delta_train=delta_train,
                                                   delta_delay=delta_delay,
                                                   delta_test=delta_assessment)
        except Exception as e:
            print(f"  -> ERROR calling get_train_test_set for fold {fold}: {e}")
            print(f"     Skipping fold {fold}.")
            print("-" * 10)
            continue # Skip to next fold

        # Get the indices from the two sets, and add them to the list of prequential splits
        # Check if sets are empty before getting indices
        if not train_df.empty and not test_df.empty:
            indices_train = list(train_df.index)
            indices_test = list(test_df.index)
            prequential_split_indices.append((indices_train, indices_test))
            print(f"  -> Train size: {len(indices_train)}, Test size: {len(indices_test)}. Added fold indices.")
        else:
             # Use the warning from the original user code
             print(f"  -> Warning (prequentialSplit): Fold {fold} generated empty train ({train_df.shape}) or test ({test_df.shape}) set for start date {start_date_training_fold.strftime('%Y-%m-%d')}. Skipping fold.")
        print("-" * 10) # Separator between folds

    if not prequential_split_indices:
        # Use the warning from the original user code
        print(f"Warning (prequentialSplit): No valid folds generated for start date {start_date_training.strftime('%Y-%m-%d')} and {n_folds} folds.")

    print("--- Finished Generating Prequential Folds ---")
    # Return the original format (list of tuples of indices) for compatibility
    return prequential_split_indices


# === Performance Assessment ===

def card_precision_top_k_day(df_day,top_k):
    """Computes card precision top k for a single day."""
    # Ensure required columns are present
    required = ['CUSTOMER_ID', 'predictions', 'TX_FRAUD']
    if not all(col in df_day.columns for col in required):
        missing = [col for col in required if col not in df_day.columns]
        print(f"Warning (card_precision_top_k_day): Missing columns {missing}. Returning empty list, 0.")
        return [], 0.0
    if df_day.empty:
        return [], 0.0

    # Group by customer, take max prediction and fraud flag
    df_day_grouped = df_day.groupby('CUSTOMER_ID').agg(
        {'predictions': 'max', 'TX_FRAUD': 'max'}
    ).sort_values(by="predictions", ascending=False).reset_index()

    # Get top k customers
    df_day_top_k = df_day_grouped.head(top_k)
    # Get list of customer IDs from the top k that are actually fraudulent
    list_detected_compromised_cards = list(df_day_top_k[df_day_top_k.TX_FRAUD == 1].CUSTOMER_ID)

    # Compute precision, handle k=0
    if top_k > 0:
        card_precision_top_k = len(list_detected_compromised_cards) / top_k
    else:
        card_precision_top_k = 0.0

    return list_detected_compromised_cards, card_precision_top_k


def card_precision_top_k(predictions_df, top_k, remove_detected_compromised_cards=True):
    """Computes average card precision top k over multiple days."""
    required = ['TX_TIME_DAYS', 'CUSTOMER_ID', 'predictions', 'TX_FRAUD']
    if not all(col in predictions_df.columns for col in required):
        missing = [col for col in required if col not in predictions_df.columns]
        raise ValueError(f"Missing required columns in predictions_df for card_precision_top_k: {missing}")

    list_days=sorted(predictions_df['TX_TIME_DAYS'].unique())
    list_detected_compromised_cards = []
    card_precision_top_k_per_day_list = []
    nb_compromised_cards_per_day = [] # For reference

    for day in list_days:
        df_day = predictions_df[predictions_df['TX_TIME_DAYS'] == day].copy()
        # Filter out already detected cards if required
        if remove_detected_compromised_cards:
             df_day = df_day[~df_day.CUSTOMER_ID.isin(list_detected_compromised_cards)]

        if df_day.empty:
             nb_compromised_cards_per_day.append(0)
             card_precision_top_k_per_day_list.append(0.0)
             continue

        # Keep track of total compromised cards on this day (before filtering for top k)
        nb_compromised_cards_per_day.append(len(df_day[df_day.TX_FRAUD == 1].CUSTOMER_ID.unique()))

        # Calculate daily precision
        detected_compromised_cards, card_precision_top_k_daily = card_precision_top_k_day(
            df_day[['CUSTOMER_ID', 'predictions', 'TX_FRAUD']], top_k
        )
        card_precision_top_k_per_day_list.append(card_precision_top_k_daily)

        # Update list of detected cards for next day's filtering
        if remove_detected_compromised_cards:
            list_detected_compromised_cards.extend(detected_compromised_cards)
            list_detected_compromised_cards = list(set(list_detected_compromised_cards)) # Keep unique

    # Compute the mean, handle empty list
    mean_card_precision_top_k = np.mean(card_precision_top_k_per_day_list) if card_precision_top_k_per_day_list else 0.0

    return nb_compromised_cards_per_day, card_precision_top_k_per_day_list, mean_card_precision_top_k


def card_precision_top_k_custom(y_true, y_pred, top_k, transactions_df):

    # Check inputs
    if not isinstance(y_true, pd.Series) or not isinstance(transactions_df, pd.DataFrame):
         print("Warning (CP@k scorer): y_true must be a pandas Series and transactions_df a DataFrame.")
         return 0.0
    if len(y_pred) != len(y_true):
        print("Warning (CP@k scorer): y_pred and y_true have different lengths.")
        return 0.0
    if transactions_df.empty:
         print("Warning (CP@k scorer): transactions_df is empty.")
         return 0.0
         
    # Let us create a predictions_df DataFrame, that contains all transactions matching the indices of the current fold
    # (indices of the y_true vector)
    current_fold_indices = y_true.index
    # Ensure indices are present in the main transaction df
    valid_indices = current_fold_indices.intersection(transactions_df.index)
    if valid_indices.empty:
        print(f"Warning (CP@k scorer): No matching indices found in transactions_df for the current fold ({len(current_fold_indices)} indices).")
        return 0.0
        
    predictions_df=transactions_df.loc[valid_indices].copy()
    
    # Add predictions ensuring alignment with potentially filtered valid_indices
    # Create a Series from y_pred with the original fold indices
    y_pred_series = pd.Series(y_pred, index=current_fold_indices)
    # Select only the predictions corresponding to valid_indices
    predictions_df['predictions'] = y_pred_series.loc[valid_indices]

    # Compute the CP@k using the function implemented in Chapter 4, Section 4.2
    nb_compromised_cards_per_day,card_precision_top_k_per_day_list,mean_card_precision_top_k= \
        card_precision_top_k(predictions_df, top_k)

    # Return the mean_card_precision_top_k
    return mean_card_precision_top_k


def performance_assessment(predictions_df, output_feature='TX_FRAUD',
                           prediction_feature='predictions', top_k_list=[100],
                           rounded=True):
    """Calculates standard and custom performance metrics."""
    required = [output_feature, prediction_feature]
    if not all(col in predictions_df.columns for col in required):
        missing = [col for col in required if col not in predictions_df.columns]
        raise ValueError(f"Missing required columns for performance_assessment: {missing}")

    y_true = predictions_df[output_feature]
    y_pred_proba = predictions_df[prediction_feature]

    AUC_ROC = np.nan
    AP = np.nan
    if len(y_true.unique()) > 1: # Check for multiple classes
        try:
            AUC_ROC = metrics.roc_auc_score(y_true, y_pred_proba)
            AP = metrics.average_precision_score(y_true, y_pred_proba)
        except ValueError as e:
            print(f"Warning (performance_assessment): ValueError calculating AUC/AP: {e}")
    else:
        print("Warning (performance_assessment): Only one class present. AUC ROC and Average Precision are undefined.")

    performances = pd.DataFrame([[AUC_ROC, AP]], columns=['AUC ROC', 'Average precision'])

    # Add CP@k metric(s)
    cpk_required = ['TX_TIME_DAYS', 'CUSTOMER_ID']
    if all(col in predictions_df.columns for col in cpk_required):
        for top_k in top_k_list:
            try:
                _, _, mean_card_precision_top_k = card_precision_top_k(predictions_df, top_k)
                performances[f'Card Precision@{top_k}'] = mean_card_precision_top_k
            except Exception as e:
                 print(f"Warning (performance_assessment): Error calculating CP@{top_k}: {e}")
                 performances[f'Card Precision@{top_k}'] = np.nan
    else:
        missing_cpk = [col for col in cpk_required if col not in predictions_df.columns]
        print(f"Warning (performance_assessment): Skipping Card Precision@k calculation due to missing columns: {missing_cpk}.")
        for top_k in top_k_list:
             performances[f'Card Precision@{top_k}'] = np.nan

    if rounded:
        performances = performances.round(3)

    return performances


def get_class_from_fraud_probability(fraud_probabilities, threshold=0.5):
    """Converts probabilities to binary classes based on a threshold."""
    predicted_classes = [1 if p >= threshold else 0 for p in fraud_probabilities]
    return predicted_classes


def threshold_based_metrics(fraud_probabilities, true_label, thresholds_list):
    """Calculates various threshold-based metrics."""
    results = []
    for threshold in thresholds_list:
        predicted_classes = get_class_from_fraud_probability(fraud_probabilities, threshold=threshold)
        try:
            cm = metrics.confusion_matrix(true_label, predicted_classes, labels=[0, 1])
            TN, FP, FN, TP = cm.ravel()
        except ValueError as e:
            print(f"Warning (threshold_metrics): Confusion matrix error for threshold {threshold}: {e}")
            TN, FP, FN, TP = 0, 0, 0, 0

        # Calculations with division-by-zero handling
        total = TN + FP + FN + TP
        MME = (FP + FN) / total if total > 0 else 0
        TPR = TP / (TP + FN) if (TP + FN) > 0 else 0
        TNR = TN / (TN + FP) if (TN + FP) > 0 else 0
        FPR = FP / (TN + FP) if (TN + FP) > 0 else 0
        FNR = FN / (TP + FN) if (TP + FN) > 0 else 0
        BER = 0.5 * (FPR + FNR)
        Gmean = np.sqrt(TPR * TNR) if TPR >= 0 and TNR >= 0 else 0 # Ensure non-negative for sqrt
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        NPV = TN / (TN + FN) if (TN + FN) > 0 else 0
        FDR = FP / (TP + FP) if (TP + FP) > 0 else 0
        FOR = FN / (TN + FN) if (TN + FN) > 0 else 0
        F1_score = 2 * (precision * TPR) / (precision + TPR) if (precision + TPR) > 0 else 0

        results.append([threshold, MME, TPR, TNR, FPR, FNR, BER, Gmean, precision, NPV, FDR, FOR, F1_score])

    results_df = pd.DataFrame(results, columns=['Threshold', 'MME', 'TPR', 'TNR', 'FPR', 'FNR', 'BER', 'G-mean', 'Precision', 'NPV', 'FDR', 'FOR', 'F1 Score'])
    return results_df


# === Model Selection ===

def params_to_str(params):
    """Helper function to convert parameter dict to a readable, sorted string summary."""
    if not isinstance(params, dict):
        return str(params)
    try:
         items = [f"{k.split('__')[1]}={v}" for k, v in sorted(params.items())]
         return ", ".join(items)
    except Exception:
         return str(params) # Fallback


def prequential_grid_search(transactions_df,
                            classifier,
                            input_features, output_feature,
                            parameters, scoring,
                            start_date_training,
                            n_folds=4,
                            expe_type='Test',
                            delta_train=7,
                            delta_delay=7,
                            delta_assessment=7,
                            performance_metrics_list_grid=['roc_auc'],
                            performance_metrics_list=['AUC ROC'],
                            n_jobs=-1):
    """Performs GridSearchCV using prequential splitting."""
    # Input validation
    if transactions_df.empty:
         print(f"ERROR (prequential_grid_search): Input transactions_df is empty for {expe_type}.")
         return pd.DataFrame() # Return empty matching expected structure later
    if not scoring:
         print(f"ERROR (prequential_grid_search): scoring dictionary is empty for {expe_type}.")
         return pd.DataFrame()

    # Create pipeline
    estimators = [('scaler', sklearn.preprocessing.StandardScaler()), ('clf', classifier)]
    pipe = sklearn.pipeline.Pipeline(estimators)

    # Generate prequential splits (returns list of (train_indices, test_indices))
    prequential_split_indices = prequentialSplit_with_dates(transactions_df,
                                                        start_date_training=start_date_training,
                                                        n_folds=n_folds,
                                                        delta_train=delta_train,
                                                        delta_delay=delta_delay,
                                                        delta_assessment=delta_assessment)

    if not prequential_split_indices:
         print(f"ERROR (prequential_grid_search): No valid prequential splits generated for {expe_type}. Cannot run GridSearchCV.")
         return pd.DataFrame() # Return empty matching expected structure later

    # Setup GridSearchCV
    grid_search = sklearn.model_selection.GridSearchCV(pipe, parameters, scoring=scoring,
                                                       cv=prequential_split_indices, # Use generated indices
                                                       refit=False, # We only care about CV results here
                                                       n_jobs=n_jobs,
                                                       return_train_score=False)

    X = transactions_df[input_features]
    y = transactions_df[output_feature]

    # Handle NaNs (Pipeline's scaler will handle or raise error)
    if X.isnull().values.any():
        print(f"Warning (prequential_grid_search): NaNs detected in features for {expe_type}. Pipeline's StandardScaler should handle this.")
        # If imputation outside pipeline is needed, add it here.

    print(f"Starting GridSearchCV for {expe_type} set (Classifier: {classifier.__class__.__name__})...")
    try:
        grid_search.fit(X, y)
    except Exception as e:
        print(f"ERROR (prequential_grid_search): GridSearchCV fit failed for {expe_type}: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame() # Return empty on failure

    print(f"Finished GridSearchCV for {expe_type} set.")

    # Extract results
    performances_df = pd.DataFrame()
    cv_results = grid_search.cv_results_

    # Map grid keys to display names (assuming lists align)
    metric_mapping = dict(zip(performance_metrics_list_grid, performance_metrics_list))

    for grid_key, display_name in metric_mapping.items():
        mean_score_key = f'mean_test_{grid_key}'
        std_score_key = f'std_test_{grid_key}'

        if mean_score_key in cv_results:
            performances_df[f'{display_name} {expe_type}'] = cv_results[mean_score_key]
        else:
            print(f"Warning: Mean score key '{mean_score_key}' not found in cv_results_ for {expe_type}.")
            performances_df[f'{display_name} {expe_type}'] = np.nan

        if std_score_key in cv_results:
            performances_df[f'{display_name} {expe_type} Std'] = cv_results[std_score_key]
        else:
            # print(f"Note: Std score key '{std_score_key}' not found in cv_results_ for {expe_type}.") # Less verbose
            performances_df[f'{display_name} {expe_type} Std'] = np.nan # Use NaN for missing std


    if 'params' in cv_results:
        performances_df['Parameters'] = cv_results['params']
        performances_df['Parameters summary'] = performances_df['Parameters'].apply(params_to_str)
    else:
        print("Warning: 'params' key not found in cv_results_. Adding empty parameter columns.")
        num_rows = len(next(iter(cv_results.values()))) # Get length from another column
        performances_df['Parameters'] = [{} for _ in range(num_rows)]
        performances_df['Parameters summary'] = 'N/A'


    if 'mean_fit_time' in cv_results:
        performances_df['Execution time'] = cv_results['mean_fit_time']
    else:
        print("Warning: 'mean_fit_time' key not found in cv_results_.")
        performances_df['Execution time'] = np.nan

    return performances_df


def model_selection_wrapper(transactions_df,
                            classifier,
                            input_features, output_feature,
                            parameters,
                            scoring,
                            start_date_training_for_valid,
                            start_date_training_for_test,
                            n_folds=4,
                            delta_train=7,
                            delta_delay=7,
                            delta_assessment=7,
                            performance_metrics_list_grid=['roc_auc'],
                            performance_metrics_list=['AUC ROC'],
                            n_jobs=-1):
    """Wraps prequential grid search for validation and test estimation."""
    # Get performances on the validation set
    print("--- Running Prequential Grid Search for Validation Set ---")
    performances_df_validation = prequential_grid_search(
        transactions_df, classifier,
        input_features, output_feature,
        parameters, scoring,
        start_date_training=start_date_training_for_valid,
        n_folds=n_folds, expe_type='Validation',
        delta_train=delta_train, delta_delay=delta_delay, delta_assessment=delta_assessment,
        performance_metrics_list_grid=performance_metrics_list_grid,
        performance_metrics_list=performance_metrics_list, n_jobs=n_jobs
    )

    # Get performances on the test set estimation
    print("--- Running Prequential Grid Search for Test Set Estimation ---")
    performances_df_test = prequential_grid_search(
        transactions_df, classifier,
        input_features, output_feature,
        parameters, scoring,
        start_date_training=start_date_training_for_test,
        n_folds=n_folds, expe_type='Test',
        delta_train=delta_train, delta_delay=delta_delay, delta_assessment=delta_assessment,
        performance_metrics_list_grid=performance_metrics_list_grid,
        performance_metrics_list=performance_metrics_list, n_jobs=n_jobs
    )

    # Merge results
    if performances_df_test.empty and performances_df_validation.empty:
        print("Warning (model_selection_wrapper): Both Test and Validation results are empty.")
        return pd.DataFrame()
    elif performances_df_test.empty:
         print("Warning (model_selection_wrapper): Test results are empty. Returning only Validation.")
         return performances_df_validation
    elif performances_df_validation.empty:
         print("Warning (model_selection_wrapper): Validation results are empty. Returning only Test.")
         return performances_df_test
    else:
        # Merge, ensuring 'Parameters summary' exists
        if 'Parameters summary' not in performances_df_test.columns or 'Parameters summary' not in performances_df_validation.columns:
             print("ERROR (model_selection_wrapper): 'Parameters summary' missing. Cannot merge. Returning Test results.")
             return performances_df_test

        # Drop redundant columns from validation before merge
        val_cols_to_drop = ['Parameters', 'Execution time']
        validation_subset = performances_df_validation.drop(columns=val_cols_to_drop, errors='ignore')
        # Outer merge preserves all parameter sets tested
        performances_df_merged = pd.merge(performances_df_test, validation_subset, on='Parameters summary', how='outer')
        return performances_df_merged


def get_summary_performances(performances_df, parameter_column_name="Parameters summary"):
    """Summarizes performance dataframe to find best parameters based on validation metrics."""
    metrics_list = ['AUC ROC', 'Average precision', 'Card Precision@100'] # Assuming CP@100 was the target
    performances_results = pd.DataFrame(columns=metrics_list)

    if performances_df.empty:
        print("Warning: Empty performance dataframe passed to get_summary_performances.")
        # Return structure with N/A
        na_vals = ['N/A'] * len(metrics_list)
        for row_name in ["Best estimated parameters", "Validation performance", "Test performance", "Optimal parameter(s)", "Optimal test performance"]:
             performances_results.loc[row_name] = na_vals
        return performances_results

    # Ensure index is reset for iloc usage
    performances_df = performances_df.reset_index(drop=True)

    best_estimated_parameters = []
    validation_performance = []
    test_performance = []

    for metric in metrics_list:
        val_metric_col = metric + ' Validation'
        val_std_col = val_metric_col + ' Std'
        test_metric_col = metric + ' Test'
        test_std_col = test_metric_col + ' Std'

        # Check if required columns exist
        if val_metric_col not in performances_df.columns or test_metric_col not in performances_df.columns:
             print(f"Warning: Missing columns for metric {metric}. Adding N/A.")
             best_estimated_parameters.append('N/A')
             validation_performance.append('N/A')
             test_performance.append('N/A')
             continue

        # Find best performance based on validation score (handle NaNs)
        valid_scores = pd.to_numeric(performances_df[val_metric_col], errors='coerce')
        if valid_scores.isna().all():
            print(f"Warning: All validation scores for {metric} are NaN.")
            index_best_validation_performance = 0 # Default to first row
            best_param_summary = performances_df.loc[index_best_validation_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            val_perf_str = 'NaN'
            # Get test perf at this index
            test_perf_val = pd.to_numeric(performances_df[test_metric_col], errors='coerce').iloc[index_best_validation_performance]
            test_std_val = pd.to_numeric(performances_df.get(test_std_col, np.nan), errors='coerce').iloc[index_best_validation_performance]
            test_perf_str = f"{test_perf_val:.3f} +/- {test_std_val:.2f}" if not pd.isna(test_perf_val) else 'NaN'
        else:
            index_best_validation_performance = valid_scores.idxmax()
            best_param_summary = performances_df.loc[index_best_validation_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            # Get validation performance string
            val_perf = valid_scores.iloc[index_best_validation_performance]
            val_std = pd.to_numeric(performances_df.get(val_std_col, np.nan), errors='coerce').iloc[index_best_validation_performance]
            val_perf_str = f"{val_perf:.3f} +/- {val_std:.2f}" if not pd.isna(val_perf) else 'NaN'
            # Get test performance string at the same index
            test_perf = pd.to_numeric(performances_df[test_metric_col], errors='coerce').iloc[index_best_validation_performance]
            test_std = pd.to_numeric(performances_df.get(test_std_col, np.nan), errors='coerce').iloc[index_best_validation_performance]
            test_perf_str = f"{test_perf:.3f} +/- {test_std:.2f}" if not pd.isna(test_perf) else 'NaN'

        best_estimated_parameters.append(best_param_summary)
        validation_performance.append(val_perf_str)
        test_performance.append(test_perf_str)

    performances_results.loc["Best estimated parameters"] = best_estimated_parameters
    performances_results.loc["Validation performance"] = validation_performance
    performances_results.loc["Test performance"] = test_performance

    # Find Optimal on Test Set (for reference)
    optimal_parameters = []
    optimal_test_performance = []
    for metric_base in metrics_list:
        test_metric_col = metric_base + ' Test'
        test_std_col = test_metric_col + ' Std'

        if test_metric_col not in performances_df.columns:
            optimal_parameters.append('N/A')
            optimal_test_performance.append('N/A')
            continue

        test_scores = pd.to_numeric(performances_df[test_metric_col], errors='coerce')
        if test_scores.isna().all():
            print(f"Warning: All test scores for {metric_base} are NaN.")
            index_optimal_test_performance = 0 # Default index
            opt_param_summary = performances_df.loc[index_optimal_test_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            opt_test_perf_str = 'NaN'
        else:
            index_optimal_test_performance = test_scores.idxmax()
            opt_param_summary = performances_df.loc[index_optimal_test_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            opt_test_perf = test_scores.iloc[index_optimal_test_performance]
            opt_test_std = pd.to_numeric(performances_df.get(test_std_col, np.nan), errors='coerce').iloc[index_optimal_test_performance]
            opt_test_perf_str = f"{opt_test_perf:.3f} +/- {opt_test_std:.2f}" if not pd.isna(opt_test_perf) else 'NaN'

        optimal_parameters.append(opt_param_summary)
        optimal_test_performance.append(opt_test_perf_str)

    performances_results.loc["Optimal parameter(s)"] = optimal_parameters
    performances_results.loc["Optimal test performance"] = optimal_test_performance

    return performances_results

================
File: infrastructure/modules/aml-workspace/main.tf
================
resource "azurerm_machine_learning_workspace" "mlw" {
  name                    = "mlw-${var.prefix}-${var.postfix}${var.env}"
  location                = var.location
  resource_group_name     = var.rg_name
  application_insights_id = var.application_insights_id
  key_vault_id            = var.key_vault_id
  storage_account_id      = var.storage_account_id
  container_registry_id   = var.container_registry_id

  sku_name = "Basic"

  identity {
    type = "SystemAssigned"
  }

  tags = var.tags
}

# Compute cluster

resource "azurerm_machine_learning_compute_cluster" "adl_aml_ws_compute_cluster" {
  name                          = "cpu-cluster"
  location                      = var.location
  vm_priority                   = "LowPriority"
  vm_size                       = "Standard_DS3_v2"
  machine_learning_workspace_id = azurerm_machine_learning_workspace.mlw.id
  count                         = var.enable_aml_computecluster ? 1 : 0

  scale_settings {
    min_node_count                       = 0
    max_node_count                       = 4
    scale_down_nodes_after_idle_duration = "PT120S" # 120 seconds
  }
}

# # Datastore

# resource "azurerm_resource_group_template_deployment" "arm_aml_create_datastore" {
#   name                = "arm_aml_create_datastore"
#   resource_group_name = var.rg_name
#   deployment_mode     = "Incremental"
#   parameters_content = jsonencode({
#     "WorkspaceName" = {
#       value = azurerm_machine_learning_workspace.mlw.name
#     },
#     "StorageAccountName" = {
#       value = var.storage_account_name
#     }
#   })

#   depends_on = [time_sleep.wait_30_seconds]

#   template_content = <<TEMPLATE
# {
#   "$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
#   "contentVersion": "1.0.0.0",
#   "parameters": {
#         "WorkspaceName": {
#             "type": "String"
#         },
#         "StorageAccountName": {
#             "type": "String"
#         }
#     },
#   "resources": [
#         {
#             "type": "Microsoft.MachineLearningServices/workspaces/datastores",
#             "apiVersion": "2021-03-01-preview",
#             "name": "[concat(parameters('WorkspaceName'), '/default')]",
#             "dependsOn": [],
#             "properties": {
#                 "contents": {
#                     "accountName": "[parameters('StorageAccountName')]",
#                     "containerName": "default",
#                     "contentsType": "AzureBlob",
#                     "credentials": {
#                       "credentialsType": "None"
#                     },
#                     "endpoint": "core.windows.net",
#                     "protocol": "https"
#                   },
#                   "description": "Default datastore for mlops-tabular",
#                   "isDefault": false,
#                   "properties": {
#                     "ServiceDataAccessAuthIdentity": "None"
#                   },
#                   "tags": {}
#                 }
#         }
#   ]
# }
# TEMPLATE
# }

# resource "time_sleep" "wait_30_seconds" {

#   depends_on = [
#     azurerm_machine_learning_workspace.mlw
#   ]

#   create_duration = "30s"
# }

================
File: infrastructure/modules/aml-workspace/outputs.tf
================
output "name" {
  value = azurerm_machine_learning_workspace.mlw.name
}

================
File: infrastructure/modules/aml-workspace/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

variable "storage_account_id" {
  type        = string
  description = "The ID of the Storage Account linked to AML workspace"
}

variable "key_vault_id" {
  type        = string
  description = "The ID of the Key Vault linked to AML workspace"
}

variable "application_insights_id" {
  type        = string
  description = "The ID of the Application Insights linked to AML workspace"
}

variable "container_registry_id" {
  type        = string
  description = "The ID of the Container Registry linked to AML workspace"
}

variable "enable_aml_computecluster" {
  description = "Variable to enable or disable AML compute cluster"
  default     = false
}

variable "storage_account_name" {
  type        = string
  description = "The Name of the Storage Account linked to AML workspace"
}

================
File: infrastructure/modules/application-insights/main.tf
================
resource "azurerm_application_insights" "appi" {
  name                = "appi-${var.prefix}-${var.postfix}${var.env}"
  location            = var.location
  resource_group_name = var.rg_name
  application_type    = "web"

  tags = var.tags
}

================
File: infrastructure/modules/application-insights/outputs.tf
================
output "id" {
  value = azurerm_application_insights.appi.id
}

================
File: infrastructure/modules/application-insights/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/container-registry/main.tf
================
locals {
  safe_prefix  = replace(var.prefix, "-", "")
  safe_postfix = replace(var.postfix, "-", "")
}

resource "azurerm_container_registry" "cr" {
  name                = "cr${local.safe_prefix}${local.safe_postfix}${var.env}"
  resource_group_name = var.rg_name
  location            = var.location
  sku                 = "Standard"
  admin_enabled       = true

  tags = var.tags
}

================
File: infrastructure/modules/container-registry/outputs.tf
================
output "id" {
  value = azurerm_container_registry.cr.id
}

================
File: infrastructure/modules/container-registry/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/data-explorer/main.tf
================
data "azurerm_client_config" "current" {}

resource "azurerm_kusto_cluster" "cluster" {
  name                = "adx${var.prefix}${var.postfix}${var.env}"
  location            = var.location
  resource_group_name = var.rg_name
  streaming_ingestion_enabled = true
  language_extensions = ["PYTHON"]
  count               = var.enable_monitoring ? 1 : 0

  sku {
    name     = "Standard_D11_v2"
    capacity = 2
  }
  tags = var.tags
}

resource "azurerm_kusto_database" "database" {
  name                = "mlmonitoring"
  resource_group_name = var.rg_name
  location            = var.location
  cluster_name        = azurerm_kusto_cluster.cluster[0].name
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "SP_ID" {
  name         = "kvmonitoringspid"
  value        = data.azurerm_client_config.current.client_id
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "SP_KEY" {
  name         = "kvmonitoringspkey"
  value        = var.client_secret
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "SP_TENANT_ID" {
  name         = "kvmonitoringadxtenantid"
  value        = data.azurerm_client_config.current.tenant_id
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "ADX_URI" {
  name         = "kvmonitoringadxuri"
  value        = azurerm_kusto_cluster.cluster[0].uri
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "ADX_DB" {
  name         = "kvmonitoringadxdb"
  value        = azurerm_kusto_database.database[0].name
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

================
File: infrastructure/modules/data-explorer/outputs.tf
================


================
File: infrastructure/modules/data-explorer/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

variable "key_vault_id" {
  type        = string
  description = "The ID of the Key Vault linked to AML workspace"
}

variable "enable_monitoring" {
  description = "Variable to enable or disable AML compute cluster"
  default     = false
}

variable "client_secret" {
  description = "client secret"
  default     = false
}

================
File: infrastructure/modules/key-vault/main.tf
================
data "azurerm_client_config" "current" {}

resource "azurerm_key_vault" "kv" {
  name                = "kv-${var.prefix}-${var.postfix}${var.env}"
  location            = var.location
  resource_group_name = var.rg_name
  tenant_id           = data.azurerm_client_config.current.tenant_id
  sku_name            = "standard"

  tags = var.tags
  access_policy {
    tenant_id = data.azurerm_client_config.current.tenant_id
    object_id = data.azurerm_client_config.current.object_id

    key_permissions = [
      "Create",
      "Get",
    ]

    secret_permissions = [
      "Set",
      "Get",
      "Delete",
      "Purge",
      "Recover"
    ]
  }
}

================
File: infrastructure/modules/key-vault/outputs.tf
================
output "id" {
  value = azurerm_key_vault.kv.id
}

================
File: infrastructure/modules/key-vault/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/resource-group/main.tf
================
resource "azurerm_resource_group" "adl_rg" {
  name     = "rg-${var.prefix}-${var.postfix}${var.env}"
  location = var.location
  tags     = var.tags
}

================
File: infrastructure/modules/resource-group/outputs.tf
================
output "name" {
  value = azurerm_resource_group.adl_rg.name
}

output "location" {
  value = azurerm_resource_group.adl_rg.location
}

================
File: infrastructure/modules/resource-group/variables.tf
================
variable "location" {
  type        = string
  default     = "North Europe"
  description = "Location of the Resource Group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the Resource Group"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/storage-account/main.tf
================
data "azurerm_client_config" "current" {}

data "http" "ip" {
  url = "https://ifconfig.me"
}

locals {
  safe_prefix  = replace(var.prefix, "-", "")
  safe_postfix = replace(var.postfix, "-", "")
}

resource "azurerm_storage_account" "st" {
  name                     = "st${local.safe_prefix}${local.safe_postfix}${var.env}"
  resource_group_name      = var.rg_name
  location                 = var.location
  account_tier             = "Standard"
  account_replication_type = "LRS"
  account_kind             = "StorageV2"
  is_hns_enabled           = var.hns_enabled

  tags = var.tags
  
}

# Virtual Network & Firewall configuration

resource "azurerm_storage_account_network_rules" "firewall_rules" {
  storage_account_id = azurerm_storage_account.st.id

  default_action             = "Allow"
  ip_rules                   = [] # [data.http.ip.body]
  virtual_network_subnet_ids = var.firewall_virtual_network_subnet_ids
  bypass                     = var.firewall_bypass
}

================
File: infrastructure/modules/storage-account/outputs.tf
================
output "id" {
  value = azurerm_storage_account.st.id
}

output "name" {
  value = azurerm_storage_account.st.name
}

================
File: infrastructure/modules/storage-account/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the Resource Group"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

variable "hns_enabled" {
  type        = bool
  description = "Hierarchical namespaces enabled/disabled"
  default     = true
}

variable "firewall_virtual_network_subnet_ids" {
  default = []
}

variable "firewall_bypass" {
  default = ["None"]
}

================
File: infrastructure/aml_deploy.tf
================
# Resource group

module "resource_group" {
  source = "./modules/resource-group"

  location = var.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

# Azure Machine Learning workspace

module "aml_workspace" {
  source = "./modules/aml-workspace"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  storage_account_id      = module.storage_account_aml.id
  key_vault_id            = module.key_vault.id
  application_insights_id = module.application_insights.id
  container_registry_id   = module.container_registry.id

  enable_aml_computecluster = var.enable_aml_computecluster
  storage_account_name      = module.storage_account_aml.name

  tags = local.tags
}

# Storage account

module "storage_account_aml" {
  source = "./modules/storage-account"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  hns_enabled                         = false
  firewall_bypass                     = ["AzureServices"]
  firewall_virtual_network_subnet_ids = []

  tags = local.tags
}

# Key vault

module "key_vault" {
  source = "./modules/key-vault"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

# Application insights

module "application_insights" {
  source = "./modules/application-insights"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

# Container registry

module "container_registry" {
  source = "./modules/container-registry"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

module "data_explorer" {
  source = "./modules/data-explorer"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment
  key_vault_id      = module.key_vault.id
  enable_monitoring = var.enable_monitoring

  client_secret = var.client_secret

  tags = local.tags
}

================
File: infrastructure/locals.tf
================
locals {
  tags = {
    Owner       = "mlops-v2"
    Project     = "mlops-v2"
    Environment = "${var.environment}"
    Toolkit     = "terraform"
    Name        = "${var.prefix}"
  }
}

================
File: infrastructure/main.tf
================
terraform {
  backend "azurerm" {} 
  required_providers {
    azurerm = {
      version = "= 2.99.0"
    }
  }
}

provider "azurerm" {
  features {}
}

data "azurerm_client_config" "current" {}

data "http" "ip" {
  url = "https://ifconfig.me"
}

================
File: infrastructure/variables.tf
================
variable "location" {
  type        = string
  description = "Location of the resource group and modules"
}

variable "prefix" {
  type        = string
  description = "Prefix for module names"
}

variable "environment" {
  type        = string
  description = "Environment information"
}

variable "postfix" {
  type        = string
  description = "Postfix for module names"
}

variable "enable_aml_computecluster" {
  description = "Variable to enable or disable AML compute cluster"
}

variable "enable_monitoring" {
  description = "Variable to enable or disable Monitoring"
}

variable "client_secret" {
  description = "Service Principal Secret"
}

================
File: mlops/azureml/deploy/batch/batch-deployment.yml
================
# Azure ML Batch Deployment Configuration
# Defines how the model is deployed to the batch endpoint

$schema: https://azuremlschemas.azureedge.net/latest/batchDeployment.schema.json
name: default-batch-deploy # Name of this specific deployment within the endpoint
endpoint_name: fraud-detection-batch # Must match the name in batch-endpoint.yml
description: Default batch deployment for the fraud detection model.

model: azureml:fraud-detection-model@latest # Reference the registered model (update name if changed)

# Compute configuration for the batch scoring job
compute: azureml:batch-cluster # Name of the compute cluster to use (created by infra or workflow)
resources:
  instance_count: 1 # Number of compute nodes to use for the job

# Job settings
max_concurrency_per_instance: 2 # Max parallel mini-batch runs per node
mini_batch_size: 1024 # Number of files processed per run (adjust based on data size/memory)
output_action: append_row # How to write results (append_row or summary_only)
output_file_name: predictions.csv # Name of the output file(s) in the job output folder
retry_settings:
  max_retries: 3    # Max retries per mini-batch
  timeout: 300      # Timeout in seconds per mini-batch
error_threshold: 10 # Abort job if more than 10 mini-batches fail (-1 means never abort)
logging_level: info # info or warning

# Optional: Environment for the scoring script (if different from training)
# environment:
#   name: fraud-scoring-env
#   version: 1
#   image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04
#   conda_file: ../../../data-science/environment/score-conda.yml # Define if needed

# Optional: Code configuration if using a custom scoring script
# code_configuration:
#   code: ../../../data-science/src # Path to scoring script folder
#   scoring_script: batch_score.py # Name of the scoring script

================
File: mlops/azureml/deploy/batch/batch-endpoint.yml
================
# Azure ML Batch Endpoint Configuration
# Defines the endpoint itself, which can host multiple deployments

$schema: https://azuremlschemas.azureedge.net/latest/batchEndpoint.schema.json
name: fraud-detection-batch # Unique name for the batch endpoint
description: Batch endpoint for scoring fraud detection transactions.
auth_mode: aad_token # Use Azure Active Directory token for authentication (key auth also possible)

# Optional tags
tags:
  project: FraudDetection
  environment: production # Or relevant environment

================
File: mlops/azureml/deploy/online/online-deployment.yml
================
# File: mlops/azureml/deploy/online/online-deployment.yml

$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue # Or your deployment name
endpoint_name: fraud-detection-online # Must match the name in online-endpoint.yml
description: Blue deployment for the real-time fraud detection model.
model: azureml:fraud-detection-model@latest # Reference the registered model
instance_type: Standard_E4s_v3 # Or your desired instance type
instance_count: 1

# --- Add/Modify Code and Environment ---
# Ensure you specify the code containing score.py and the environment with dependencies
code_configuration:
  code: . # Corrected path: The directory containing this YAML and score.py
  scoring_script: score.py
environment: azureml:fraud-detection-train-env@latest # Use the env with azureml-ai-monitoring

# --- Add Data Collector Configuration ---
data_collector:
  # rolling_rate removed - Default partitioning based on ingestion time will be used.
  # The default storage path structure still provides YYYY/MM/DD/HH granularity.
  collections:
    # Enable the collector named 'model_inputs' in score.py
    model_inputs:
      enabled: 'True'
      # Optional: Define specific data asset details if you don't want the default
      # data:
      #   name: my_model_inputs_asset_name # Custom asset name
      #   path: azureml://datastores/workspaceblobstore/paths/my_custom_path/inputs # Custom path
      #   version: 1
    # Enable the collector named 'model_outputs' in score.py
    model_outputs:
      enabled: 'True'
      # Optional: Define specific data asset details
      # data:
      #   name: my_model_outputs_asset_name
      #   path: azureml://datastores/workspaceblobstore/paths/my_custom_path/outputs
      #   version: 1

# Optional: request_settings, probes etc. can remain as they were
# request_settings:
#   request_timeout_ms: 5000
# liveness_probe:
#   failure_threshold: 30
#   period_seconds: 10
# readiness_probe:
#   failure_threshold: 3
#   period_seconds: 10

================
File: mlops/azureml/deploy/online/online-endpoint.yml
================
# Azure ML Managed Online Endpoint Configuration
# Defines the endpoint URL and authentication

$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: fraud-detection-online # Unique name for the online endpoint
description: Real-time endpoint for predicting transaction fraud likelihood.
auth_mode: key # Use key-based authentication (alternatively: 'aad_token')

# Optional tags
tags:
  project: FraudDetection
  environment: production # Or relevant environment

================
File: mlops/azureml/deploy/online/score.py
================
# File: mlops/azureml/deploy/online/score.py
import os
import logging
import json
import pandas as pd
import mlflow
from azureml.ai.monitoring import Collector
from azureml.ai.monitoring.context import BasicCorrelationContext

# Define the expected input features for your model (adjust as needed!)
INPUT_FEATURES = [
    'TX_AMOUNT', 'TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',
    'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',
    'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',
    'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',
    'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',
    'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',
    'TERMINAL_ID_RISK_30DAY_WINDOW'
]

# Global variables for the model and collectors
model = None
inputs_collector = None
outputs_collector = None

def init():
    """
    This function is called when the container is initialized/started, typically after create/update of the deployment.
    You can write the logic here to perform init operations like caching the model in memory
    """
    global model, inputs_collector, outputs_collector

    # Define collectors with names. Using 'model_inputs' and 'model_outputs'
    # helps Azure ML Model Monitoring auto-detect them.
    # Adding error handling for robustness during collection
    inputs_collector = Collector(name='model_inputs', on_error=lambda e: logging.error(f"Inputs Collector Error: {e}"))
    outputs_collector = Collector(name='model_outputs', on_error=lambda e: logging.error(f"Outputs Collector Error: {e}"))

    # Get the path to the deployed model folder
    # AZUREML_MODEL_DIR is an environment variable created during deployment. It is the path to the model folder
    # (./azureml-models/$MODEL_NAME/$VERSION)
    model_path = os.path.join(
        os.getenv("AZUREML_MODEL_DIR", default="."), "fraud-detection-model" # Assuming model artifact folder name is 'fraud-detection-model'
    )

    # Load the MLflow model
    try:
        model = mlflow.sklearn.load_model(model_path)
        logging.info("Model loaded successfully")
    except Exception as e:
        logging.error(f"Failed to load model from path: {model_path}. Error: {e}")
        raise

    logging.info("Init complete")

def run(raw_data):
    """
    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.
    """
    global model, inputs_collector, outputs_collector
    logging.info("Request received")

    try:
        # Expecting a JSON string with a single transaction's data as a dictionary
        data_dict = json.loads(raw_data)
        logging.info(f"Input data (dict): {data_dict}")

        # Extract the custom ID for correlation
        transaction_id = data_dict.get('TRANSACTION_ID', None)
        if transaction_id is None:
             logging.warning("TRANSACTION_ID not found in payload. Correlation ID will be auto-generated by collector.")
             # Fallback or raise error depending on requirements
             # For this example, we'll let the collector auto-generate if missing
             custom_context = None
        else:
            # Create the correlation context with the custom ID
             custom_context = BasicCorrelationContext(id=str(transaction_id)) # Ensure it's a string
             logging.info(f"Using custom correlation ID: {transaction_id}")

        # Convert the single dictionary to a DataFrame for the collector and model
        # We wrap the dictionary in a list before creating the DataFrame
        input_df = pd.DataFrame([data_dict])

        # --- Collect Input Data ---
        # The context returned contains info to correlate inputs and outputs
        # Pass the custom context object here
        try:
            context = inputs_collector.collect(input_df, custom_context) # Pass the custom context if available
            logging.info("Input data collected.")
        except Exception as e:
            logging.error(f"Error during input data collection: {e}")
            # Decide if you want to proceed without collection or raise error
            context = None # Ensure context is None if collection fails

        # --- Perform Prediction ---
        # Ensure only the required features are passed to the model
        # Make a copy to avoid SettingWithCopyWarning if input_df is used later
        model_input_df = input_df[INPUT_FEATURES].copy()

        # Check for required columns (optional but good practice)
        missing_cols = [col for col in INPUT_FEATURES if col not in model_input_df.columns]
        if missing_cols:
            raise ValueError(f"Missing required input columns for model: {missing_cols}")

        # Check for NaNs before prediction (optional, depends on how model/pipeline handles them)
        if model_input_df.isnull().values.any():
            logging.warning("NaNs detected in input data before prediction. Ensure model pipeline handles them.")
            # Add imputation logic here if necessary and not handled by the model's pipeline

        prediction = model.predict(model_input_df)
        probabilities = model.predict_proba(model_input_df)
        logging.info("Prediction generated")

        # --- Prepare output DataFrame for collection with 'TX_FRAUD' column ---
        output_df = pd.DataFrame({
            'TX_FRAUD': prediction, # Changed column name here
            'probability_non_fraud': probabilities[:, 0], # Renamed for clarity
            'probability_fraud': probabilities[:, 1] # Assuming class 1 is fraud
        })

        # --- Collect Output Data ---
        if context: # Only collect output if input collection was successful and returned context
            try:
                outputs_collector.collect(output_df, context) # Pass the context from input collection
                logging.info("Output data collected.")
            except Exception as e:
                logging.error(f"Error during output data collection: {e}")
                # Decide if you want to proceed without collection or raise error
        else:
             logging.warning("Skipping output data collection because input collection failed or context is missing.")


        # --- Format Response ---
        # Return prediction and probabilities (adjust format as needed by consuming application)
        # Keep the response format potentially different from the collected data format
        result = {
            "TX_FRAUD": int(prediction[0]), # Using TX_FRAUD in the response as well for consistency
            "probability_fraud": float(probabilities[0, 1]) # Probability of class 1 (fraud)
        }
        logging.info(f"Sending response: {result}")
        return result

    except json.JSONDecodeError as e:
        logging.error(f"Bad JSON format received: {e}")
        return {"error": "Invalid JSON input", "details": str(e)}, 400 # Example error response
    except KeyError as e:
         logging.error(f"Missing expected key in input data: {e}")
         return {"error": f"Missing key: {e}"}, 400
    except ValueError as e:
         logging.error(f"Data validation or processing error: {e}")
         return {"error": f"Data error: {e}"}, 400
    except Exception as e:
        logging.error(f"An error occurred during scoring: {e}")
        import traceback
        traceback.print_exc()
        return {"error": "Prediction failed", "details": str(e)}, 500

================
File: mlops/azureml/train/data.yml
================
# Azure ML Data Asset Definition (v2 YAML)
# Defines the RAW data used as input to the preparation step of the pipeline.

$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

type: uri_folder # Use uri_folder for a directory of files
name: raw-fraud-data # Name for this data asset in Azure ML
description: Raw daily transaction data files (.pkl format) for fraud detection.

# Path to the data within your Azure ML datastore.
# Assumes the raw .pkl files are in a folder named 'raw-fraud-data'
# in the default workspace datastore ('workspaceblobstore').
# Adjust 'path' if your data is elsewhere (e.g., different folder, different datastore).
path: azureml://datastores/workspaceblobstore/paths/fraud-data-raw/

# Optional: Specify version (if omitted, defaults to autoincrementing version)
# version: 1

# Optional: Add tags
# tags:
#   stage: raw
#   project: FraudDetection

================
File: mlops/azureml/train/environment.yml
================
# Azure ML Environment Definition (v2 YAML)
# Defines the runtime environment for pipeline steps.

$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: fraud-detection-train-env # Name for the environment asset in Azure ML
version: 1 # Version for the environment asset
description: Conda environment for training the fraud detection model, based on user notebooks.

# Using a standard Azure ML curated image as a base
image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04 # Choose a suitable base image

# Reference the Conda environment file
conda_file: ../../../data-science/environment/train-conda.yml

# Optional: Add OS packages if needed
# os_packages:
#   - package1
#   # ...

# Optional: Add environment variables
# environment_variables:
#   MY_ENV_VAR: "value"

# Optional: Add tags
# tags:
#   project: FraudDetection

================
File: mlops/azureml/train/pipeline.yml
================
# Azure ML Pipeline Definition (v2 YAML)
# Defines the sequence of steps for training the fraud detection model.


$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline

display_name: Fraud_Detection_Training_Pipeline_MLTable
description: Pipeline to preprocess data (MLTable), train (XGBoost GridSearch), evaluate, and register a fraud detection model.
experiment_name: fraud-detection-training-mltable 

# --- Pipeline Inputs ---
inputs:
  prep_step_input_data:
    type: mltable 
    path: azureml:daily_raw_transactions_fraud@latest 
    mode: ro_mount 

  # Date range for *filtering* the loaded MLTable in prep step
  prep_output_start_date: "2025-06-11"
  prep_output_end_date: "2025-08-14"
  baseline_date_str: "2025-04-01" # Keep if prep.py uses it for time calculations

  # Anchor date and deltas for training splits (used *inside* train.py)
  anchor_date_str: "2025-07-25"
  delta_train: 7
  delta_delay: 7
  delta_assessment: 7
  n_folds: 4

  # Evaluation threshold
  deploy_threshold_value: 0.65

# # --- Pipeline Outputs ---
# outputs:
#   prepared_data_asset_output:
#     type: mltable # Output type is mltable
#     mode: upload # Needs to be uploaded to be registered

outputs:
  # REVISED: Output of prep is now a folder containing MLTable definition + data
  prepared_data_folder_output: # Renamed for clarity
    type: uri_folder # Changed type to uri_folder
    mode: upload

  # Keep existing outputs
  trained_model_output:
    type: mlflow_model
    mode: upload
  evaluation_report_output:
    type: uri_folder
    mode: upload
  model_info_output:
     type: uri_folder
     mode: upload

# --- Pipeline Settings ---
settings:
  default_datastore: azureml:workspaceblobstore
  default_compute: azureml:cpu-cluster
  continue_on_step_failure: false

# --- Pipeline Jobs (Steps) ---
jobs:
  # 1. Preparation Step (UPDATED)
  prep_data:
    type: command
    name: prep_tabular_data
    display_name: Prepare Tabular Data (MLTable)
    description: Loads MLTable, applies transformations, outputs registered MLTable.
    inputs:
      # UPDATED: Use the new pipeline input
      input_tabular_data: ${{parent.inputs.prep_step_input_data}}
      # Keep date/baseline inputs if prep.py still uses them for filtering/calcs
      output_start_date: ${{parent.inputs.prep_output_start_date}}
      output_end_date: ${{parent.inputs.prep_output_end_date}}
      baseline_date_str: ${{parent.inputs.baseline_date_str}}
    outputs:
      # UPDATED: Output is now the prepared MLTable, map to pipeline output
      # prepared_data_output:
      #   type: mltable # Explicitly declare type
      #   mode: upload # Ensure it's uploaded
      #   # Define name and version here to register the output directly
      #   name: daily_transactions_fraud_transformed # Name of the data asset to register
      #   # Link to the pipeline output definition
      #   path: ${{parent.outputs.prepared_data_asset_output}}
      prepared_data_folder: # Renamed variable for clarity
        type: uri_folder # Changed type
        mode: upload
        path: ${{parent.outputs.prepared_data_folder_output}} # Map to pipeline output
    code: ../../../data-science/src
    command: >-
      python prep.py
      --input_tabular_data ${{inputs.input_tabular_data}} 
      --output_mltable_path ${{outputs.prepared_data_folder}}
      --output_start_date ${{inputs.output_start_date}}
      --output_end_date ${{inputs.output_end_date}}
      --baseline_date_str ${{inputs.baseline_date_str}}
    environment: azureml:fraud-detection-train-env@latest

  # 2. Training Step (UPDATED Input)
  train_model:
    type: command
    name: train_fraud_model_mltable
    display_name: Train Model (from MLTable)
    description: Trains model using the prepared MLTable data asset.
    inputs:
      # UPDATED: Input is now the output MLTable from the prep step
      # prepared_data: ${{parent.jobs.prep_data.outputs.prepared_data_output}} # Use output from prep_data
      prepared_data_folder: ${{parent.jobs.prep_data.outputs.prepared_data_folder}}
      # Keep anchor date and deltas for splitting logic within train.py
      anchor_date_str: ${{parent.inputs.anchor_date_str}}
      delta_train: ${{parent.inputs.delta_train}}
      delta_delay: ${{parent.inputs.delta_delay}}
      delta_assessment: ${{parent.inputs.delta_assessment}}
      n_folds: ${{parent.inputs.n_folds}}
      n_jobs: -1
      # REMOVED: train_load_start/end_date not needed as input is single MLTable
    outputs:
      model_output: ${{parent.outputs.trained_model_output}}
      test_data_output: # Output folder containing final_test_data.pkl (evaluate needs this)
          type: uri_folder
          mode: upload
    code: ../../../data-science/src
    command: >-
      python train.py
      --input_mltable_data ${{inputs.prepared_data_folder}}
      --model_output ${{outputs.model_output}}
      --test_data_output ${{outputs.test_data_output}}
      --anchor_date_str ${{inputs.anchor_date_str}}
      --delta_train ${{inputs.delta_train}}
      --delta_delay ${{inputs.delta_delay}}
      --delta_assessment ${{inputs.delta_assessment}}
      --n_folds ${{inputs.n_folds}}
      --n_jobs ${{inputs.n_jobs}}
    environment: azureml:fraud-detection-train-env@latest

  # 3. Evaluation Step 
  evaluate_model:
    type: command
    name: evaluate_trained_model
    display_name: Evaluate Model Performance
    description: Evaluates the trained model on the test set and determines deploy flag.
    inputs:
      model_input: ${{parent.jobs.train_model.outputs.model_output}}
      test_data_folder: ${{parent.jobs.train_model.outputs.test_data_output}} # Still needs the pkl test split
      deploy_threshold_value: ${{parent.inputs.deploy_threshold_value}}
    outputs:
      evaluation_output: ${{parent.outputs.evaluation_report_output}}
    code: ../../../data-science/src
    command: >-
      python evaluate.py
      --model_input ${{inputs.model_input}}
      --test_data ${{inputs.test_data_folder}}
      --evaluation_output ${{outputs.evaluation_output}}
      --deploy_threshold_value ${{inputs.deploy_threshold_value}}
    environment: azureml:fraud-detection-train-env@latest

  # 4. Registration Step 
  register_model:
    type: command
    name: register_evaluated_model
    display_name: Register Model (if approved)
    description: Registers the model in the Azure ML workspace if deploy flag is set.
    inputs:
      model_path: ${{parent.jobs.train_model.outputs.model_output}}
      evaluation_output: ${{parent.jobs.evaluate_model.outputs.evaluation_output}}
      model_name: "fraud-detection-model" 
    outputs:
       model_info_output_path: ${{parent.outputs.model_info_output}}
    code: ../../../data-science/src
    command: >-
      python register.py
      --model_name ${{inputs.model_name}}
      --model_path ${{inputs.model_path}}
      --evaluation_output ${{inputs.evaluation_output}}
      --model_info_output_path ${{outputs.model_info_output_path}}
    environment: azureml:fraud-detection-train-env@latest

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb # Ignore notebooks themselves, keep only scripts

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# VS Code settings
.vscode/

# Terraform state and cache
.terraform/
*.tfstate
*.tfstate.*
terraform.tfvars
*.tfvars
*.tfplan
.terraform.lock.hcl
*.hcl.lock

# Azure ML specific
aml_config/
config.json # Store securely or generate dynamically
*.azureml
.azureml/

# Data files (add specific patterns if needed)
/data/ # Ignore raw data folder if added to repo locally
/simulated-data-raw/
/simulated-data-transformed/
*.pkl # Ignore pickle files if generated locally
*.csv # Ignore csv files if generated locally
*.parquet # Ignore parquet files if generated locally
rpout.txt
# Model files (if saved locally outside pipeline outputs)
*.joblib
*.onnx
outputs/ # Ignore typical output folders

# OS generated files
.DS_Store
Thumbs.db

# Log files
*.log

# Temporary files
*.tmp
*.swp
*~

================
File: .pre-commit-config.yaml
================
# See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0 # Use a recent version
    hooks:
    -   id: trailing-whitespace
    -   id: end-of-file-fixer
    -   id: check-yaml
    -   id: check-added-large-files
    -   id: check-toml
    -   id: check-merge-conflict
-   repo: https://github.com/psf/black
    rev: 24.4.0 # Use a recent version of Black
    hooks:
    -   id: black
        language_version: python3.9 # Specify your python version
-   repo: https://github.com/pycqa/flake8
    rev: 7.0.0 # Use a recent version of Flake8
    hooks:
    -   id: flake8
        # args: ['--max-line-length=88', '--extend-ignore=E203'] # Example: align with Black
-   repo: https://github.com/pycqa/isort
    rev: 5.13.2 # Use a recent version of isort
    hooks:
      - id: isort
        name: isort (python)
        args: ["--profile", "black"] # Make isort compatible with Black
# -   repo: https://github.com/antonbabenko/pre-commit-terraform
#     rev: v1.88.4 # Use a recent version
#     hooks:
#       - id: terraform_fmt
#       - id: terraform_validate
#       - id: terraform_docs # Optional: generate TF docs

================
File: CODE_OF_CONDUCT.md
================
# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

*   [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
*   [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
*   Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

================
File: config-infra-prod.yml
================
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

# Production Environment Infrastructure Configuration
# --- USER ACTION REQUIRED: Update values below ---
variables:

  # --- Global Settings ---
  # Define a short, lowercase alphanumeric namespace for your project (e.g., 'fraudproj')
  # Note: Max length constraints apply to derived names (e.g., storage accounts max 24 chars). Keep it short.
  namespace: fraudtrml # EXAMPLE: Replace with your project namespace
  # Define a short, unique postfix (e.g., initials, random chars like 'a0b1')
  postfix: fdx012 # EXAMPLE: Replace with your unique postfix
  # Azure region for deployment (e.g., 'westus2', 'eastus', 'westeurope')
  location: westeurope # EXAMPLE: Replace with your desired Azure region

  # Environment identifier (used in resource naming)
  environment: prod

  # --- Feature Flags ---
  # Enable creation of a default CPU compute cluster in AML Workspace?
  enable_aml_computecluster: true
  # Enable creation of Azure Data Explorer for monitoring (adds cost)?
  enable_monitoring: false
  # Enable secure workspace features (e.g., VNet integration - requires more setup)?
  enable_aml_secure_workspace: false # Set to true for VNet scenarios


  # Azure DevOps
  # ado_service_connection_rg: Azure-ARM-Prod
  # ado_service_connection_aml_ws: Azure-ARM-Prod

  # DO NOT TOUCH

  # For pipeline reference
  resource_group: rg-$(namespace)-$(postfix)$(environment)
  aml_workspace: mlw-$(namespace)-$(postfix)$(environment)
  application_insights: mlw-$(namespace)-$(postfix)$(environment)
  key_vault: kv-$(namespace)-$(postfix)$(environment)
  container_registry: cr$(namespace)$(postfix)$(environment)
  storage_account: st$(namespace)$(postfix)$(environment)

  # For terraform reference
  terraform_version: 0.14.7
  terraform_workingdir: infrastructure
  terraform_st_resource_group: rg-$(namespace)-$(postfix)$(environment)-tf
  terraform_st_location: $(location)
  terraform_st_storage_account: st$(namespace)$(postfix)$(environment)tf
  terraform_st_container_name: default
  # terraform_st_container_name: tfstate
  terraform_st_key: mlops-tab
  # terraform_st_key: fraud-mlops-$(environment).tfstate


  # Batch/Online Endpoint names (example derivation, adjust as needed)
  # Ensure these result in valid Azure resource names
  bep: fraud-batch-ep-$(postfix) # Batch Endpoint name part
  oep: fraud-online-ep-$(postfix) # Online Endpoint name part

================
File: LICENSE
================
MIT License

Copyright (c) Microsoft Corporation.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE

================
File: README.md
================
# Azure MLOps - Fraud Detection Example

This repository provides a structured example of implementing an MLOps pipeline for a fraud detection machine learning model using Azure Machine Learning and GitHub Actions. It adapts the concepts from the [Azure MLOps (v2) solution accelerator](https://github.com/Azure/mlops-v2) and applies them to a specific fraud detection scenario based on provided notebooks.

## Project Structure

*   **`.github/workflows`**: Contains GitHub Actions workflows for:
    *   `tf-gha-deploy-infra.yml`: Deploying Azure infrastructure (AML Workspace, Storage, etc.) using Terraform.
    *   `deploy-model-training-pipeline.yml`: Orchestrating the model training pipeline in Azure ML.
    *   `deploy-online-endpoint-pipeline.yml`: Deploying the trained model to a managed online endpoint.
    *   `deploy-batch-endpoint-pipeline.yml`: Deploying the trained model to a batch endpoint.
*   **`data-science`**: Holds the core data science code.
    *   `environment/train-conda.yml`: Conda environment definition for training.
    *   `src/`: Python scripts for individual pipeline steps (`prep.py`, `train.py`, `evaluate.py`, `register.py`) and shared utilities (`utils.py`).
*   **`infrastructure`**: Contains Terraform modules and configuration for deploying Azure resources.
*   **`mlops/azureml`**: Azure Machine Learning specific configurations.
    *   `train/`: Definitions for the training pipeline (`pipeline.yml`), data asset (`data.yml`), and environment (`environment.yml`).
    *   `deploy/`: Definitions for online and batch endpoints and deployments.
*   **`config-infra-prod.yml`**: Configuration file defining resource names, locations, and feature flags for the production environment infrastructure.

## Scenario Overview

The pipeline implements the following steps:

1.  **Data Preparation (`prep.py`)**:
    *   Reads raw daily transaction data (pickle files).
    *   Applies feature transformations:
        *   Date/Time features (weekend, night).
        *   Customer spending behavior (aggregates over time windows).
        *   Terminal risk scores (aggregates over time windows with delay).
    *   Saves the transformed data back into daily pickle files.
2.  **Model Training (`train.py`)**:
    *   Loads the transformed data.
    *   Performs model selection using prequential cross-validation **specifically for a Decision Tree classifier** (as requested). Hyperparameter tuning uses GridSearchCV.
    *   Trains the final Decision Tree model on the designated training split using the best found hyperparameters (or defaults).
    *   Logs the trained model artifact using MLflow.
    *   Saves the defined test data split for the evaluation step.
3.  **Model Evaluation (`evaluate.py`)**:
    *   Loads the trained model artifact.
    *   Loads the test data split.
    *   Calculates performance metrics (AUC ROC, Average Precision, Card Precision@k).
    *   Determines a deployment flag based on whether a primary metric (e.g., Average Precision) meets a predefined threshold.
    *   Logs metrics to MLflow.
4.  **Model Registration (`register.py`)**:
    *   Checks the deployment flag from the evaluation step.
    *   If the flag is set, registers the trained model artifact in the Azure ML Model Registry.
    *   Outputs model registration information (name and version).

## Prerequisites

1.  **Azure Subscription**: Access to an Azure subscription.
2.  **Azure ML Workspace**: An existing Azure Machine Learning workspace (or deploy one using the `tf-gha-deploy-infra.yml` workflow).
3.  **GitHub Repository**: A GitHub repository based on this template.
4.  **Service Principal**: An Azure Service Principal with `Contributor` rights on the subscription (or target resource group).
5.  **GitHub Secrets**: Configure the following secrets in your GitHub repository settings:
    *   `AZURE_CREDENTIALS`: The JSON output from creating the service principal (`az ad sp create-for-rbac --role Contributor --sdk-auth`).
    *   `ARM_CLIENT_ID`: The Client ID of the service principal.
    *   `ARM_CLIENT_SECRET`: The Client Secret of the service principal.
    *   `ARM_SUBSCRIPTION_ID`: Your Azure Subscription ID.
    *   `ARM_TENANT_ID`: Your Azure Tenant ID.
6.  **Terraform State Storage**: An Azure Storage Account and container created beforehand to store Terraform state files (configure names in `config-infra-prod.yml`).
7.  **Raw Data**: Upload your raw daily transaction pickle files to the location specified in `mlops/azureml/train/data.yml` (default: `raw-fraud-data` folder in the default workspace datastore).

## Getting Started

1.  **Fork/Clone**: Fork or clone this repository.
2.  **Configure**:
    *   Update `config-infra-prod.yml` with your desired `namespace`, `postfix`, `location`, and Terraform backend details.
    *   Review `mlops/azureml/train/data.yml` and ensure the `path` points to your raw data location in Azure ML.
    *   Set up the required GitHub Secrets.
    *   Ensure the Terraform state storage account/container exists.
3.  **Deploy Infrastructure (Optional)**: Run the `tf-gha-deploy-infra` workflow from the GitHub Actions tab to provision Azure resources.
4.  **Run Training Pipeline**: Run the `deploy-model-training-pipeline` workflow from the GitHub Actions tab. This will execute the prep, train, evaluate, and register steps in Azure ML.
5.  **Deploy Model (Optional)**:
    *   Run the `deploy-online-endpoint-pipeline` workflow to deploy the registered model to a real-time endpoint.
    *   Run the `deploy-batch-endpoint-pipeline` workflow to deploy the registered model to a batch scoring endpoint.

## Customization

*   **Data Source**: Modify `mlops/azureml/train/data.yml` if your raw data format or location differs.
*   **Feature Engineering**: Update `data-science/src/prep.py` and potentially `data-science/src/utils.py` to change feature transformations.
*   **Model Selection**: Modify `data-science/src/train.py` to include different classifiers or change the hyperparameter grids. Remember the current version is hardcoded for Decision Tree grid search only.
*   **Evaluation**: Adjust metrics or the deployment logic in `data-science/src/evaluate.py`.
*   **Environment**: Update `data-science/environment/train-conda.yml` to add or remove dependencies.
*   **Infrastructure**: Modify Terraform files in the `infrastructure` directory if different Azure resources are needed.

================
File: requirements.txt
================
# Requirements for local development or testing (if needed)
# The primary environment definition is in data-science/environment/train-conda.yml

# Linters and Formatters (for pre-commit)
black>=23.0,<25.0
flake8>=6.0,<8.0
isort>=5.10,<6.0
pre-commit>=3.0,<4.0

# Core ML libraries (versions should ideally match conda env for consistency)
# numpy>=1.19.5,<1.24.0
# pandas>=1.3.5,<1.6.0
# scikit-learn>=1.0.0,<1.2.0
# imbalanced-learn>=0.8.1,<0.11.0
# xgboost>=1.5.1,<1.8.0
# matplotlib
# seaborn>=0.11.2,<0.13.0
# joblib

# Azure ML & MLflow (Ensure compatibility)
# azure-ai-ml>=1.5.0 # V2 SDK
# azure-identity>=1.10.0
# or
# azureml-sdk>=1.50.0 # V1 SDK
# azureml-mlflow>=1.50.0 # V1 MLflow integration
# mlflow>=2.0,<2.10

# Other utilities
# GitPython>=3.1,<3.2 # For Git integration if needed locally

# Note: It's generally recommended to manage environments using Conda via the .yml file,
# especially when dealing with complex dependencies like those in scientific Python.
# This requirements.txt is mainly for tooling like pre-commit.

================
File: SECURITY.md
================
<!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc).

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

*   Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
*   Full paths of source file(s) related to the manifestation of the issue
*   The location of the affected source code (tag/branch/commit or direct URL)
*   Any special configuration required to reproduce the issue
*   Step-by-step instructions to reproduce the issue
*   Proof-of-concept or exploit code (if possible)
*   Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->

================
File: SUPPORT.md
================
# Support

## How to file issues and get help

This project uses GitHub Issues to track bugs and feature requests. Please search the existing
issues before filing new issues to avoid duplicates. For new issues, file your bug or
feature request as a new Issue.

For help and questions about using this project, please refer to the project's README.md file or file an issue in the GitHub repository. Community contributions and discussions via issues and pull requests are welcome.

## Microsoft Support Policy

Support for this project is limited to the resources listed above. This is a community-supported project and not officially supported by Microsoft Customer Service & Support (CSS).
