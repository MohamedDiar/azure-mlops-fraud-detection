This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-04-18T13:41:56.284Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.github/
  workflows/
    deploy-batch-endpoint-pipeline.yml
    deploy-model-training-pipeline.yml
    deploy-online-endpoint-pipeline.yml
    tf-gha-deploy-infra.yml
data-science/
  environment/
    train-conda.yml
  src/
    evaluate.py
    prep.py
    register.py
    train.py
    utils.py
infrastructure/
  modules/
    aml-workspace/
      main.tf
      outputs.tf
      variables.tf
    application-insights/
      main.tf
      outputs.tf
      variables.tf
    container-registry/
      main.tf
      outputs.tf
      variables.tf
    data-explorer/
      main.tf
      outputs.tf
      variables.tf
    key-vault/
      main.tf
      outputs.tf
      variables.tf
    resource-group/
      main.tf
      outputs.tf
      variables.tf
    storage-account/
      main.tf
      outputs.tf
      variables.tf
  aml_deploy.tf
  locals.tf
  main.tf
  variables.tf
mlops/
  azureml/
    deploy/
      batch/
        batch-deployment.yml
        batch-endpoint.yml
      online/
        online-deployment.yml
        online-endpoint.yml
    train/
      data.yml
      environment.yml
      pipeline.yml
.gitignore
.pre-commit-config.yaml
CODE_OF_CONDUCT.md
config-infra-prod.yml
LICENSE
README.md
requirements.txt
SECURITY.md
SUPPORT.md

================================================================
Repository Files
================================================================

================
File: .github/workflows/deploy-batch-endpoint-pipeline.yml
================
name: deploy-batch-endpoint-pipeline
on:
  workflow_dispatch:
jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@main
      with:
        file_name: config-infra-prod.yml
  create-compute:
      needs: get-config
      uses: Azure/mlops-templates/.github/workflows/create-compute.yml@main
      with:
        cluster_name: batch-cluster # Compute cluster name for batch endpoint
        size: STANDARD_DS3_V2 # Adjust VM size if needed
        min_instances: 0
        max_instances: 5
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
      secrets:
          creds: ${{secrets.AZURE_CREDENTIALS}}
  create-endpoint:
      needs: [get-config,create-compute]
      uses: Azure/mlops-templates/.github/workflows/create-endpoint.yml@main
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/batch/batch-endpoint.yml
        endpoint_name: ${{ needs.get-config.outputs.bep }} # Use name from config
        endpoint_type: batch
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}
  create-deployment:
      uses: Azure/mlops-templates/.github/workflows/create-deployment.yml@main
      needs: [get-config,create-endpoint]
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/batch/batch-deployment.yml
        endpoint_name: ${{ needs.get-config.outputs.bep }} # Use name from config
        endpoint_type: batch
        deployment_name: default-batch-deploy # Name your deployment
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

================
File: .github/workflows/deploy-model-training-pipeline.yml
================
name: deploy-model-training-pipeline
on:
  workflow_dispatch:

jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@v1.1.0
      with:
        file_name: config-infra-prod.yml

  register-environment:
      needs: get-config
      uses: Azure/mlops-templates/.github/workflows/register-environment.yml@v1.1.0
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        environment_file: mlops/azureml/train/environment.yml # Points to AML Env definition
        conda_file: data-science/environment/train-conda.yml # Points to conda file
      secrets:
          creds: ${{secrets.AZURE_CREDENTIALS}}

  # Optional: Register raw data if not already registered
  register-raw-data:
    needs: get-config
    uses: Azure/mlops-templates/.github/workflows/register-dataset.yml@v1.1.0
    with:
      resource_group: ${{ needs.get-config.outputs.resource_group }}
      workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
      name: raw-fraud-data # Name for the raw data asset
      data_file: mlops/azureml/train/data.yml # Points to data asset definition
    secrets:
      creds: ${{secrets.AZURE_CREDENTIALS}}

  create-compute:
      # Depends on environment registration (and optionally data registration)
      needs: [get-config, register-environment,register-raw-data] # Add register-raw-data if uncommented above
      uses: Azure/mlops-templates/.github/workflows/create-compute.yml@v1.1.0
      with:
        cluster_name: cpu-cluster # Default compute for pipeline steps
        size: Standard_E4ds_v5 # Adjust as needed
        min_instances: 0
        max_instances: 1
        cluster_tier: dedicated # Use low priority to save cost (optional)
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
      secrets:
          creds: ${{secrets.AZURE_CREDENTIALS}}

  run-pipeline:
      needs: [get-config, register-environment, create-compute,register-raw-data] # Add register-raw-data if uncommented above
      uses: Azure/mlops-templates/.github/workflows/run-pipeline.yml@v1.1.0
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        # experiment_name: fraud-detection-training 
        parameters-file: mlops/azureml/train/pipeline.yml
        job-name: fraud-detection-pipeline-run # Display name for the run

      secrets:
          creds: ${{secrets.AZURE_CREDENTIALS}}

================
File: .github/workflows/deploy-online-endpoint-pipeline.yml
================
name: deploy-online-endpoint-pipeline
on:
  workflow_dispatch:

jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@main
      with:
        file_name: config-infra-prod.yml

  create-endpoint:
      needs: get-config
      uses: Azure/mlops-templates/.github/workflows/create-endpoint.yml@main
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/online/online-endpoint.yml
        endpoint_name: ${{ needs.get-config.outputs.oep }} # Use name from config
        endpoint_type: online
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

  create-deployment:
      uses: Azure/mlops-templates/.github/workflows/create-deployment.yml@main
      needs: [get-config,create-endpoint]
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        endpoint_file: mlops/azureml/deploy/online/online-deployment.yml
        endpoint_name: ${{ needs.get-config.outputs.oep }} # Use name from config
        endpoint_type: online
        deployment_name: blue # Default deployment name (can be parameterized)
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

  allocate-traffic:
      uses: Azure/mlops-templates/.github/workflows/allocate-traffic.yml@main
      needs: [get-config,create-deployment]
      with:
        resource_group: ${{ needs.get-config.outputs.resource_group }}
        workspace_name: ${{ needs.get-config.outputs.aml_workspace }}
        traffic_allocation: blue=100 # Allocate all traffic to the 'blue' deployment
        endpoint_name: ${{ needs.get-config.outputs.oep }} # Use name from config
      secrets:
        creds: ${{secrets.AZURE_CREDENTIALS}}

================
File: .github/workflows/tf-gha-deploy-infra.yml
================
# This file remains largely the same as the template.
# It reads config-infra-prod.yml and uses Terraform to deploy Azure resources.
# No specific changes needed for the fraud detection logic itself,
# assuming the resource requirements (AML workspace, storage, etc.) are standard.

name: tf-gha-deploy-infra.yml

on:
  workflow_dispatch:
env:
    config_env: 'none'
jobs:
  get-config:
      uses: Azure/mlops-templates/.github/workflows/read-yaml.yml@main
      with:
        file_name: config-infra-prod.yml
  test-terraform-state-deployment:
    needs: get-config
    uses: Azure/mlops-templates/.github/workflows/tf-gha-install-terraform.yml@main
    with:
      TFAction: 'apply'
      # Note: The original template had a 'set-env-branch' step which seems missing or simplified here.
      # Assuming config-infra-prod.yml is always used for this workflow dispatch.
      # dply_environment: ${{ needs.set-env-branch.outputs.config-file }} # This would need adjustment if branching logic is added
      dply_environment: 'prod' # Hardcoding for now based on file name
      location: ${{ needs.get-config.outputs.location }}
      namespace: ${{ needs.get-config.outputs.namespace }}
      postfix: ${{ needs.get-config.outputs.postfix }}
      environment: ${{ needs.get-config.outputs.environment }}
      enable_aml_computecluster: ${{ needs.get-config.outputs.enable_aml_computecluster == true }}
      enable_monitoring: ${{ needs.get-config.outputs.enable_monitoring == true  }}
      terraform_version: ${{ needs.get-config.outputs.terraform_version }}
      terraform_workingdir: ${{ needs.get-config.outputs.terraform_workingdir }}
      terraform_st_location: ${{ needs.get-config.outputs.terraform_st_location }}
      terraform_st_storage_account: ${{ needs.get-config.outputs.terraform_st_storage_account }}
      terraform_st_resource_group: ${{ needs.get-config.outputs.terraform_st_resource_group }}
      terraform_st_container_name: ${{ needs.get-config.outputs.terraform_st_container_name }}
      terraform_st_key: ${{ needs.get-config.outputs.terraform_st_key }}
      # terraform_plan_location: ${{ needs.get-config.outputs.location }} # Plan location might not be needed for apply
      # terraform_plan_vnet: "TBD" # VNet planning might be 
      terraform_plan_location: ${{ needs.get-config.outputs.location }} # Use the main location
      terraform_plan_vnet: "none" # Provide a placeholder if no specific VNet in plan
    secrets:
      azure_creds: ${{ secrets.AZURE_CREDENTIALS }}
      clientId: ${{ secrets.ARM_CLIENT_ID }}
      clientSecret: ${{ secrets.ARM_CLIENT_SECRET }}
      subscriptionId: ${{ secrets.ARM_SUBSCRIPTION_ID }}
      tenantId: ${{ secrets.ARM_TENANT_ID }}
  # The deploy-azureml-resources step in the original template seems like a placeholder.
  # The actual deployment happens via the tf-gha-install-terraform.yml reusable workflow.
  # deploy-azureml-resources:
  #   runs-on: ubuntu-latest
  #   steps:
  #   - id: deploy-aml-workspace
  #     name: deploy-aml-workspace
  #     run: echo "OK"

================
File: data-science/environment/train-conda.yml
================
# Conda environment specification for Fraud Detection Training Pipeline
# Based on user's environment.yml and MLOps requirements
name: fraud-detection-train-env
channels:
  - conda-forge # Prioritize conda-forge
  - defaults
dependencies:
  # --- Core Environment ---
  - python=3.9 # Align with user's notebook env
  - pip # IMPORTANT: Include pip itself

  # --- Data Handling & Core ML (from user's env) ---
  - numpy>=1.19.5,<1.24.0 # Example range, adjust if needed
  - pandas>=1.3.5,<1.6.0 # Example range, adjust if needed
  - scikit-learn>=1.0.0,<1.2.0 # Example range, adjust if needed
  - joblib # Often used with scikit-learn

  # --- Specific Libraries (from user's env) ---
  - imbalanced-learn>=0.8.1,<0.11.0 # Example range
  - xgboost>=1.5.1,<1.8.0 # Example range
  - matplotlib # Version compatibility handled by conda usually
  - seaborn>=0.11.2,<0.13.0 # Example range
  - python-graphviz>=0.19.1,<0.21.0 # Use 'python-graphviz' on conda
  - pandarallel>=1.5.4,<1.7.0 # Example range

  # --- MLOps & Azure ML ---
  - pip: # Use pip for Azure ML SDKs and MLflow
      - mlflow>=2.0,<2.10 # Keep mlflow updated
      # Choose V1 or V2 SDK based on preference/environment
      # - azureml-sdk>=1.50.0 # V1 SDK (as used in user notebooks)
      # - azureml-mlflow>=1.50.0 # V1 MLflow integration
      - azure-ai-ml>=1.5.0 # V2 SDK (Recommended for new development)
      - azure-identity>=1.10.0 # Often needed for V2 auth
      - msal>=1.20 # Dependency for azure-identity

      # DVC and Git (Optional, if used within pipeline steps)
      # - dvc[azure]>=2.0,<3.0
      # - GitPython>=3.1,<3.2

      # Cloudpickle often needed for pipeline serialization
      - cloudpickle>=2.0.0,<3.0.0

# Notes:
# - Version ranges are examples; adjust based on exact requirements and compatibility testing.
# - Consider pinning more versions for strict reproducibility.
# - Remove unused libraries (Sphinx?) if not needed for the pipeline steps.
# - If using V1 SDK (azureml-sdk), ensure compatibility with Python 3.9.
# - If using V2 SDK (azure-ai-ml), adjust imports and code in pipeline scripts accordingly.

================
File: data-science/src/evaluate.py
================
# data-science/src/evaluate.py
import os
import argparse
import datetime
from pathlib import Path
import pickle
import json
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn

# Import shared functions from utils.py
from utils import (
    performance_assessment # Still needed
    # get_train_test_set # NO LONGER NEEDED here
)

def parse_args():
    parser = argparse.ArgumentParser("evaluate")
    parser.add_argument("--model_input", type=str, help="Path to input model directory")
    # --- UPDATED: Input the specific test data file ---
    parser.add_argument("--test_data", type=str, help="Path to the final test data file (final_test_data.pkl)")
    parser.add_argument("--evaluation_output", type=str, help="Path to save evaluation results")
    parser.add_argument("--model_name", type=str, help="Name of the model for registration comparison", default="fraud-detection-model")

    # Threshold for deploy flag
    parser.add_argument("--deploy_threshold_metric", type=str, default="Average precision", help="Metric to check for deployment threshold")
    parser.add_argument("--deploy_threshold_value", type=float, default=0.7, help="Threshold value for deployment metric")

    # --- REMOVED date/delta arguments as split is pre-calculated ---
    # parser.add_argument("--final_train_start_date_str", type=str, ...)
    # parser.add_argument("--delta_train", type=int, ...)
    # ... etc ...

    args = parser.parse_args()
    return args

# --- REMOVED load_data_for_split function ---

def main(args):
    mlflow.start_run()
    print("Evaluation script started")
    print(f"Args: {args}")

    evaluation_output_path = Path(args.evaluation_output)
    evaluation_output_path.mkdir(parents=True, exist_ok=True)

    # --- Load Model ---
    print(f"Loading model from: {args.model_input}")
    try:
        model = mlflow.sklearn.load_model(args.model_input)
        print("Model loaded successfully.")
        mlflow.log_param("model_input_path", args.model_input)
    except Exception as e:
        print(f"ERROR: Failed to load model from {args.model_input}: {e}")
        mlflow.log_param("evaluation_error", f"Model loading failed: {e}")
        mlflow.end_run(status="FAILED")
        return

    # --- Load Pre-split Test Data ---
    print(f"Loading final test data from: {args.test_data}")
    test_data_file = Path(args.test_data)
    test_df_final = pd.DataFrame() # Initialize
    if test_data_file.is_file():
        try:
            test_df_final = pd.read_pickle(test_data_file)
            if test_df_final.empty:
                print("Warning: Loaded test data file is empty.")
            else:
                print(f"Loaded final test set with shape: {test_df_final.shape}")
            mlflow.log_metric("test_set_rows", test_df_final.shape[0])
            if not test_df_final.empty and 'TX_FRAUD' in test_df_final.columns:
                 mlflow.log_metric("test_set_fraud_rate", test_df_final['TX_FRAUD'].mean())

        except Exception as e:
            print(f"ERROR loading test data file {test_data_file}: {e}")
            mlflow.log_param("evaluation_error", f"Test data loading failed: {e}")
            mlflow.end_run(status="FAILED")
            return
    else:
        print(f"ERROR: Test data file not found at {test_data_file}")
        mlflow.log_param("evaluation_error", "Test data file not found.")
        mlflow.end_run(status="FAILED")
        return

    # --- Evaluate (if test data loaded) ---
    if test_df_final.empty:
        print("Warning: Final test set is empty. Skipping evaluation.")
        performance_metrics = pd.DataFrame(columns=['AUC ROC', 'Average precision', f'Card Precision@{100}'])
        performance_metrics.loc[0] = [np.nan, np.nan, np.nan]
        deploy_flag = 0 # Cannot evaluate, so do not deploy
    else:
        try:
            # --- Get Predictions ---
            print("Making predictions on test set...")
            INPUT_FEATURES = ['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',
                              'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',
                              'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',
                              'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',
                              'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',
                              'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',
                              'TERMINAL_ID_RISK_30DAY_WINDOW']
            OUTPUT_FEATURE = "TX_FRAUD"

            X_test = test_df_final[INPUT_FEATURES]
            y_test = test_df_final[OUTPUT_FEATURE]

            if X_test.isnull().values.any():
                 print("Warning: NaNs detected in test features before prediction. Pipeline should handle this.")

            predictions = model.predict_proba(X_test)[:, 1]
            test_df_final['predictions'] = predictions
            print("Predictions generated.")

            # --- Evaluate Performance ---
            print("Calculating performance metrics...")
            performance_metrics = performance_assessment(
                test_df_final, output_feature=OUTPUT_FEATURE, prediction_feature='predictions',
                top_k_list=[100], rounded=False
            )
            print("Performance Metrics on Test Set:")
            print(performance_metrics)

            # --- Log Metrics ---
            print("Logging metrics to MLflow...")
            for metric in performance_metrics.columns:
                value = performance_metrics[metric].iloc[0]
                if not pd.isna(value):
                    mlflow_metric_name = metric.lower().replace(" ", "_").replace("@", "_at_")
                    mlflow.log_metric(f"test_{mlflow_metric_name}", value)
            print("Metrics logged.")

            # --- Determine Deploy Flag ---
            print("Determining deploy flag...")
            deploy_flag = 0
            if args.deploy_threshold_value >= 0 and args.deploy_threshold_metric in performance_metrics.columns:
                metric_value = performance_metrics[args.deploy_threshold_metric].iloc[0]
                if not pd.isna(metric_value) and metric_value >= args.deploy_threshold_value:
                    deploy_flag = 1
                    print(f"Deploy flag set to 1 ({args.deploy_threshold_metric} {metric_value:.4f} >= {args.deploy_threshold_value})")
                else:
                    print(f"Deploy flag remains 0 ({args.deploy_threshold_metric} {metric_value:.4f} < {args.deploy_threshold_value} or NaN)")
            elif args.deploy_threshold_value < 0:
                 deploy_flag = 1
                 print("Deploy flag set to 1 (threshold is negative).")
            else:
                 print(f"Deploy flag remains 0 (metric '{args.deploy_threshold_metric}' not found or threshold <= 0)")

            mlflow.log_metric("deploy_flag", deploy_flag)
            mlflow.log_param("deploy_threshold_metric", args.deploy_threshold_metric)
            mlflow.log_param("deploy_threshold_value", args.deploy_threshold_value)

        except Exception as e:
            print(f"ERROR during prediction or evaluation: {e}")
            import traceback; traceback.print_exc()
            mlflow.log_param("evaluation_error", f"Prediction/eval failed: {e}")
            performance_metrics = pd.DataFrame(columns=['AUC ROC', 'Average precision', f'Card Precision@{100}'])
            performance_metrics.loc[0] = [np.nan, np.nan, np.nan]
            deploy_flag = 0
            mlflow.end_run(status="FAILED")
            # Still try to save empty metrics and flag

    # --- Save Evaluation Results ---
    print(f"Saving evaluation results to {evaluation_output_path}")
    metrics_file = evaluation_output_path / "test_performance_metrics.csv"
    try:
        performance_metrics.round(5).to_csv(metrics_file, index=False)
        mlflow.log_artifact(str(metrics_file))
        print(f"Metrics saved to {metrics_file}")
    except Exception as e: print(f"Error saving metrics file: {e}")

    deploy_flag_file = evaluation_output_path / "deploy_flag"
    try:
        with open(deploy_flag_file, 'w') as f: f.write(str(deploy_flag))
        mlflow.log_artifact(str(deploy_flag_file))
        print(f"Deploy flag ({deploy_flag}) saved to {deploy_flag_file}")
    except Exception as e: print(f"Error saving deploy flag file: {e}")

    mlflow.end_run()
    print("Evaluation script finished.")

if __name__ == "__main__":
    args = parse_args()
    main(args)

================
File: data-science/src/prep.py
================
# data-science/src/prep.py
import os
import argparse
import datetime 
from time import time
from pathlib import Path
import pandas as pd
import numpy as np
import mlflow

# Import shared functions from utils.py
from utils import (
    is_weekend,
    is_night,
    get_customer_spending_behaviour_features,
    get_count_risk_rolling_window
)

# Define maximum lookback needed for feature calculation (30 days + 7 day delay)
MAX_LOOKBACK_DAYS = 37

def parse_args():
    parser = argparse.ArgumentParser("prep")
    parser.add_argument("--raw_data", type=str, help="Path to raw data directory (contains daily .pkl files)")
    parser.add_argument("--transformed_data", type=str, help="Path to output directory for transformed data (.pkl files)")

    # Date range for the DESIRED TRANSFORMED OUTPUT
    parser.add_argument("--output_start_date", type=str, help="Start date for the transformed data output (YYYY-MM-DD)")
    parser.add_argument("--output_end_date", type=str, help="End date for the transformed data output (YYYY-MM-DD)")

    args = parser.parse_args()
    return args

def read_raw_files(DIR_INPUT, BEGIN_DATE_STR, END_DATE_STR):
    """Reads raw pickle files within a calculated date range, handling lookback."""
    try:
        # Calculate the actual raw data start date needed including lookback
        output_start_dt = datetime.datetime.strptime(BEGIN_DATE_STR, "%Y-%m-%d")
        raw_load_start_dt = output_start_dt - datetime.timedelta(days=MAX_LOOKBACK_DAYS)
        raw_load_start_str = raw_load_start_dt.strftime("%Y-%m-%d")
        raw_load_end_str = END_DATE_STR # Load up to the desired output end date

        print(f"Prep: Required transformed output range: {BEGIN_DATE_STR} to {END_DATE_STR}")
        print(f"Prep: Calculated raw data load range (including lookback): {raw_load_start_str} to {raw_load_end_str}")

        files = [os.path.join(DIR_INPUT, f) for f in os.listdir(DIR_INPUT) if f.endswith('.pkl') and raw_load_start_str + '.pkl' <= f <= raw_load_end_str + '.pkl']
        files.sort()
    except FileNotFoundError:
         print(f"ERROR: Raw data input directory not found: {DIR_INPUT}")
         return pd.DataFrame()
    except ValueError as e:
        print(f"ERROR: Invalid date format provided for output range: {e}")
        return pd.DataFrame()

    if not files:
        print(f"WARNING: No raw '.pkl' files found in {DIR_INPUT} for calculated range {raw_load_start_str} to {raw_load_end_str}")
        return pd.DataFrame()

    print(f"Found {len(files)} raw files to load for transformation.")
    frames = []
    required_cols = ['TRANSACTION_ID', 'TX_DATETIME', 'CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT', 'TX_FRAUD']
    for f_path in files:
        try:
            df = pd.read_pickle(f_path)
            if not all(col in df.columns for col in required_cols):
                 print(f"Warning: Required columns missing in {Path(f_path).name}. Skipping file.")
                 continue
            frames.append(df)
        except Exception as e:
            print(f"Error reading raw file {Path(f_path).name}: {e}")

    if not frames:
        print("No raw dataframes were successfully loaded.")
        return pd.DataFrame()

    df_final = pd.concat(frames, ignore_index=True)
    df_final = df_final.sort_values('TRANSACTION_ID')
    df_final.reset_index(drop=True, inplace=True)

    # Convert TX_DATETIME and add time columns
    if 'TX_DATETIME' in df_final.columns and not pd.api.types.is_datetime64_any_dtype(df_final['TX_DATETIME']):
        try:
            df_final['TX_DATETIME'] = pd.to_datetime(df_final['TX_DATETIME'])
        except Exception as e:
            print(f"Warning: Could not convert TX_DATETIME to datetime in raw data: {e}. This may cause errors.")
            df_final['TX_TIME_SECONDS'] = np.nan
            df_final['TX_TIME_DAYS'] = np.nan
            return df_final # Return early if critical conversion fails

    if 'TX_DATETIME' in df_final.columns and pd.api.types.is_datetime64_any_dtype(df_final['TX_DATETIME']):
         # IMPORTANT: Calculate days relative to the *true minimum* date in the loaded raw data
         # to ensure rolling windows work correctly across the entire loaded span.
         min_raw_date = df_final['TX_DATETIME'].dt.date.min()
         if min_raw_date:
             df_final['TX_TIME_SECONDS'] = (df_final['TX_DATETIME'] - df_final['TX_DATETIME'].min()).dt.total_seconds()
             df_final['TX_TIME_DAYS'] = (df_final['TX_DATETIME'].dt.date - min_raw_date).apply(lambda x: x.days)
             print(f"Generated 'TX_TIME_SECONDS' and 'TX_TIME_DAYS' relative to raw data start: {min_raw_date.strftime('%Y-%m-%d')}")
         else:
              print("Warning: Could not determine minimum date from loaded raw data. Cannot generate time columns.")
              df_final['TX_TIME_SECONDS'] = np.nan
              df_final['TX_TIME_DAYS'] = np.nan
    else:
         print("Warning: Cannot generate time-based columns ('TX_TIME_SECONDS', 'TX_TIME_DAYS') as TX_DATETIME is missing or not datetime type.")
         df_final['TX_TIME_SECONDS'] = np.nan
         df_final['TX_TIME_DAYS'] = np.nan

    return df_final


def main(args):
    mlflow.start_run() # Start MLflow run for this component
    print("Preparation script started")
    print(f"Args: {args}")

    # Log parameters
    mlflow.log_param("raw_data_input_path", args.raw_data)
    mlflow.log_param("transformed_data_output_path", args.transformed_data)
    mlflow.log_param("output_start_date", args.output_start_date)
    mlflow.log_param("output_end_date", args.output_end_date)
    mlflow.log_param("max_lookback_days", MAX_LOOKBACK_DAYS)

    # --- Load Raw Data (Handles lookback inside the function) ---
    print(f"Loading raw data from: {args.raw_data} to cover output range {args.output_start_date} to {args.output_end_date} with lookback")
    transactions_df = read_raw_files(args.raw_data, args.output_start_date, args.output_end_date)

    if transactions_df.empty:
        print("ERROR: No raw data loaded after handling lookback. Exiting.")
        mlflow.log_metric("raw_rows_loaded_prep", 0)
        mlflow.end_run(status="FAILED")
        return

    print(f"Loaded {len(transactions_df)} raw transactions for processing.")
    mlflow.log_metric("raw_rows_loaded_prep", len(transactions_df))

    # --- Apply Transformations ---
    # Important: Transformations run on the *entire* loaded df (including lookback period)
    # to ensure correct rolling calculations for the *target* output dates.

    print("Applying feature transformations (this may take time)...")
    start_transform_time = time.time()

    # 1. Date/Time Transformations
    if 'TX_DATETIME' in transactions_df.columns:
        transactions_df['TX_DURING_WEEKEND'] = transactions_df.TX_DATETIME.apply(is_weekend)
        transactions_df['TX_DURING_NIGHT'] = transactions_df.TX_DATETIME.apply(is_night)
        mlflow.set_tag("transformation_datetime", "Success")
    else: mlflow.set_tag("transformation_datetime", "Skipped - Missing Column")

    # 2. Customer Spending Behavior Transformations
    if 'CUSTOMER_ID' in transactions_df.columns and 'TX_DATETIME' in transactions_df.columns:
        try:
            # Apply transformation per group
            transactions_df = transactions_df.groupby('CUSTOMER_ID', group_keys=False).apply(
                lambda x: get_customer_spending_behaviour_features(x, windows_size_in_days=[1, 7, 30])
            )
            # Groupby+apply can mess up index/order, ensure sorting
            transactions_df = transactions_df.sort_values('TRANSACTION_ID').reset_index(drop=True)
            mlflow.set_tag("transformation_customer", "Success")
        except Exception as e:
            print(f"  ERROR during customer spending transformations: {e}")
            mlflow.set_tag("transformation_customer", f"Failed: {e}")
    else: mlflow.set_tag("transformation_customer", "Skipped - Missing Column")

    # 3. Terminal Risk Transformations
    if 'TERMINAL_ID' in transactions_df.columns and 'TX_DATETIME' in transactions_df.columns and 'TX_FRAUD' in transactions_df.columns:
        try:
            # Apply transformation per group
            transactions_df = transactions_df.groupby('TERMINAL_ID', group_keys=False).apply(
                lambda x: get_count_risk_rolling_window(x, delay_period=7, windows_size_in_days=[1, 7, 30], feature="TERMINAL_ID")
            )
            # Groupby+apply can mess up index/order, ensure sorting
            transactions_df = transactions_df.sort_values('TRANSACTION_ID').reset_index(drop=True)
            mlflow.set_tag("transformation_terminal", "Success")
        except Exception as e:
            print(f"  ERROR during terminal risk transformations: {e}")
            mlflow.set_tag("transformation_terminal", f"Failed: {e}")
    else: mlflow.set_tag("transformation_terminal", "Skipped - Missing Column")

    transform_time = time.time() - start_transform_time
    print(f"Transformations applied in {transform_time:.2f} seconds.")
    mlflow.log_metric("prep_transform_time_sec", transform_time)

    # --- Filter and Save Transformed Data ---
    # Now, filter the dataframe to keep only the desired output date range before saving daily files.
    print(f"Filtering transformed data to output range: {args.output_start_date} to {args.output_end_date}")
    try:
        output_start_dt = datetime.datetime.strptime(args.output_start_date, "%Y-%m-%d")
        output_end_dt = datetime.datetime.strptime(args.output_end_date, "%Y-%m-%d")

        # Filter based on TX_DATETIME
        transactions_df_filtered = transactions_df[
            (transactions_df['TX_DATETIME'] >= output_start_dt) &
            (transactions_df['TX_DATETIME'] < output_end_dt + datetime.timedelta(days=1)) # Make end date inclusive
        ].copy()

        print(f"Filtered data shape: {transactions_df_filtered.shape}")
        if transactions_df_filtered.empty:
            print("ERROR: No data remains after filtering for the desired output date range. No files will be saved.")
            mlflow.log_metric("transformed_files_saved", 0)
            mlflow.log_metric("transformed_rows_saved", 0)
            mlflow.set_tag("Data Saving Status", "Failed - Empty after filter")
            mlflow.end_run(status="FAILED")
            return

    except Exception as e:
        print(f"ERROR filtering data for output range: {e}")
        mlflow.set_tag("Data Saving Status", "Failed - Filtering error")
        mlflow.end_run(status="FAILED")
        return

    # --- Save Filtered Data to Daily Files ---
    print(f"Saving filtered transformed data to: {args.transformed_data}")
    output_path = Path(args.transformed_data)
    output_path.mkdir(parents=True, exist_ok=True)

    # Need TX_TIME_DAYS relative to the output start date for saving correctly named files
    # Re-calculate TX_TIME_DAYS based on the filtered data's min date (should be output_start_date)
    min_output_date = transactions_df_filtered['TX_DATETIME'].dt.date.min()
    if min_output_date:
        transactions_df_filtered['OUTPUT_TX_TIME_DAYS'] = (transactions_df_filtered['TX_DATETIME'].dt.date - min_output_date).apply(lambda x: x.days)
        print(f"Generated 'OUTPUT_TX_TIME_DAYS' relative to output start: {min_output_date.strftime('%Y-%m-%d')}")

        start_save_time = time.time()
        max_output_days = transactions_df_filtered.OUTPUT_TX_TIME_DAYS.max()
        days_processed = 0
        total_rows_saved = 0

        # Use the newly calculated relative days for saving loop
        for day_idx in range(max_output_days + 1):
            transactions_day = transactions_df_filtered[transactions_df_filtered.OUTPUT_TX_TIME_DAYS == day_idx].sort_values('TX_TIME_SECONDS')

            if not transactions_day.empty:
                actual_date = min_output_date + datetime.timedelta(days=day_idx)
                filename_output = actual_date.strftime("%Y-%m-%d") + '.pkl'
                file_path = output_path / filename_output
                try:
                    # Drop the temporary day column before saving
                    transactions_day_to_save = transactions_day.drop(columns=['OUTPUT_TX_TIME_DAYS'])
                    transactions_day_to_save.to_pickle(str(file_path), protocol=4)
                    days_processed += 1
                    total_rows_saved += len(transactions_day_to_save)
                except Exception as e:
                    print(f"  Error saving file {filename_output}: {e}")

        save_time = time.time() - start_save_time
        print(f"Saved transformed data for {days_processed} days ({total_rows_saved} rows) in {args.transformed_data} ({save_time:.2f} seconds)")
        mlflow.log_metric("transformed_files_saved", days_processed)
        mlflow.log_metric("transformed_rows_saved", total_rows_saved)
        mlflow.log_metric("prep_save_time_sec", save_time)
        mlflow.set_tag("Data Saving Status", "Success")
    else:
         print("ERROR: Cannot save daily files because min output date could not be determined.")
         mlflow.set_tag("Data Saving Status", "Failed - Min Date Error")
         mlflow.end_run(status="FAILED")
         return

    mlflow.end_run()
    print("Preparation script finished.")


if __name__ == "__main__":
    args = parse_args()
    main(args)

================
File: data-science/src/register.py
================
# data-science/src/register.py
import argparse
from pathlib import Path
import pickle
import json
import os
import mlflow
import mlflow.sklearn

def parse_args():
    parser = argparse.ArgumentParser("register")
    parser.add_argument('--model_name', type=str, help='Name under which model will be registered', default="fraud-detection-model")
    parser.add_argument('--model_path', type=str, help='Path to trained model directory')
    parser.add_argument('--evaluation_output', type=str, help='Path of evaluation results directory (contains deploy_flag)')
    parser.add_argument('--model_info_output_path', type=str, help="Path to write model info JSON")
    args, _ = parser.parse_known_args()
    print(f'Arguments: {args}')
    return args

def main(args):
    mlflow.start_run() # Start MLflow run for this component
    print("Register script started")

    # --- Read Deploy Flag ---
    deploy_flag = 0 # Default to not deploying
    deploy_flag_file = Path(args.evaluation_output) / "deploy_flag"
    if deploy_flag_file.exists():
        try:
            with open(deploy_flag_file, 'r') as f:
                deploy_flag = int(f.read().strip())
            print(f"Read deploy_flag: {deploy_flag}")
        except Exception as e:
            print(f"Warning: Could not read deploy flag file at {deploy_flag_file}. Defaulting to 0. Error: {e}")
    else:
        print(f"Warning: Deploy flag file not found at {deploy_flag_file}. Defaulting to 0.")

    mlflow.log_metric("deploy_flag_read", deploy_flag)

    # --- Register Model Condition ---
    # Set deploy_flag=1 to force registration for testing/simplicity if needed
    # deploy_flag = 1
    # print(f"Overriding deploy_flag to: {deploy_flag}")

    if deploy_flag == 1:
        print(f"Attempting to register model '{args.model_name}' from path '{args.model_path}'...")
        try:
            # Load model (optional, but good practice to check loading)
            # model = mlflow.sklearn.load_model(args.model_path)
            # print("Model loaded successfully for verification.")

            # Register the model artifact logged in the parent run (usually from train step)
            # Assuming the model was logged as an artifact named 'model'
            # Need the parent run_id if this is a separate run, or use active run if logged here.
            # For simplicity in AML pipelines, assume model was logged by train step.
            # We reference the *path* passed as input, which *is* the logged model artifact dir.

            # The path passed to --model_path *is* the MLflow model directory
            model_uri = args.model_path

            # Check if URI looks like an MLflow artifact path (optional)
            if not Path(model_uri, "MLmodel").exists():
                 print(f"Warning: MLmodel file not found in {model_uri}. Registration might fail or use unexpected format.")

            print(f"Registering model from URI: {model_uri}")
            registered_model = mlflow.register_model(
                model_uri=model_uri,
                name=args.model_name
            )
            model_version = registered_model.version
            print(f"Successfully registered model '{args.model_name}' version {model_version}")
            mlflow.log_param("registered_model_name", args.model_name)
            mlflow.log_param("registered_model_version", model_version)

            # Write model info JSON output for downstream steps (like deployment workflows)
            print("Writing model info JSON...")
            model_info = {"id": f"{args.model_name}:{model_version}"}
            output_dir = Path(args.model_info_output_path)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / "model_info.json"
            try:
                with open(output_path, "w") as of:
                    json.dump(model_info, fp=of, indent=4)
                print(f"Model info saved to {output_path}")
                mlflow.log_artifact(str(output_path))
            except Exception as e:
                 print(f"Error writing model info JSON: {e}")

        except Exception as e:
            print(f"ERROR during model registration: {e}")
            mlflow.log_param("registration_error", str(e))
            # Optionally, fail the run explicitly
            mlflow.end_run(status="FAILED")
            return # Stop execution

    else:
        print("Deploy flag is 0. Model will not be registered.")
        # Create an empty model_info.json to avoid downstream errors if file is expected
        print("Creating empty model info JSON.")
        model_info = {"id": "None:0"} # Indicate no model registered
        output_dir = Path(args.model_info_output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / "model_info.json"
        try:
            with open(output_path, "w") as of:
                json.dump(model_info, fp=of, indent=4)
            print(f"Empty model info saved to {output_path}")
            # mlflow.log_artifact(str(output_path)) # Optionally log the empty file
        except Exception as e:
             print(f"Error writing empty model info JSON: {e}")

    mlflow.end_run()
    print("Register script finished.")

if __name__ == "__main__":
    args = parse_args()
    main(args)

================
File: data-science/src/train.py
================
# data-science/src/train.py
import os
import argparse
import datetime
import time
import pickle
import json
from pathlib import Path
import pandas as pd
import numpy as np

import sklearn
from sklearn import model_selection, metrics, pipeline, preprocessing, tree, ensemble
import xgboost

import mlflow
import mlflow.sklearn

# Import shared functions from utils.py
from utils import (
    card_precision_top_k_custom,
    get_summary_performances,
    model_selection_wrapper,
    prequentialSplit_with_dates,
    get_train_test_set
)

def parse_args():
    parser = argparse.ArgumentParser("train")
    parser.add_argument("--transformed_data", type=str, help="Path to folder containing ALL transformed data files (.pkl) from prep step")
    parser.add_argument("--model_output", type=str, help="Path to save output model artifact")
    parser.add_argument("--test_data_output", type=str, help="Path to save the final test data split (as folder containing pkl)")

    # --- CORRECTED DATE ARGUMENTS ---
    # Dates defining the specific window of TRANSFORMED data to load for this step
    parser.add_argument("--train_load_start_date", type=str, required=True, help="Start date of transformed data window to load (YYYY-MM-DD)")
    parser.add_argument("--train_load_end_date", type=str, required=True, help="End date of transformed data window to load (YYYY-MM-DD)")
    # Anchor date for splitting the loaded window
    parser.add_argument("--anchor_date_str", type=str, required=True, help="Anchor date for deriving training/validation splits (YYYY-MM-DD)")

    # --- DELTA ARGUMENTS ---
    parser.add_argument("--delta_train", type=int, default=7, help="Duration of training period in days")
    parser.add_argument("--delta_delay", type=int, default=7, help="Duration of delay period in days")
    parser.add_argument("--delta_assessment", type=int, default=7, help="Duration of assessment period in days")
    parser.add_argument("--n_folds", type=int, default=4, help="Number of folds for prequential validation")

    # --- Other Parameters ---
    parser.add_argument("--top_k_value", type=int, default=100, help="Value K for Card Precision@k")
    parser.add_argument("--n_jobs", type=int, default=5, help="Number of parallel jobs for GridSearchCV")

    args = parser.parse_args()
    return args

def load_transformed_data_window(data_path, start_date_str, end_date_str):
    """Loads transformed pickle files ONLY within the specified window."""
    print(f"Train: Loading transformed data from path '{data_path}' for window: {start_date_str} to {end_date_str}")
    all_files = sorted([f for f in Path(data_path).glob('*.pkl') if f.is_file()])
    target_files = [
        f for f in all_files
        if start_date_str <= f.stem <= end_date_str # Filter based on dates
    ]

    if not target_files:
        print(f"ERROR: No transformed files found in {data_path} for required training window {start_date_str} to {end_date_str}")
        return pd.DataFrame()

    print(f"Found {len(target_files)} transformed files for training window.")
    frames = []
    for f_path in target_files:
         try:
             df = pd.read_pickle(f_path)
             frames.append(df)
         except Exception as e:
              print(f"Error reading transformed file {f_path.name}: {e}")

    if not frames:
        print("No transformed dataframes were successfully loaded for the window.")
        return pd.DataFrame()

    df_final = pd.concat(frames, ignore_index=True)
    df_final = df_final.sort_values('TRANSACTION_ID').reset_index(drop=True)
    if 'TX_DATETIME' in df_final.columns and not pd.api.types.is_datetime64_any_dtype(df_final['TX_DATETIME']):
         df_final['TX_DATETIME'] = pd.to_datetime(df_final['TX_DATETIME'])
    return df_final

def main(args):
    mlflow.start_run()
    print("Training script started")
    print(f"Args: {args}")

    # Log parameters (including the newly passed dates)
    mlflow.log_params({k: v for k, v in vars(args).items() if k not in ['transformed_data', 'model_output', 'test_data_output']})
    # Explicitly log the train window dates passed as args
    mlflow.log_param("train_data_load_start", args.train_load_start_date)
    mlflow.log_param("train_data_load_end", args.train_load_end_date)


    # --- Load SPECIFIC Window of Transformed Data using arguments ---
    print(f"Loading specific window of transformed data ({args.train_load_start_date} to {args.train_load_end_date}) from input path: {args.transformed_data}")
    transactions_df = load_transformed_data_window(
        Path(args.transformed_data),
        args.train_load_start_date, # Use argument
        args.train_load_end_date    # Use argument
    )

    if transactions_df.empty:
        print(f"ERROR: No transformed data loaded for the required window {args.train_load_start_date} to {args.train_load_end_date}. Exiting.")
        mlflow.log_metric("transformed_rows_loaded_train", 0)
        mlflow.end_run(status="FAILED")
        return

    print(f"Loaded {len(transactions_df)} transformed transactions for training/selection window.")
    mlflow.log_metric("transformed_rows_loaded_train", len(transactions_df))

    # Define features and output
    OUTPUT_FEATURE = "TX_FRAUD"
    INPUT_FEATURES = ['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',
                      'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',
                      'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',
                      'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',
                      'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',
                      'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',
                      'TERMINAL_ID_RISK_30DAY_WINDOW']
    mlflow.log_param("input_features", json.dumps(INPUT_FEATURES))
    mlflow.log_param("output_feature", OUTPUT_FEATURE)


    # --- Derive Dates (Based on Anchor Date arg) ---
    try:
        start_date_training_anchor = datetime.datetime.strptime(args.anchor_date_str, "%Y-%m-%d")
        start_date_validation = start_date_training_anchor - datetime.timedelta(days=(args.delta_delay + args.delta_assessment))
        start_date_test_estimation = start_date_training_anchor
        final_train_start_date = start_date_training_anchor

        print(f"\nDerived Dates:")
        print(f"  Anchor Date: {start_date_training_anchor.strftime('%Y-%m-%d')}")
        print(f"  Validation GridSearch Start: {start_date_validation.strftime('%Y-%m-%d')}")
        print(f"  Test Estimation GridSearch Start: {start_date_test_estimation.strftime('%Y-%m-%d')}")
        print(f"  Final Training Start: {final_train_start_date.strftime('%Y-%m-%d')}")

        mlflow.log_param("derived_validation_start_date", start_date_validation.strftime('%Y-%m-%d'))
        mlflow.log_param("derived_test_estimation_start_date", start_date_test_estimation.strftime('%Y-%m-%d'))

    except ValueError as e:
        print(f"ERROR parsing anchor date '{args.anchor_date_str}': {e}")
        mlflow.end_run(status="FAILED")
        return


    # --- Model Selection (Grid Search for XGBoost ONLY) ---
    print("\n===== Starting Model Selection (XGBoost Only) =====")
    start_selection_time = time.time()

    classifier_xgb = xgboost.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss', n_jobs=1)
    parameters_xgb = {
        'clf__max_depth': [3, 6, 9], 'clf__n_estimators': [25, 50, 100], 'clf__learning_rate': [0.1, 0.3],
        'clf__random_state':[0], 'clf__n_jobs':[1], 'clf__verbosity':[0],
        'clf__use_label_encoder':[False], 'clf__eval_metric':['logloss']
    }
    mlflow.log_param("xgboost_param_grid", json.dumps({k:str(v) for k,v in parameters_xgb.items()}))

    # Prepare scorer dataframe subset from the loaded window
    transactions_df_scorer = pd.DataFrame()
    try:
        scorer_cols = ['CUSTOMER_ID', 'TX_FRAUD', 'TX_TIME_DAYS']
        if not all(col in transactions_df.columns for col in scorer_cols):
            raise ValueError(f"Missing columns required for scorer DF: {scorer_cols}")
        transactions_df_scorer = transactions_df[scorer_cols].copy()
        print(f"Created scorer helper DataFrame with shape: {transactions_df_scorer.shape}")
    except Exception as e:
        print(f"ERROR creating scorer helper DF: {e}")
        mlflow.end_run(status="FAILED")
        return

    # Create custom scorer
    card_precision_top_k_scorer = None
    if not transactions_df_scorer.empty:
        try:
            card_precision_top_k_scorer = sklearn.metrics.make_scorer(
                card_precision_top_k_custom, needs_proba=True,
                top_k=args.top_k_value, transactions_df=transactions_df_scorer)
            print(f"Custom scorer 'card_precision@{args.top_k_value}' created.")
        except Exception as e:
            print(f"Warning: Failed to create custom scorer: {e}.")

    # Define scoring dictionary
    scoring = {'roc_auc': 'roc_auc', 'average_precision': 'average_precision'}
    if card_precision_top_k_scorer:
        scoring[f'card_precision@{args.top_k_value}'] = card_precision_top_k_scorer
    performance_metrics_list_grid = list(scoring.keys())
    performance_metrics_list = ['AUC ROC', 'Average precision']
    if card_precision_top_k_scorer: performance_metrics_list.append(f'Card Precision@{args.top_k_value}')

    # Run model selection wrapper FOR XGBOOST ONLY
    performances_df_xgb = pd.DataFrame()
    try:
        performances_df_xgb = model_selection_wrapper(
            transactions_df, # Use the loaded window
            classifier_xgb, INPUT_FEATURES, OUTPUT_FEATURE,
            parameters_xgb, scoring,
            start_date_validation, start_date_test_estimation, # Use DERIVED dates
            n_folds=args.n_folds,
            delta_train=args.delta_train, delta_delay=args.delta_delay, delta_assessment=args.delta_assessment,
            performance_metrics_list_grid=performance_metrics_list_grid,
            performance_metrics_list=performance_metrics_list, n_jobs=args.n_jobs
        )
        selection_time = time.time() - start_selection_time
        print(f"XGBoost GridSearchCV finished in {selection_time:.2f} seconds.")
        mlflow.log_metric("xgb_gridsearch_time_sec", selection_time)

        if not performances_df_xgb.empty:
             perf_artifact_path = "xgboost_grid_search_results.csv"
             performances_df_xgb.to_csv(perf_artifact_path, index=False)
             mlflow.log_artifact(perf_artifact_path)
             print(f"Logged XGBoost grid search results to {perf_artifact_path}")
        else:
             print("Warning: XGBoost grid search returned empty results.")
             mlflow.set_tag("XGBoost Grid Search Status", "Completed - No Results")

    except Exception as e:
        print(f"ERROR during XGBoost GridSearchCV: {e}")
        import traceback; traceback.print_exc()
        mlflow.log_metric("xgb_gridsearch_time_sec", time.time() - start_selection_time)
        mlflow.set_tag("XGBoost Grid Search Status", f"Failed: {e}")

    # --- Determine Best Parameters for XGBoost ---
    best_params_xgb_dict = None
    best_params_xgb_summary = "Defaults"
    primary_metric = 'Average precision'

    if not performances_df_xgb.empty:
        print("\nDetermining best XGBoost parameters...")
        try:
            if 'Parameters summary' not in performances_df_xgb.columns:
                 if 'Parameters' in performances_df_xgb.columns:
                     def params_to_str_fallback(params):
                         try:
                             if isinstance(params, dict): items = [f"{k.split('__')[1]}={v}" for k, v in sorted(params.items())]; return ", ".join(items)
                             else: return str(params)
                         except Exception: return str(params)
                     performances_df_xgb['Parameters summary'] = performances_df_xgb['Parameters'].apply(params_to_str_fallback)
                     print("Created fallback 'Parameters summary' column.")
                 else: raise KeyError("Missing parameter summary columns.")

            summary_xgb = get_summary_performances(performances_df_xgb, parameter_column_name="Parameters summary")

            if primary_metric in summary_xgb.columns:
                best_params_xgb_summary = summary_xgb.loc["Best estimated parameters", primary_metric]
                validation_perf_str = summary_xgb.loc["Validation performance", primary_metric]
                best_row = performances_df_xgb[performances_df_xgb['Parameters summary'] == best_params_xgb_summary]
                if not best_row.empty:
                    best_params_xgb_dict = best_row['Parameters'].iloc[0]
                    print(f"Best XGBoost parameters (Summary): {best_params_xgb_summary}")
                    mlflow.set_tag("best_xgb_params_source", "GridSearch")
                    mlflow.log_param("best_xgb_params_summary", best_params_xgb_summary)
                    try:
                         best_val_score = float(validation_perf_str.split('+/-')[0])
                         mlflow.log_metric(f"best_validation_{primary_metric.lower().replace(' ','_')}", best_val_score)
                    except: pass
                else: print(f"Warning: Could not find row matching best '{best_params_xgb_summary}'. Using defaults.")
            else: print(f"Warning: Primary metric '{primary_metric}' not in summary. Using defaults.")
        except Exception as e: print(f"Error determining best XGBoost parameters: {e}. Using defaults.")
    else: print("XGBoost grid search results empty. Using default parameters.")

    # Fallback to default parameters
    if best_params_xgb_dict is None:
        best_params_xgb_dict = {
            'clf__max_depth': 6, 'clf__n_estimators': 100, 'clf__learning_rate': 0.3,
            'clf__random_state': 0, 'clf__n_jobs': 1, 'clf__verbosity': 0,
            'clf__use_label_encoder': False, 'clf__eval_metric': 'logloss' }
        best_params_xgb_summary = "eval_metric=logloss, learning_rate=0.3, max_depth=6, n_estimators=100, n_jobs=1, random_state=0, use_label_encoder=False, verbosity=0"
        print(f"Using default XGBoost parameters: {best_params_xgb_summary}")
        mlflow.set_tag("best_xgb_params_source", "Default")
        mlflow.log_param("best_xgb_params_summary", best_params_xgb_summary)

    # --- Train Final XGBoost Model ---
    print("\n===== Training Final XGBoost Model =====")

    # Prepare final training data split using the DERIVED final_train_start_date
    final_train_df = pd.DataFrame()
    final_test_df = pd.DataFrame()
    try:
        # Use get_train_test_set on the ALREADY LOADED data window (transactions_df)
        (final_train_df, final_test_df) = get_train_test_set(
            transactions_df, # Use the window loaded at the start of this script
            start_date_training=final_train_start_date, # Use derived date
            delta_train=args.delta_train, delta_delay=args.delta_delay, delta_test=args.delta_assessment
        )
        if final_train_df.empty: raise ValueError("Final training set is empty after split.")
        print(f"Final training set shape: {final_train_df.shape}")
        print(f"Final test set shape: {final_test_df.shape}")
        mlflow.log_metric("final_train_rows", final_train_df.shape[0])
        mlflow.log_metric("final_test_rows", final_test_df.shape[0])
    except Exception as e:
        print(f"ERROR creating final train/test split: {e}")
        mlflow.end_run(status="FAILED")
        return

    # Create final pipeline
    final_classifier_xgb = xgboost.XGBClassifier()
    final_params_xgb_filtered = {k.split('__', 1)[1]: v for k, v in best_params_xgb_dict.items() if k.startswith('clf__')}
    if 'use_label_encoder' in final_params_xgb_filtered: final_params_xgb_filtered['use_label_encoder'] = bool(str(final_params_xgb_filtered['use_label_encoder']).lower() == 'true')
    if 'verbosity' in final_params_xgb_filtered: final_params_xgb_filtered['verbosity'] = int(final_params_xgb_filtered['verbosity'])
    try:
        final_classifier_xgb.set_params(**final_params_xgb_filtered)
        print(f"Set final XGBoost params: {final_params_xgb_filtered}")
    except Exception as e:
        print(f"Warning: Error setting final XGBoost params: {e}. Using defaults.")
        final_classifier_xgb = xgboost.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss', n_jobs=1)

    final_pipeline = sklearn.pipeline.Pipeline([
        ('scaler', sklearn.preprocessing.StandardScaler()), ('clf', final_classifier_xgb)
    ])

    # Fit final pipeline
    start_fit_time = time.time()
    try:
        X_train_final = final_train_df[INPUT_FEATURES]
        y_train_final = final_train_df[OUTPUT_FEATURE]
        if X_train_final.isnull().values.any(): print("Warning: NaNs detected in final training features.")

        final_pipeline.fit(X_train_final, y_train_final)
        final_fit_time = time.time() - start_fit_time
        print(f"Final XGBoost model fitted in {final_fit_time:.2f} seconds.")
        mlflow.log_metric("final_model_train_time_sec", final_fit_time)

        # --- Log and Save Final Model ---
        print(f"Logging final model to MLflow run and preparing output path: {args.model_output}")
        mlflow.sklearn.log_model(
            sk_model=final_pipeline, artifact_path="model",
            serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE )
        mlflow.set_tag("Final Model Training Status", "Success")
        print(f"Model artifact expected at pipeline output path: {args.model_output}")

        # --- Save Final Test Data Split to Output Path ---
        print(f"Saving final test data split to output path: {args.test_data_output}")
        test_data_output_path = Path(args.test_data_output)
        test_data_output_path.mkdir(parents=True, exist_ok=True)
        test_data_file = test_data_output_path / "final_test_data.pkl"
        try:
            if not final_test_df.empty:
                final_test_df.to_pickle(test_data_file)
                print(f"Final test data saved to {test_data_file}")
            else: print("Warning: Final test data is empty, not saving file.")
        except Exception as e: print(f"ERROR saving final test data: {e}")

    except Exception as e:
        print(f"ERROR fitting final XGBoost model: {e}")
        import traceback; traceback.print_exc()
        mlflow.set_tag("Final Model Training Status", f"Failed: {e}")
        mlflow.end_run(status="FAILED")
        return

    mlflow.end_run()
    print("Training script finished.")

if __name__ == "__main__":
    args = parse_args()
    main(args)

================
File: data-science/src/utils.py
================
# data-science/src/utils.py
# Contains shared functions adapted from the user's notebooks

import os
import datetime
import time
import pickle
import json
import pandas as pd
import numpy as np

import sklearn
from sklearn import metrics, preprocessing, model_selection, pipeline, tree, ensemble, linear_model

# === Data Loading/Saving ===

def save_object(obj, filename):
    """Saves object as pickle file."""
    with open(filename, 'wb') as output:
        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)

# === Data Preprocessing & Transformation ===

def is_weekend(tx_datetime):
    """Checks if a datetime object falls on a weekend."""
    weekday = tx_datetime.weekday() # Monday is 0 and Sunday is 6
    return int(weekday >= 5)

def is_night(tx_datetime):
    """Checks if a datetime object's time is between 00:00 and 06:00."""
    tx_hour = tx_datetime.hour
    return int(tx_hour <= 6)

def get_customer_spending_behaviour_features(customer_transactions, windows_size_in_days=[1,7,30]):
    """Calculates customer spending behavior features over specified windows."""
    # Let us first order transactions chronologically
    customer_transactions=customer_transactions.sort_values('TX_DATETIME')

    # The transaction date and time is set as the index, which will allow the use of the rolling function
    # Ensure index is unique if TX_DATETIME has duplicates, append a counter or use Transaction ID if unique per customer
    customer_transactions.index=customer_transactions.TX_DATETIME

    # For each window size
    for window_size in windows_size_in_days:

        # Compute the sum of the transaction amounts and the number of transactions for the given window size
        # Rolling sums/counts include the current transaction
        SUM_AMOUNT_TX_WINDOW=customer_transactions['TX_AMOUNT'].rolling(str(window_size)+'d').sum()
        NB_TX_WINDOW=customer_transactions['TX_AMOUNT'].rolling(str(window_size)+'d').count()

        # Compute the average transaction amount for the given window size
        # NB_TX_WINDOW is always >=1 since current transaction is always included
        AVG_AMOUNT_TX_WINDOW=SUM_AMOUNT_TX_WINDOW/NB_TX_WINDOW

        # Save feature values
        customer_transactions['CUSTOMER_ID_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)
        customer_transactions['CUSTOMER_ID_AVG_AMOUNT_'+str(window_size)+'DAY_WINDOW']=list(AVG_AMOUNT_TX_WINDOW)

    # Reindex according to transaction IDs (assuming TRANSACTION_ID is unique)
    customer_transactions.index=customer_transactions.TRANSACTION_ID

    # And return the dataframe with the new features
    return customer_transactions

def get_count_risk_rolling_window(terminal_transactions, delay_period=7, windows_size_in_days=[1,7,30], feature="TERMINAL_ID"):
    """Calculates terminal risk features over specified windows with a delay."""
    terminal_transactions=terminal_transactions.sort_values('TX_DATETIME')

    # Set index for rolling calculation
    terminal_transactions.index=terminal_transactions.TX_DATETIME

    # Calculate frauds and transactions within the delay period immediately preceding the current transaction
    NB_FRAUD_DELAY=terminal_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').sum()
    NB_TX_DELAY=terminal_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').count()

    # Calculate frauds and transactions for the window size plus the delay period
    for window_size in windows_size_in_days:

        NB_FRAUD_DELAY_WINDOW=terminal_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').sum()
        NB_TX_DELAY_WINDOW=terminal_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').count()

        # Calculate frauds and transactions for the specific window (shifted back by delay)
        NB_FRAUD_WINDOW=NB_FRAUD_DELAY_WINDOW-NB_FRAUD_DELAY
        NB_TX_WINDOW=NB_TX_DELAY_WINDOW-NB_TX_DELAY

        # Calculate risk score for the window
        # Avoid division by zero: if NB_TX_WINDOW is 0, risk is 0
        RISK_WINDOW = (NB_FRAUD_WINDOW / NB_TX_WINDOW).fillna(0)

        terminal_transactions[feature+'_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)
        terminal_transactions[feature+'_RISK_'+str(window_size)+'DAY_WINDOW']=list(RISK_WINDOW)

    # Restore original index
    terminal_transactions.index=terminal_transactions.TRANSACTION_ID

    # Replace any remaining NA values (should not happen with fillna(0) above, but as safeguard)
    terminal_transactions.fillna(0,inplace=True)

    return terminal_transactions

# === Train/Test Splitting ===

def get_train_test_set(transactions_df,
                       start_date_training,
                       delta_train=7,delta_delay=7,delta_test=7,
                       sampling_ratio=1.0, # Not used in final model pipeline, kept for function consistency
                       random_state=0):    # Not used in final model pipeline, kept for function consistency
    """Gets Train and Test sets for a given start date and time deltas."""
    # Validate inputs
    required_cols = ['TX_DATETIME', 'TX_TIME_DAYS', 'CUSTOMER_ID', 'TX_FRAUD', 'TRANSACTION_ID']
    if not all(col in transactions_df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in transactions_df.columns]
        raise ValueError(f"Missing required columns in transactions_df for get_train_test_set: {missing}")
    if not isinstance(start_date_training, datetime.datetime):
         raise ValueError("start_date_training must be a datetime object")

    # Get the training set data based on TX_DATETIME
    train_df = transactions_df[
        (transactions_df.TX_DATETIME >= start_date_training) &
        (transactions_df.TX_DATETIME < start_date_training + datetime.timedelta(days=delta_train))
    ]

    # Get the test set data, applying the delay logic
    test_df_list = []

    # First, get known defrauded customers from the training set
    known_defrauded_customers = set(train_df[train_df.TX_FRAUD == 1].CUSTOMER_ID)

    # Get the relative starting day of training set (using TX_TIME_DAYS)
    if train_df.empty:
        print(f"Warning (get_train_test_set): Training period starting {start_date_training.strftime('%Y-%m-%d')} is empty.")
        # Return empty DataFrames matching expected columns
        return (train_df.copy(), pd.DataFrame(columns=transactions_df.columns))

    start_tx_time_days_training = train_df.TX_TIME_DAYS.min()

    # Iterate through each day of the test period delta
    for day in range(delta_test):
        # Target day for test data
        current_test_day = start_tx_time_days_training + delta_train + delta_delay + day
        test_df_day = transactions_df[transactions_df.TX_TIME_DAYS == current_test_day]

        # Day to check for newly compromised cards (test_day - delay_period)
        # The original logic seems to use 'day-1' relative to the end of the training period start
        delay_check_day = start_tx_time_days_training + delta_train + day - 1 # Check this logic carefully matches original intent
        compromised_cards_delay_period = transactions_df[transactions_df.TX_TIME_DAYS == delay_check_day]

        new_defrauded_customers = set(compromised_cards_delay_period[compromised_cards_delay_period.TX_FRAUD == 1].CUSTOMER_ID)
        known_defrauded_customers.update(new_defrauded_customers) # Use update for sets

        # Filter the current day's test data
        test_df_day_filtered = test_df_day[~test_df_day.CUSTOMER_ID.isin(known_defrauded_customers)]
        test_df_list.append(test_df_day_filtered)

    # Concatenate daily test sets
    if not test_df_list:
        print(f"Warning (get_train_test_set): Test period resulted in an empty set after filtering for training start {start_date_training.strftime('%Y-%m-%d')}.")
        test_df_final = pd.DataFrame(columns=transactions_df.columns)
    else:
        test_df_final = pd.concat(test_df_list, ignore_index=True)


    # Sampling - Not typically used in the final pipeline run, but part of the original function
    if sampling_ratio < 1.0:
        train_df_frauds = train_df[train_df.TX_FRAUD==1].sample(frac=sampling_ratio, random_state=random_state)
        train_df_genuine = train_df[train_df.TX_FRAUD==0].sample(frac=sampling_ratio, random_state=random_state)
        train_df = pd.concat([train_df_frauds, train_df_genuine])

    # Sort final data sets by ascending order of transaction ID
    train_df = train_df.sort_values('TRANSACTION_ID').reset_index(drop=True)
    test_df_final = test_df_final.sort_values('TRANSACTION_ID').reset_index(drop=True)

    return (train_df, test_df_final)


def prequentialSplit_with_dates(transactions_df,
                                start_date_training,
                                n_folds=4,
                                delta_train=7,
                                delta_delay=7,
                                delta_assessment=7):
    """Generates prequential splits, returning indices and printing date ranges for each fold."""
    prequential_split_indices = []
    print(f"\n--- Generating Prequential Folds (n_folds={n_folds}) ---")
    print(f"Base Start Date (Fold 0 Train Start): {start_date_training.strftime('%Y-%m-%d')}")
    print(f"Deltas: Train={delta_train}, Delay={delta_delay}, Assessment={delta_assessment}")
    print("-" * 60)

    for fold in range(n_folds):
        start_date_training_fold = start_date_training - datetime.timedelta(days=fold * delta_assessment)
        end_date_training_fold = start_date_training_fold + datetime.timedelta(days=delta_train)
        start_date_test_fold = end_date_training_fold + datetime.timedelta(days=delta_delay)
        end_date_test_fold = start_date_test_fold + datetime.timedelta(days=delta_assessment)

        inclusive_end_train = end_date_training_fold - datetime.timedelta(days=1)
        inclusive_end_test = end_date_test_fold - datetime.timedelta(days=1)

        print(f"Fold {fold}:")
        print(f"  Train Period: {start_date_training_fold.strftime('%Y-%m-%d')} to {inclusive_end_train.strftime('%Y-%m-%d')}")
        print(f"  Test Period:  {start_date_test_fold.strftime('%Y-%m-%d')} to {inclusive_end_test.strftime('%Y-%m-%d')}")

        try:
            (train_df, test_df) = get_train_test_set(transactions_df,
                                                   start_date_training=start_date_training_fold,
                                                   delta_train=delta_train,
                                                   delta_delay=delta_delay,
                                                   delta_test=delta_assessment)
        except Exception as e:
            print(f"  -> ERROR calling get_train_test_set for fold {fold}: {e}")
            print(f"     Skipping fold {fold}.")
            print("-" * 10)
            continue

        if not train_df.empty and not test_df.empty:
            # Use intersection to ensure indices are valid in the original df passed to GridSearchCV
            valid_train_indices = transactions_df.index.intersection(train_df.index)
            valid_test_indices = transactions_df.index.intersection(test_df.index)
            # Convert to lists for GridSearchCV compatibility
            indices_train = list(valid_train_indices)
            indices_test = list(valid_test_indices)

            if indices_train and indices_test: # Ensure both lists are non-empty after intersection
                 prequential_split_indices.append((indices_train, indices_test))
                 print(f"  -> Train size: {len(indices_train)}, Test size: {len(indices_test)}. Added fold indices.")
            else:
                 print(f"  -> Warning: Fold {fold} resulted in empty train or test indices after intersection. Skipping.")
        else:
             print(f"  -> Warning: Fold {fold} generated empty train ({train_df.shape}) or test ({test_df.shape}) set. Skipping fold.")
        print("-" * 10)

    if not prequential_split_indices:
        print(f"Warning (prequentialSplit): No valid folds generated for start date {start_date_training.strftime('%Y-%m-%d')} and {n_folds} folds.")

    print("--- Finished Generating Prequential Folds ---")
    return prequential_split_indices


# === Performance Assessment ===

def card_precision_top_k_day(df_day,top_k):
    """Computes card precision top k for a single day."""
    # Ensure required columns are present
    required = ['CUSTOMER_ID', 'predictions', 'TX_FRAUD']
    if not all(col in df_day.columns for col in required):
        missing = [col for col in required if col not in df_day.columns]
        print(f"Warning (card_precision_top_k_day): Missing columns {missing}. Returning empty list, 0.")
        return [], 0.0
    if df_day.empty:
        return [], 0.0

    # Group by customer, take max prediction and fraud flag
    df_day_grouped = df_day.groupby('CUSTOMER_ID').agg(
        {'predictions': 'max', 'TX_FRAUD': 'max'}
    ).sort_values(by="predictions", ascending=False).reset_index()

    # Get top k customers
    df_day_top_k = df_day_grouped.head(top_k)
    # Get list of customer IDs from the top k that are actually fraudulent
    list_detected_compromised_cards = list(df_day_top_k[df_day_top_k.TX_FRAUD == 1].CUSTOMER_ID)

    # Compute precision, handle k=0
    if top_k > 0:
        card_precision_top_k = len(list_detected_compromised_cards) / top_k
    else:
        card_precision_top_k = 0.0

    return list_detected_compromised_cards, card_precision_top_k


def card_precision_top_k(predictions_df, top_k, remove_detected_compromised_cards=True):
    """Computes average card precision top k over multiple days."""
    required = ['TX_TIME_DAYS', 'CUSTOMER_ID', 'predictions', 'TX_FRAUD']
    if not all(col in predictions_df.columns for col in required):
        missing = [col for col in required if col not in predictions_df.columns]
        raise ValueError(f"Missing required columns in predictions_df for card_precision_top_k: {missing}")

    list_days=sorted(predictions_df['TX_TIME_DAYS'].unique())
    list_detected_compromised_cards = []
    card_precision_top_k_per_day_list = []
    nb_compromised_cards_per_day = [] # For reference

    for day in list_days:
        df_day = predictions_df[predictions_df['TX_TIME_DAYS'] == day].copy()
        # Filter out already detected cards if required
        if remove_detected_compromised_cards:
             df_day = df_day[~df_day.CUSTOMER_ID.isin(list_detected_compromised_cards)]

        if df_day.empty:
             nb_compromised_cards_per_day.append(0)
             card_precision_top_k_per_day_list.append(0.0)
             continue

        # Keep track of total compromised cards on this day (before filtering for top k)
        nb_compromised_cards_per_day.append(len(df_day[df_day.TX_FRAUD == 1].CUSTOMER_ID.unique()))

        # Calculate daily precision
        detected_compromised_cards, card_precision_top_k_daily = card_precision_top_k_day(
            df_day[['CUSTOMER_ID', 'predictions', 'TX_FRAUD']], top_k
        )
        card_precision_top_k_per_day_list.append(card_precision_top_k_daily)

        # Update list of detected cards for next day's filtering
        if remove_detected_compromised_cards:
            list_detected_compromised_cards.extend(detected_compromised_cards)
            list_detected_compromised_cards = list(set(list_detected_compromised_cards)) # Keep unique

    # Compute the mean, handle empty list
    mean_card_precision_top_k = np.mean(card_precision_top_k_per_day_list) if card_precision_top_k_per_day_list else 0.0

    return nb_compromised_cards_per_day, card_precision_top_k_per_day_list, mean_card_precision_top_k


def card_precision_top_k_custom(y_true, y_pred, top_k, transactions_df):
    """Scorer function for GridSearchCV using Card Precision Top K."""
    # Ensure inputs are usable
    if not isinstance(y_true, pd.Series) or not isinstance(transactions_df, pd.DataFrame):
         print("Warning (CP@k scorer): y_true must be Series, transactions_df must be DataFrame.")
         return 0.0 # Return default score on error
    if len(y_pred) != len(y_true):
        print("Warning (CP@k scorer): y_pred and y_true lengths differ.")
        return 0.0
    if transactions_df.empty:
         print("Warning (CP@k scorer): Scorer helper transactions_df is empty.")
         return 0.0

    # Get indices from y_true (representing the current fold)
    current_fold_indices = y_true.index

    # Select the relevant transactions from the helper dataframe
    # Use intersection to handle cases where index might not exist in helper (shouldn't happen if prepared correctly)
    valid_indices = current_fold_indices.intersection(transactions_df.index)
    if valid_indices.empty:
        print(f"Warning (CP@k scorer): No matching indices found in scorer helper DF for the current fold ({len(current_fold_indices)} indices).")
        return 0.0

    predictions_df_fold = transactions_df.loc[valid_indices].copy()

    # Add predictions, ensuring alignment using the original fold indices
    y_pred_series = pd.Series(y_pred, index=current_fold_indices)
    # Select only the predictions corresponding to the valid indices found in the scorer helper
    predictions_df_fold['predictions'] = y_pred_series.loc[valid_indices]

    # Compute the CP@k metric using the main function
    try:
        _, _, mean_card_precision_top_k = card_precision_top_k(predictions_df_fold, top_k)
    except Exception as e:
        print(f"Error calculating CP@{top_k} within scorer: {e}")
        return 0.0 # Return default score on calculation error

    return mean_card_precision_top_k


def performance_assessment(predictions_df, output_feature='TX_FRAUD',
                           prediction_feature='predictions', top_k_list=[100],
                           rounded=True):
    """Calculates standard and custom performance metrics."""
    required = [output_feature, prediction_feature]
    if not all(col in predictions_df.columns for col in required):
        missing = [col for col in required if col not in predictions_df.columns]
        raise ValueError(f"Missing required columns for performance_assessment: {missing}")

    y_true = predictions_df[output_feature]
    y_pred_proba = predictions_df[prediction_feature]

    AUC_ROC = np.nan
    AP = np.nan
    if len(y_true.unique()) > 1: # Check for multiple classes
        try:
            AUC_ROC = metrics.roc_auc_score(y_true, y_pred_proba)
            AP = metrics.average_precision_score(y_true, y_pred_proba)
        except ValueError as e:
            print(f"Warning (performance_assessment): ValueError calculating AUC/AP: {e}")
    else:
        print("Warning (performance_assessment): Only one class present. AUC ROC and Average Precision are undefined.")

    performances = pd.DataFrame([[AUC_ROC, AP]], columns=['AUC ROC', 'Average precision'])

    # Add CP@k metric(s)
    cpk_required = ['TX_TIME_DAYS', 'CUSTOMER_ID']
    if all(col in predictions_df.columns for col in cpk_required):
        for top_k in top_k_list:
            try:
                _, _, mean_card_precision_top_k = card_precision_top_k(predictions_df, top_k)
                performances[f'Card Precision@{top_k}'] = mean_card_precision_top_k
            except Exception as e:
                 print(f"Warning (performance_assessment): Error calculating CP@{top_k}: {e}")
                 performances[f'Card Precision@{top_k}'] = np.nan
    else:
        missing_cpk = [col for col in cpk_required if col not in predictions_df.columns]
        print(f"Warning (performance_assessment): Skipping Card Precision@k calculation due to missing columns: {missing_cpk}.")
        for top_k in top_k_list:
             performances[f'Card Precision@{top_k}'] = np.nan

    if rounded:
        performances = performances.round(3)

    return performances


def get_class_from_fraud_probability(fraud_probabilities, threshold=0.5):
    """Converts probabilities to binary classes based on a threshold."""
    predicted_classes = [1 if p >= threshold else 0 for p in fraud_probabilities]
    return predicted_classes


def threshold_based_metrics(fraud_probabilities, true_label, thresholds_list):
    """Calculates various threshold-based metrics."""
    results = []
    for threshold in thresholds_list:
        predicted_classes = get_class_from_fraud_probability(fraud_probabilities, threshold=threshold)
        try:
            cm = metrics.confusion_matrix(true_label, predicted_classes, labels=[0, 1])
            TN, FP, FN, TP = cm.ravel()
        except ValueError as e:
            print(f"Warning (threshold_metrics): Confusion matrix error for threshold {threshold}: {e}")
            TN, FP, FN, TP = 0, 0, 0, 0

        # Calculations with division-by-zero handling
        total = TN + FP + FN + TP
        MME = (FP + FN) / total if total > 0 else 0
        TPR = TP / (TP + FN) if (TP + FN) > 0 else 0
        TNR = TN / (TN + FP) if (TN + FP) > 0 else 0
        FPR = FP / (TN + FP) if (TN + FP) > 0 else 0
        FNR = FN / (TP + FN) if (TP + FN) > 0 else 0
        BER = 0.5 * (FPR + FNR)
        Gmean = np.sqrt(TPR * TNR) if TPR >= 0 and TNR >= 0 else 0 # Ensure non-negative for sqrt
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        NPV = TN / (TN + FN) if (TN + FN) > 0 else 0
        FDR = FP / (TP + FP) if (TP + FP) > 0 else 0
        FOR = FN / (TN + FN) if (TN + FN) > 0 else 0
        F1_score = 2 * (precision * TPR) / (precision + TPR) if (precision + TPR) > 0 else 0

        results.append([threshold, MME, TPR, TNR, FPR, FNR, BER, Gmean, precision, NPV, FDR, FOR, F1_score])

    results_df = pd.DataFrame(results, columns=['Threshold', 'MME', 'TPR', 'TNR', 'FPR', 'FNR', 'BER', 'G-mean', 'Precision', 'NPV', 'FDR', 'FOR', 'F1 Score'])
    return results_df


# === Model Selection ===

def params_to_str(params):
    """Helper function to convert parameter dict to a readable, sorted string summary."""
    if not isinstance(params, dict):
        return str(params)
    try:
         items = [f"{k.split('__')[1]}={v}" for k, v in sorted(params.items())]
         return ", ".join(items)
    except Exception:
         return str(params) # Fallback


def prequential_grid_search(transactions_df,
                            classifier,
                            input_features, output_feature,
                            parameters, scoring,
                            start_date_training,
                            n_folds=4,
                            expe_type='Test',
                            delta_train=7,
                            delta_delay=7,
                            delta_assessment=7,
                            performance_metrics_list_grid=['roc_auc'],
                            performance_metrics_list=['AUC ROC'],
                            n_jobs=-1):
    """Performs GridSearchCV using prequential splitting."""
    # Input validation
    if transactions_df.empty:
         print(f"ERROR (prequential_grid_search): Input transactions_df is empty for {expe_type}.")
         return pd.DataFrame() # Return empty matching expected structure later
    if not scoring:
         print(f"ERROR (prequential_grid_search): scoring dictionary is empty for {expe_type}.")
         return pd.DataFrame()

    # Create pipeline
    estimators = [('scaler', sklearn.preprocessing.StandardScaler()), ('clf', classifier)]
    pipe = sklearn.pipeline.Pipeline(estimators)

    # Generate prequential splits (returns list of (train_indices, test_indices))
    prequential_split_indices = prequentialSplit_with_dates(transactions_df,
                                                        start_date_training=start_date_training,
                                                        n_folds=n_folds,
                                                        delta_train=delta_train,
                                                        delta_delay=delta_delay,
                                                        delta_assessment=delta_assessment)

    if not prequential_split_indices:
         print(f"ERROR (prequential_grid_search): No valid prequential splits generated for {expe_type}. Cannot run GridSearchCV.")
         return pd.DataFrame() # Return empty matching expected structure later

    # Setup GridSearchCV
    grid_search = sklearn.model_selection.GridSearchCV(pipe, parameters, scoring=scoring,
                                                       cv=prequential_split_indices, # Use generated indices
                                                       refit=False, # We only care about CV results here
                                                       n_jobs=n_jobs,
                                                       return_train_score=False)

    X = transactions_df[input_features]
    y = transactions_df[output_feature]

    # Handle NaNs (Pipeline's scaler will handle or raise error)
    if X.isnull().values.any():
        print(f"Warning (prequential_grid_search): NaNs detected in features for {expe_type}. Pipeline's StandardScaler should handle this.")
        # If imputation outside pipeline is needed, add it here.

    print(f"Starting GridSearchCV for {expe_type} set (Classifier: {classifier.__class__.__name__})...")
    try:
        grid_search.fit(X, y)
    except Exception as e:
        print(f"ERROR (prequential_grid_search): GridSearchCV fit failed for {expe_type}: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame() # Return empty on failure

    print(f"Finished GridSearchCV for {expe_type} set.")

    # Extract results
    performances_df = pd.DataFrame()
    cv_results = grid_search.cv_results_

    # Map grid keys to display names (assuming lists align)
    metric_mapping = dict(zip(performance_metrics_list_grid, performance_metrics_list))

    for grid_key, display_name in metric_mapping.items():
        mean_score_key = f'mean_test_{grid_key}'
        std_score_key = f'std_test_{grid_key}'

        if mean_score_key in cv_results:
            performances_df[f'{display_name} {expe_type}'] = cv_results[mean_score_key]
        else:
            print(f"Warning: Mean score key '{mean_score_key}' not found in cv_results_ for {expe_type}.")
            performances_df[f'{display_name} {expe_type}'] = np.nan

        if std_score_key in cv_results:
            performances_df[f'{display_name} {expe_type} Std'] = cv_results[std_score_key]
        else:
            # print(f"Note: Std score key '{std_score_key}' not found in cv_results_ for {expe_type}.") # Less verbose
            performances_df[f'{display_name} {expe_type} Std'] = np.nan # Use NaN for missing std


    if 'params' in cv_results:
        performances_df['Parameters'] = cv_results['params']
        performances_df['Parameters summary'] = performances_df['Parameters'].apply(params_to_str)
    else:
        print("Warning: 'params' key not found in cv_results_. Adding empty parameter columns.")
        num_rows = len(next(iter(cv_results.values()))) # Get length from another column
        performances_df['Parameters'] = [{} for _ in range(num_rows)]
        performances_df['Parameters summary'] = 'N/A'


    if 'mean_fit_time' in cv_results:
        performances_df['Execution time'] = cv_results['mean_fit_time']
    else:
        print("Warning: 'mean_fit_time' key not found in cv_results_.")
        performances_df['Execution time'] = np.nan

    return performances_df


def model_selection_wrapper(transactions_df,
                            classifier,
                            input_features, output_feature,
                            parameters,
                            scoring,
                            start_date_training_for_valid,
                            start_date_training_for_test,
                            n_folds=4,
                            delta_train=7,
                            delta_delay=7,
                            delta_assessment=7,
                            performance_metrics_list_grid=['roc_auc'],
                            performance_metrics_list=['AUC ROC'],
                            n_jobs=-1):
    """Wraps prequential grid search for validation and test estimation."""
    # Get performances on the validation set
    print("--- Running Prequential Grid Search for Validation Set ---")
    performances_df_validation = prequential_grid_search(
        transactions_df, classifier,
        input_features, output_feature,
        parameters, scoring,
        start_date_training=start_date_training_for_valid,
        n_folds=n_folds, expe_type='Validation',
        delta_train=delta_train, delta_delay=delta_delay, delta_assessment=delta_assessment,
        performance_metrics_list_grid=performance_metrics_list_grid,
        performance_metrics_list=performance_metrics_list, n_jobs=n_jobs
    )

    # Get performances on the test set estimation
    print("--- Running Prequential Grid Search for Test Set Estimation ---")
    performances_df_test = prequential_grid_search(
        transactions_df, classifier,
        input_features, output_feature,
        parameters, scoring,
        start_date_training=start_date_training_for_test,
        n_folds=n_folds, expe_type='Test',
        delta_train=delta_train, delta_delay=delta_delay, delta_assessment=delta_assessment,
        performance_metrics_list_grid=performance_metrics_list_grid,
        performance_metrics_list=performance_metrics_list, n_jobs=n_jobs
    )

    # Merge results
    if performances_df_test.empty and performances_df_validation.empty:
        print("Warning (model_selection_wrapper): Both Test and Validation results are empty.")
        return pd.DataFrame()
    elif performances_df_test.empty:
         print("Warning (model_selection_wrapper): Test results are empty. Returning only Validation.")
         return performances_df_validation
    elif performances_df_validation.empty:
         print("Warning (model_selection_wrapper): Validation results are empty. Returning only Test.")
         return performances_df_test
    else:
        # Merge, ensuring 'Parameters summary' exists
        if 'Parameters summary' not in performances_df_test.columns or 'Parameters summary' not in performances_df_validation.columns:
             print("ERROR (model_selection_wrapper): 'Parameters summary' missing. Cannot merge. Returning Test results.")
             return performances_df_test

        # Drop redundant columns from validation before merge
        val_cols_to_drop = ['Parameters', 'Execution time']
        validation_subset = performances_df_validation.drop(columns=val_cols_to_drop, errors='ignore')
        # Outer merge preserves all parameter sets tested
        performances_df_merged = pd.merge(performances_df_test, validation_subset, on='Parameters summary', how='outer')
        return performances_df_merged


def get_summary_performances(performances_df, parameter_column_name="Parameters summary"):
    """Summarizes performance dataframe to find best parameters based on validation metrics."""
    metrics_list = ['AUC ROC', 'Average precision', 'Card Precision@100'] # Assuming CP@100 was the target
    performances_results = pd.DataFrame(columns=metrics_list)

    if performances_df.empty:
        print("Warning: Empty performance dataframe passed to get_summary_performances.")
        # Return structure with N/A
        na_vals = ['N/A'] * len(metrics_list)
        for row_name in ["Best estimated parameters", "Validation performance", "Test performance", "Optimal parameter(s)", "Optimal test performance"]:
             performances_results.loc[row_name] = na_vals
        return performances_results

    # Ensure index is reset for iloc usage
    performances_df = performances_df.reset_index(drop=True)

    best_estimated_parameters = []
    validation_performance = []
    test_performance = []

    for metric in metrics_list:
        val_metric_col = metric + ' Validation'
        val_std_col = val_metric_col + ' Std'
        test_metric_col = metric + ' Test'
        test_std_col = test_metric_col + ' Std'

        # Check if required columns exist
        if val_metric_col not in performances_df.columns or test_metric_col not in performances_df.columns:
             print(f"Warning: Missing columns for metric {metric}. Adding N/A.")
             best_estimated_parameters.append('N/A')
             validation_performance.append('N/A')
             test_performance.append('N/A')
             continue

        # Find best performance based on validation score (handle NaNs)
        valid_scores = pd.to_numeric(performances_df[val_metric_col], errors='coerce')
        if valid_scores.isna().all():
            print(f"Warning: All validation scores for {metric} are NaN.")
            index_best_validation_performance = 0 # Default to first row
            best_param_summary = performances_df.loc[index_best_validation_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            val_perf_str = 'NaN'
            # Get test perf at this index
            test_perf_val = pd.to_numeric(performances_df[test_metric_col], errors='coerce').iloc[index_best_validation_performance]
            test_std_val = pd.to_numeric(performances_df.get(test_std_col, np.nan), errors='coerce').iloc[index_best_validation_performance]
            test_perf_str = f"{test_perf_val:.3f} +/- {test_std_val:.2f}" if not pd.isna(test_perf_val) else 'NaN'
        else:
            index_best_validation_performance = valid_scores.idxmax()
            best_param_summary = performances_df.loc[index_best_validation_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            # Get validation performance string
            val_perf = valid_scores.iloc[index_best_validation_performance]
            val_std = pd.to_numeric(performances_df.get(val_std_col, np.nan), errors='coerce').iloc[index_best_validation_performance]
            val_perf_str = f"{val_perf:.3f} +/- {val_std:.2f}" if not pd.isna(val_perf) else 'NaN'
            # Get test performance string at the same index
            test_perf = pd.to_numeric(performances_df[test_metric_col], errors='coerce').iloc[index_best_validation_performance]
            test_std = pd.to_numeric(performances_df.get(test_std_col, np.nan), errors='coerce').iloc[index_best_validation_performance]
            test_perf_str = f"{test_perf:.3f} +/- {test_std:.2f}" if not pd.isna(test_perf) else 'NaN'

        best_estimated_parameters.append(best_param_summary)
        validation_performance.append(val_perf_str)
        test_performance.append(test_perf_str)

    performances_results.loc["Best estimated parameters"] = best_estimated_parameters
    performances_results.loc["Validation performance"] = validation_performance
    performances_results.loc["Test performance"] = test_performance

    # Find Optimal on Test Set (for reference)
    optimal_parameters = []
    optimal_test_performance = []
    for metric_base in metrics_list:
        test_metric_col = metric_base + ' Test'
        test_std_col = test_metric_col + ' Std'

        if test_metric_col not in performances_df.columns:
            optimal_parameters.append('N/A')
            optimal_test_performance.append('N/A')
            continue

        test_scores = pd.to_numeric(performances_df[test_metric_col], errors='coerce')
        if test_scores.isna().all():
            print(f"Warning: All test scores for {metric_base} are NaN.")
            index_optimal_test_performance = 0 # Default index
            opt_param_summary = performances_df.loc[index_optimal_test_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            opt_test_perf_str = 'NaN'
        else:
            index_optimal_test_performance = test_scores.idxmax()
            opt_param_summary = performances_df.loc[index_optimal_test_performance, parameter_column_name] if parameter_column_name in performances_df.columns else 'N/A'
            opt_test_perf = test_scores.iloc[index_optimal_test_performance]
            opt_test_std = pd.to_numeric(performances_df.get(test_std_col, np.nan), errors='coerce').iloc[index_optimal_test_performance]
            opt_test_perf_str = f"{opt_test_perf:.3f} +/- {opt_test_std:.2f}" if not pd.isna(opt_test_perf) else 'NaN'

        optimal_parameters.append(opt_param_summary)
        optimal_test_performance.append(opt_test_perf_str)

    performances_results.loc["Optimal parameter(s)"] = optimal_parameters
    performances_results.loc["Optimal test performance"] = optimal_test_performance

    return performances_results

================
File: infrastructure/modules/aml-workspace/main.tf
================
resource "azurerm_machine_learning_workspace" "mlw" {
  name                    = "mlw-${var.prefix}-${var.postfix}${var.env}"
  location                = var.location
  resource_group_name     = var.rg_name
  application_insights_id = var.application_insights_id
  key_vault_id            = var.key_vault_id
  storage_account_id      = var.storage_account_id
  container_registry_id   = var.container_registry_id

  sku_name = "Basic"

  identity {
    type = "SystemAssigned"
  }

  tags = var.tags
}

# Compute cluster

resource "azurerm_machine_learning_compute_cluster" "adl_aml_ws_compute_cluster" {
  name                          = "cpu-cluster"
  location                      = var.location
  vm_priority                   = "LowPriority"
  vm_size                       = "Standard_DS3_v2"
  machine_learning_workspace_id = azurerm_machine_learning_workspace.mlw.id
  count                         = var.enable_aml_computecluster ? 1 : 0

  scale_settings {
    min_node_count                       = 0
    max_node_count                       = 4
    scale_down_nodes_after_idle_duration = "PT120S" # 120 seconds
  }
}

# # Datastore

# resource "azurerm_resource_group_template_deployment" "arm_aml_create_datastore" {
#   name                = "arm_aml_create_datastore"
#   resource_group_name = var.rg_name
#   deployment_mode     = "Incremental"
#   parameters_content = jsonencode({
#     "WorkspaceName" = {
#       value = azurerm_machine_learning_workspace.mlw.name
#     },
#     "StorageAccountName" = {
#       value = var.storage_account_name
#     }
#   })

#   depends_on = [time_sleep.wait_30_seconds]

#   template_content = <<TEMPLATE
# {
#   "$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
#   "contentVersion": "1.0.0.0",
#   "parameters": {
#         "WorkspaceName": {
#             "type": "String"
#         },
#         "StorageAccountName": {
#             "type": "String"
#         }
#     },
#   "resources": [
#         {
#             "type": "Microsoft.MachineLearningServices/workspaces/datastores",
#             "apiVersion": "2021-03-01-preview",
#             "name": "[concat(parameters('WorkspaceName'), '/default')]",
#             "dependsOn": [],
#             "properties": {
#                 "contents": {
#                     "accountName": "[parameters('StorageAccountName')]",
#                     "containerName": "default",
#                     "contentsType": "AzureBlob",
#                     "credentials": {
#                       "credentialsType": "None"
#                     },
#                     "endpoint": "core.windows.net",
#                     "protocol": "https"
#                   },
#                   "description": "Default datastore for mlops-tabular",
#                   "isDefault": false,
#                   "properties": {
#                     "ServiceDataAccessAuthIdentity": "None"
#                   },
#                   "tags": {}
#                 }
#         }
#   ]
# }
# TEMPLATE
# }

# resource "time_sleep" "wait_30_seconds" {

#   depends_on = [
#     azurerm_machine_learning_workspace.mlw
#   ]

#   create_duration = "30s"
# }

================
File: infrastructure/modules/aml-workspace/outputs.tf
================
output "name" {
  value = azurerm_machine_learning_workspace.mlw.name
}

================
File: infrastructure/modules/aml-workspace/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

variable "storage_account_id" {
  type        = string
  description = "The ID of the Storage Account linked to AML workspace"
}

variable "key_vault_id" {
  type        = string
  description = "The ID of the Key Vault linked to AML workspace"
}

variable "application_insights_id" {
  type        = string
  description = "The ID of the Application Insights linked to AML workspace"
}

variable "container_registry_id" {
  type        = string
  description = "The ID of the Container Registry linked to AML workspace"
}

variable "enable_aml_computecluster" {
  description = "Variable to enable or disable AML compute cluster"
  default     = false
}

variable "storage_account_name" {
  type        = string
  description = "The Name of the Storage Account linked to AML workspace"
}

================
File: infrastructure/modules/application-insights/main.tf
================
resource "azurerm_application_insights" "appi" {
  name                = "appi-${var.prefix}-${var.postfix}${var.env}"
  location            = var.location
  resource_group_name = var.rg_name
  application_type    = "web"

  tags = var.tags
}

================
File: infrastructure/modules/application-insights/outputs.tf
================
output "id" {
  value = azurerm_application_insights.appi.id
}

================
File: infrastructure/modules/application-insights/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/container-registry/main.tf
================
locals {
  safe_prefix  = replace(var.prefix, "-", "")
  safe_postfix = replace(var.postfix, "-", "")
}

resource "azurerm_container_registry" "cr" {
  name                = "cr${local.safe_prefix}${local.safe_postfix}${var.env}"
  resource_group_name = var.rg_name
  location            = var.location
  sku                 = "Standard"
  admin_enabled       = true

  tags = var.tags
}

================
File: infrastructure/modules/container-registry/outputs.tf
================
output "id" {
  value = azurerm_container_registry.cr.id
}

================
File: infrastructure/modules/container-registry/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/data-explorer/main.tf
================
data "azurerm_client_config" "current" {}

resource "azurerm_kusto_cluster" "cluster" {
  name                = "adx${var.prefix}${var.postfix}${var.env}"
  location            = var.location
  resource_group_name = var.rg_name
  streaming_ingestion_enabled = true
  language_extensions = ["PYTHON"]
  count               = var.enable_monitoring ? 1 : 0

  sku {
    name     = "Standard_D11_v2"
    capacity = 2
  }
  tags = var.tags
}

resource "azurerm_kusto_database" "database" {
  name                = "mlmonitoring"
  resource_group_name = var.rg_name
  location            = var.location
  cluster_name        = azurerm_kusto_cluster.cluster[0].name
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "SP_ID" {
  name         = "kvmonitoringspid"
  value        = data.azurerm_client_config.current.client_id
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "SP_KEY" {
  name         = "kvmonitoringspkey"
  value        = var.client_secret
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "SP_TENANT_ID" {
  name         = "kvmonitoringadxtenantid"
  value        = data.azurerm_client_config.current.tenant_id
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "ADX_URI" {
  name         = "kvmonitoringadxuri"
  value        = azurerm_kusto_cluster.cluster[0].uri
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

resource "azurerm_key_vault_secret" "ADX_DB" {
  name         = "kvmonitoringadxdb"
  value        = azurerm_kusto_database.database[0].name
  key_vault_id = var.key_vault_id
  count               = var.enable_monitoring ? 1 : 0
}

================
File: infrastructure/modules/data-explorer/outputs.tf
================


================
File: infrastructure/modules/data-explorer/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

variable "key_vault_id" {
  type        = string
  description = "The ID of the Key Vault linked to AML workspace"
}

variable "enable_monitoring" {
  description = "Variable to enable or disable AML compute cluster"
  default     = false
}

variable "client_secret" {
  description = "client secret"
  default     = false
}

================
File: infrastructure/modules/key-vault/main.tf
================
data "azurerm_client_config" "current" {}

resource "azurerm_key_vault" "kv" {
  name                = "kv-${var.prefix}-${var.postfix}${var.env}"
  location            = var.location
  resource_group_name = var.rg_name
  tenant_id           = data.azurerm_client_config.current.tenant_id
  sku_name            = "standard"

  tags = var.tags
  access_policy {
    tenant_id = data.azurerm_client_config.current.tenant_id
    object_id = data.azurerm_client_config.current.object_id

    key_permissions = [
      "Create",
      "Get",
    ]

    secret_permissions = [
      "Set",
      "Get",
      "Delete",
      "Purge",
      "Recover"
    ]
  }
}

================
File: infrastructure/modules/key-vault/outputs.tf
================
output "id" {
  value = azurerm_key_vault.kv.id
}

================
File: infrastructure/modules/key-vault/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the deployed resource"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/resource-group/main.tf
================
resource "azurerm_resource_group" "adl_rg" {
  name     = "rg-${var.prefix}-${var.postfix}${var.env}"
  location = var.location
  tags     = var.tags
}

================
File: infrastructure/modules/resource-group/outputs.tf
================
output "name" {
  value = azurerm_resource_group.adl_rg.name
}

output "location" {
  value = azurerm_resource_group.adl_rg.location
}

================
File: infrastructure/modules/resource-group/variables.tf
================
variable "location" {
  type        = string
  default     = "North Europe"
  description = "Location of the Resource Group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the Resource Group"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

================
File: infrastructure/modules/storage-account/main.tf
================
data "azurerm_client_config" "current" {}

data "http" "ip" {
  url = "https://ifconfig.me"
}

locals {
  safe_prefix  = replace(var.prefix, "-", "")
  safe_postfix = replace(var.postfix, "-", "")
}

resource "azurerm_storage_account" "st" {
  name                     = "st${local.safe_prefix}${local.safe_postfix}${var.env}"
  resource_group_name      = var.rg_name
  location                 = var.location
  account_tier             = "Standard"
  account_replication_type = "LRS"
  account_kind             = "StorageV2"
  is_hns_enabled           = var.hns_enabled

  tags = var.tags
  
}

# Virtual Network & Firewall configuration

resource "azurerm_storage_account_network_rules" "firewall_rules" {
  storage_account_id = azurerm_storage_account.st.id

  default_action             = "Allow"
  ip_rules                   = [] # [data.http.ip.body]
  virtual_network_subnet_ids = var.firewall_virtual_network_subnet_ids
  bypass                     = var.firewall_bypass
}

================
File: infrastructure/modules/storage-account/outputs.tf
================
output "id" {
  value = azurerm_storage_account.st.id
}

output "name" {
  value = azurerm_storage_account.st.name
}

================
File: infrastructure/modules/storage-account/variables.tf
================
variable "rg_name" {
  type        = string
  description = "Resource group name"
}

variable "location" {
  type        = string
  description = "Location of the resource group"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "A mapping of tags which should be assigned to the Resource Group"
}

variable "prefix" {
  type        = string
  description = "Prefix for the module name"
}

variable "postfix" {
  type        = string
  description = "Postfix for the module name"
}

variable "env" {
  type        = string
  description = "Environment prefix"
}

variable "hns_enabled" {
  type        = bool
  description = "Hierarchical namespaces enabled/disabled"
  default     = true
}

variable "firewall_virtual_network_subnet_ids" {
  default = []
}

variable "firewall_bypass" {
  default = ["None"]
}

================
File: infrastructure/aml_deploy.tf
================
# Resource group

module "resource_group" {
  source = "./modules/resource-group"

  location = var.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

# Azure Machine Learning workspace

module "aml_workspace" {
  source = "./modules/aml-workspace"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  storage_account_id      = module.storage_account_aml.id
  key_vault_id            = module.key_vault.id
  application_insights_id = module.application_insights.id
  container_registry_id   = module.container_registry.id

  enable_aml_computecluster = var.enable_aml_computecluster
  storage_account_name      = module.storage_account_aml.name

  tags = local.tags
}

# Storage account

module "storage_account_aml" {
  source = "./modules/storage-account"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  hns_enabled                         = false
  firewall_bypass                     = ["AzureServices"]
  firewall_virtual_network_subnet_ids = []

  tags = local.tags
}

# Key vault

module "key_vault" {
  source = "./modules/key-vault"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

# Application insights

module "application_insights" {
  source = "./modules/application-insights"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

# Container registry

module "container_registry" {
  source = "./modules/container-registry"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment

  tags = local.tags
}

module "data_explorer" {
  source = "./modules/data-explorer"

  rg_name  = module.resource_group.name
  location = module.resource_group.location

  prefix  = var.prefix
  postfix = var.postfix
  env = var.environment
  key_vault_id      = module.key_vault.id
  enable_monitoring = var.enable_monitoring

  client_secret = var.client_secret

  tags = local.tags
}

================
File: infrastructure/locals.tf
================
locals {
  tags = {
    Owner       = "mlops-v2"
    Project     = "mlops-v2"
    Environment = "${var.environment}"
    Toolkit     = "terraform"
    Name        = "${var.prefix}"
  }
}

================
File: infrastructure/main.tf
================
terraform {
  backend "azurerm" {} 
  required_providers {
    azurerm = {
      version = "= 2.99.0"
    }
  }
}

provider "azurerm" {
  features {}
}

data "azurerm_client_config" "current" {}

data "http" "ip" {
  url = "https://ifconfig.me"
}

================
File: infrastructure/variables.tf
================
variable "location" {
  type        = string
  description = "Location of the resource group and modules"
}

variable "prefix" {
  type        = string
  description = "Prefix for module names"
}

variable "environment" {
  type        = string
  description = "Environment information"
}

variable "postfix" {
  type        = string
  description = "Postfix for module names"
}

variable "enable_aml_computecluster" {
  description = "Variable to enable or disable AML compute cluster"
}

variable "enable_monitoring" {
  description = "Variable to enable or disable Monitoring"
}

variable "client_secret" {
  description = "Service Principal Secret"
}

================
File: mlops/azureml/deploy/batch/batch-deployment.yml
================
# Azure ML Batch Deployment Configuration
# Defines how the model is deployed to the batch endpoint

$schema: https://azuremlschemas.azureedge.net/latest/batchDeployment.schema.json
name: default-batch-deploy # Name of this specific deployment within the endpoint
endpoint_name: fraud-detection-batch # Must match the name in batch-endpoint.yml
description: Default batch deployment for the fraud detection model.

model: azureml:fraud-detection-model@latest # Reference the registered model (update name if changed)

# Compute configuration for the batch scoring job
compute: azureml:batch-cluster # Name of the compute cluster to use (created by infra or workflow)
resources:
  instance_count: 1 # Number of compute nodes to use for the job

# Job settings
max_concurrency_per_instance: 2 # Max parallel mini-batch runs per node
mini_batch_size: 1024 # Number of files processed per run (adjust based on data size/memory)
output_action: append_row # How to write results (append_row or summary_only)
output_file_name: predictions.csv # Name of the output file(s) in the job output folder
retry_settings:
  max_retries: 3    # Max retries per mini-batch
  timeout: 300      # Timeout in seconds per mini-batch
error_threshold: 10 # Abort job if more than 10 mini-batches fail (-1 means never abort)
logging_level: info # info or warning

# Optional: Environment for the scoring script (if different from training)
# environment:
#   name: fraud-scoring-env
#   version: 1
#   image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04
#   conda_file: ../../../data-science/environment/score-conda.yml # Define if needed

# Optional: Code configuration if using a custom scoring script
# code_configuration:
#   code: ../../../data-science/src # Path to scoring script folder
#   scoring_script: batch_score.py # Name of the scoring script

================
File: mlops/azureml/deploy/batch/batch-endpoint.yml
================
# Azure ML Batch Endpoint Configuration
# Defines the endpoint itself, which can host multiple deployments

$schema: https://azuremlschemas.azureedge.net/latest/batchEndpoint.schema.json
name: fraud-detection-batch # Unique name for the batch endpoint
description: Batch endpoint for scoring fraud detection transactions.
auth_mode: aad_token # Use Azure Active Directory token for authentication (key auth also possible)

# Optional tags
tags:
  project: FraudDetection
  environment: production # Or relevant environment

================
File: mlops/azureml/deploy/online/online-deployment.yml
================
# Azure ML Managed Online Deployment Configuration
# Defines a specific deployment under a managed online endpoint

$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue # Name of this deployment (e.g., blue, green for A/B testing)
endpoint_name: fraud-detection-online # Must match the name in online-endpoint.yml
description: Blue deployment for the real-time fraud detection model.

model: azureml:fraud-detection-model@latest # Reference the registered model (update name if changed)

# Instance configuration
instance_type: Standard_DS2_v2 # VM SKU for the deployment (choose based on load/latency needs)
instance_count: 1              # Number of instances (nodes) for the deployment

# Optional: Define request settings (timeouts, etc.)
# request_settings:
#   request_timeout_ms: 5000
#   max_concurrent_requests_per_instance: 1
#   max_queue_wait_ms: 500

# Optional: Liveness/Readiness probes
# liveness_probe:
#   failure_threshold: 30
#   period_seconds: 10
#   initial_delay_seconds: 10
# readiness_probe:
#   failure_threshold: 3
#   period_seconds: 10
#   initial_delay_seconds: 10

# Optional: Define environment and code for scoring (if different from model's default)
# environment:
#   name: fraud-scoring-env
#   version: 1
#   image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04
#   conda_file: ../../../data-science/environment/score-conda.yml # Define if needed
# code_configuration:
#   code: ../../../data-science/src/scoring # Path to scoring script folder
#   scoring_script: score.py # Name of the online scoring script

================
File: mlops/azureml/deploy/online/online-endpoint.yml
================
# Azure ML Managed Online Endpoint Configuration
# Defines the endpoint URL and authentication

$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: fraud-detection-online # Unique name for the online endpoint
description: Real-time endpoint for predicting transaction fraud likelihood.
auth_mode: key # Use key-based authentication (alternatively: 'aad_token')

# Optional tags
tags:
  project: FraudDetection
  environment: production # Or relevant environment

# Optional: Define public network access (defaults to enabled)
# public_network_access: "enabled" # or "disabled"

# Optional: Link to specific compute (not typically needed for managed endpoints)

================
File: mlops/azureml/train/data.yml
================
# Azure ML Data Asset Definition (v2 YAML)
# Defines the RAW data used as input to the preparation step of the pipeline.

$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

type: uri_folder # Use uri_folder for a directory of files
name: raw-fraud-data # Name for this data asset in Azure ML
description: Raw daily transaction data files (.pkl format) for fraud detection.

# Path to the data within your Azure ML datastore.
# Assumes the raw .pkl files are in a folder named 'raw-fraud-data'
# in the default workspace datastore ('workspaceblobstore').
# Adjust 'path' if your data is elsewhere (e.g., different folder, different datastore).
path: https://mlopthesis4168280320.blob.core.windows.net/azureml-blobstore-3fb6f68d-09da-48f9-b653-4256fe80b9db/fraud-data-raw/

# Optional: Specify version (if omitted, defaults to autoincrementing version)
# version: 1

# Optional: Add tags
# tags:
#   stage: raw
#   project: FraudDetection

================
File: mlops/azureml/train/environment.yml
================
# Azure ML Environment Definition (v2 YAML)
# Defines the runtime environment for pipeline steps.

$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: fraud-detection-train-env # Name for the environment asset in Azure ML
version: 1 # Version for the environment asset
description: Conda environment for training the fraud detection model, based on user notebooks.

# Using a standard Azure ML curated image as a base
image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04 # Choose a suitable base image

# Reference the Conda environment file
conda_file: ../../../data-science/environment/train-conda.yml

# Optional: Add OS packages if needed
# os_packages:
#   - package1
#   # ...

# Optional: Add environment variables
# environment_variables:
#   MY_ENV_VAR: "value"

# Optional: Add tags
# tags:
#   project: FraudDetection

================
File: mlops/azureml/train/pipeline.yml
================
# Azure ML Pipeline Definition (v2 YAML)
# Defines the sequence of steps for training the fraud detection model.


$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline

display_name: Fraud_Detection_Training_Pipeline
description: Pipeline to preprocess data, train (XGBoost GridSearch), evaluate, and register a fraud detection model.
experiment_name: fraud-detection-training

# --- Pipeline Inputs ---
inputs:
  raw_data_input:
    type: uri_folder
    path: azureml:raw-fraud-data@latest # Assumes raw data is registered
    mode: download
  # Date range for the *output* of the prep step (and input to train step)
  # This also defines the end date for raw data loading in prep step.
  prep_output_start_date: "2025-06-11"
  prep_output_end_date: "2025-08-14"
  # Anchor date for training splits (used *inside* train.py)
  anchor_date_str: "2025-07-25"
  # Deltas used for calculating splits (used *inside* train.py)
  delta_train: 7
  delta_delay: 7
  delta_assessment: 7
  # Folds for prequential validation (used *inside* train.py)
  n_folds: 4
  # Evaluation threshold
  deploy_threshold_value: 0.7

# --- Pipeline Outputs ---
outputs:
  trained_model_output:
    type: mlflow_model
    mode: upload
  evaluation_report_output:
    type: uri_folder
    mode: upload
  model_info_output:
     type: uri_folder
     mode: upload

# --- Pipeline Settings ---
settings:
  default_datastore: azureml:workspaceblobstore
  default_compute: azureml:cpu-cluster
  continue_on_step_failure: false

# --- Pipeline Jobs (Steps) ---
jobs:
  # 1. Preparation Step
  prep_data:
    type: command
    name: prep_raw_data
    display_name: Prepare Raw Data
    description: Applies feature transformations considering lookback.
    inputs:
      raw_data: ${{parent.inputs.raw_data_input}}
      # Pass the desired *output* date range for transformed data
      output_start_date: ${{parent.inputs.prep_output_start_date}}
      output_end_date: ${{parent.inputs.prep_output_end_date}}
    outputs:
      transformed_data: # Output folder containing transformed daily files
        type: uri_folder
        mode: upload
    code: ../../../data-science/src
    command: >-
      python prep.py
      --raw_data ${{inputs.raw_data}}
      --transformed_data ${{outputs.transformed_data}}
      --output_start_date ${{inputs.output_start_date}}
      --output_end_date ${{inputs.output_end_date}}
    environment: azureml:fraud-detection-train-env@latest

  # 2. Training Step (Adjusted Inputs and Command)
  train_model:
    type: command
    name: train_fraud_model
    display_name: Train Fraud Detection Model (XGBoost GridSearch)
    description: Performs grid search for XGBoost and trains the final model.
    inputs:
      transformed_data: ${{parent.jobs.prep_data.outputs.transformed_data}} # Input folder of transformed data
      # --- Pass the DATES defining the window of transformed data to load ---
      train_load_start_date: ${{parent.inputs.prep_output_start_date}} # Use the start date from prep output
      train_load_end_date: ${{parent.inputs.prep_output_end_date}}   # Use the end date from prep output
      # --- Pass the ANCHOR date and DELTAS for splitting ---
      anchor_date_str: ${{parent.inputs.anchor_date_str}}
      delta_train: ${{parent.inputs.delta_train}}
      delta_delay: ${{parent.inputs.delta_delay}}
      delta_assessment: ${{parent.inputs.delta_assessment}}
      n_folds: ${{parent.inputs.n_folds}}
      # Optional: Pass top_k, n_jobs if needed
      # top_k_value: 100
      n_jobs: -1
    outputs:
      model_output: ${{parent.outputs.trained_model_output}}
      test_data_output: # Output folder containing the final_test_data.pkl
          type: uri_folder
          mode: upload
    code: ../../../data-science/src
    command: >-
      python train.py
      --transformed_data ${{inputs.transformed_data}}
      --model_output ${{outputs.model_output}}
      --test_data_output ${{outputs.test_data_output}}
      # --- Pass correct arguments to train.py ---
      --train_load_start_date ${{inputs.train_load_start_date}}
      --train_load_end_date ${{inputs.train_load_end_date}}
      --anchor_date_str ${{inputs.anchor_date_str}}
      --delta_train ${{inputs.delta_train}}
      --delta_delay ${{inputs.delta_delay}}
      --delta_assessment ${{inputs.delta_assessment}}
      --n_folds ${{inputs.n_folds}}
      # Pass optional args if defined in inputs:
      # --top_k_value ${{inputs.top_k_value}}
      # --n_jobs ${{inputs.n_jobs}}
    environment: azureml:fraud-detection-train-env@latest

  # 4. Registration Step (No changes needed here)
  register_model:
    type: command
    name: register_evaluated_model
    display_name: Register Model (if approved)
    description: Registers the model in the Azure ML workspace if deploy flag is set.
    inputs:
      model_path: ${{parent.jobs.train_model.outputs.model_output}}
      evaluation_output: ${{parent.jobs.evaluate_model.outputs.evaluation_output}}
      model_name: "fraud-detection-model"
    outputs:
       model_info_output_path: ${{parent.outputs.model_info_output}}
    code: ../../../data-science/src
    command: >-
      python register.py
      --model_name ${{inputs.model_name}}
      --model_path ${{inputs.model_path}}
      --evaluation_output ${{inputs.evaluation_output}}
      --model_info_output_path ${{outputs.model_info_output_path}}
    environment: azureml:fraud-detection-train-env@latest

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb # Ignore notebooks themselves, keep only scripts

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# VS Code settings
.vscode/

# Terraform state and cache
.terraform/
*.tfstate
*.tfstate.*
terraform.tfvars
*.tfvars
*.tfplan
.terraform.lock.hcl
*.hcl.lock

# Azure ML specific
aml_config/
config.json # Store securely or generate dynamically
*.azureml
.azureml/

# Data files (add specific patterns if needed)
/data/ # Ignore raw data folder if added to repo locally
/simulated-data-raw/
/simulated-data-transformed/
*.pkl # Ignore pickle files if generated locally
*.csv # Ignore csv files if generated locally
*.parquet # Ignore parquet files if generated locally

# Model files (if saved locally outside pipeline outputs)
*.joblib
*.onnx
outputs/ # Ignore typical output folders

# OS generated files
.DS_Store
Thumbs.db

# Log files
*.log

# Temporary files
*.tmp
*.swp
*~

================
File: .pre-commit-config.yaml
================
# See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0 # Use a recent version
    hooks:
    -   id: trailing-whitespace
    -   id: end-of-file-fixer
    -   id: check-yaml
    -   id: check-added-large-files
    -   id: check-toml
    -   id: check-merge-conflict
-   repo: https://github.com/psf/black
    rev: 24.4.0 # Use a recent version of Black
    hooks:
    -   id: black
        language_version: python3.9 # Specify your python version
-   repo: https://github.com/pycqa/flake8
    rev: 7.0.0 # Use a recent version of Flake8
    hooks:
    -   id: flake8
        # args: ['--max-line-length=88', '--extend-ignore=E203'] # Example: align with Black
-   repo: https://github.com/pycqa/isort
    rev: 5.13.2 # Use a recent version of isort
    hooks:
      - id: isort
        name: isort (python)
        args: ["--profile", "black"] # Make isort compatible with Black
# -   repo: https://github.com/antonbabenko/pre-commit-terraform
#     rev: v1.88.4 # Use a recent version
#     hooks:
#       - id: terraform_fmt
#       - id: terraform_validate
#       - id: terraform_docs # Optional: generate TF docs

================
File: CODE_OF_CONDUCT.md
================
# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

*   [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
*   [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
*   Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

================
File: config-infra-prod.yml
================
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

# Production Environment Infrastructure Configuration
# --- USER ACTION REQUIRED: Update values below ---
variables:

  # --- Global Settings ---
  # Define a short, lowercase alphanumeric namespace for your project (e.g., 'fraudproj')
  # Note: Max length constraints apply to derived names (e.g., storage accounts max 24 chars). Keep it short.
  namespace: fraudml # EXAMPLE: Replace with your project namespace
  # Define a short, unique postfix (e.g., initials, random chars like 'a0b1')
  postfix: fdx01 # EXAMPLE: Replace with your unique postfix
  # Azure region for deployment (e.g., 'westus2', 'eastus', 'westeurope')
  location: westus2 # EXAMPLE: Replace with your desired Azure region

  # Environment identifier (used in resource naming)
  environment: prod

  # --- Feature Flags ---
  # Enable creation of a default CPU compute cluster in AML Workspace?
  enable_aml_computecluster: true
  # Enable creation of Azure Data Explorer for monitoring (adds cost)?
  enable_monitoring: false
  # Enable secure workspace features (e.g., VNet integration - requires more setup)?
  enable_aml_secure_workspace: false # Set to true for VNet scenarios


  # Azure DevOps
  # ado_service_connection_rg: Azure-ARM-Prod
  # ado_service_connection_aml_ws: Azure-ARM-Prod

  # DO NOT TOUCH

  # For pipeline reference
  resource_group: rg-$(namespace)-$(postfix)$(environment)
  aml_workspace: mlw-$(namespace)-$(postfix)$(environment)
  application_insights: mlw-$(namespace)-$(postfix)$(environment)
  key_vault: kv-$(namespace)-$(postfix)$(environment)
  container_registry: cr$(namespace)$(postfix)$(environment)
  storage_account: st$(namespace)$(postfix)$(environment)

  # For terraform reference
  terraform_version: 0.14.7
  terraform_workingdir: infrastructure
  terraform_st_resource_group: rg-$(namespace)-$(postfix)$(environment)-tf
  terraform_st_location: $(location)
  terraform_st_storage_account: st$(namespace)$(postfix)$(environment)tf
  terraform_st_container_name: default
  # terraform_st_container_name: tfstate
  terraform_st_key: mlops-tab
  # terraform_st_key: fraud-mlops-$(environment).tfstate


  # Batch/Online Endpoint names (example derivation, adjust as needed)
  # Ensure these result in valid Azure resource names
  bep: fraud-batch-ep-$(postfix) # Batch Endpoint name part
  oep: fraud-online-ep-$(postfix) # Online Endpoint name part

================
File: LICENSE
================
MIT License

Copyright (c) Microsoft Corporation.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE

================
File: README.md
================
# Azure MLOps - Fraud Detection Example

This repository provides a structured example of implementing an MLOps pipeline for a fraud detection machine learning model using Azure Machine Learning and GitHub Actions. It adapts the concepts from the [Azure MLOps (v2) solution accelerator](https://github.com/Azure/mlops-v2) and applies them to a specific fraud detection scenario based on provided notebooks.

## Project Structure

*   **`.github/workflows`**: Contains GitHub Actions workflows for:
    *   `tf-gha-deploy-infra.yml`: Deploying Azure infrastructure (AML Workspace, Storage, etc.) using Terraform.
    *   `deploy-model-training-pipeline.yml`: Orchestrating the model training pipeline in Azure ML.
    *   `deploy-online-endpoint-pipeline.yml`: Deploying the trained model to a managed online endpoint.
    *   `deploy-batch-endpoint-pipeline.yml`: Deploying the trained model to a batch endpoint.
*   **`data-science`**: Holds the core data science code.
    *   `environment/train-conda.yml`: Conda environment definition for training.
    *   `src/`: Python scripts for individual pipeline steps (`prep.py`, `train.py`, `evaluate.py`, `register.py`) and shared utilities (`utils.py`).
*   **`infrastructure`**: Contains Terraform modules and configuration for deploying Azure resources.
*   **`mlops/azureml`**: Azure Machine Learning specific configurations.
    *   `train/`: Definitions for the training pipeline (`pipeline.yml`), data asset (`data.yml`), and environment (`environment.yml`).
    *   `deploy/`: Definitions for online and batch endpoints and deployments.
*   **`config-infra-prod.yml`**: Configuration file defining resource names, locations, and feature flags for the production environment infrastructure.

## Scenario Overview

The pipeline implements the following steps:

1.  **Data Preparation (`prep.py`)**:
    *   Reads raw daily transaction data (pickle files).
    *   Applies feature transformations:
        *   Date/Time features (weekend, night).
        *   Customer spending behavior (aggregates over time windows).
        *   Terminal risk scores (aggregates over time windows with delay).
    *   Saves the transformed data back into daily pickle files.
2.  **Model Training (`train.py`)**:
    *   Loads the transformed data.
    *   Performs model selection using prequential cross-validation **specifically for a Decision Tree classifier** (as requested). Hyperparameter tuning uses GridSearchCV.
    *   Trains the final Decision Tree model on the designated training split using the best found hyperparameters (or defaults).
    *   Logs the trained model artifact using MLflow.
    *   Saves the defined test data split for the evaluation step.
3.  **Model Evaluation (`evaluate.py`)**:
    *   Loads the trained model artifact.
    *   Loads the test data split.
    *   Calculates performance metrics (AUC ROC, Average Precision, Card Precision@k).
    *   Determines a deployment flag based on whether a primary metric (e.g., Average Precision) meets a predefined threshold.
    *   Logs metrics to MLflow.
4.  **Model Registration (`register.py`)**:
    *   Checks the deployment flag from the evaluation step.
    *   If the flag is set, registers the trained model artifact in the Azure ML Model Registry.
    *   Outputs model registration information (name and version).

## Prerequisites

1.  **Azure Subscription**: Access to an Azure subscription.
2.  **Azure ML Workspace**: An existing Azure Machine Learning workspace (or deploy one using the `tf-gha-deploy-infra.yml` workflow).
3.  **GitHub Repository**: A GitHub repository based on this template.
4.  **Service Principal**: An Azure Service Principal with `Contributor` rights on the subscription (or target resource group).
5.  **GitHub Secrets**: Configure the following secrets in your GitHub repository settings:
    *   `AZURE_CREDENTIALS`: The JSON output from creating the service principal (`az ad sp create-for-rbac --role Contributor --sdk-auth`).
    *   `ARM_CLIENT_ID`: The Client ID of the service principal.
    *   `ARM_CLIENT_SECRET`: The Client Secret of the service principal.
    *   `ARM_SUBSCRIPTION_ID`: Your Azure Subscription ID.
    *   `ARM_TENANT_ID`: Your Azure Tenant ID.
6.  **Terraform State Storage**: An Azure Storage Account and container created beforehand to store Terraform state files (configure names in `config-infra-prod.yml`).
7.  **Raw Data**: Upload your raw daily transaction pickle files to the location specified in `mlops/azureml/train/data.yml` (default: `raw-fraud-data` folder in the default workspace datastore).

## Getting Started

1.  **Fork/Clone**: Fork or clone this repository.
2.  **Configure**:
    *   Update `config-infra-prod.yml` with your desired `namespace`, `postfix`, `location`, and Terraform backend details.
    *   Review `mlops/azureml/train/data.yml` and ensure the `path` points to your raw data location in Azure ML.
    *   Set up the required GitHub Secrets.
    *   Ensure the Terraform state storage account/container exists.
3.  **Deploy Infrastructure (Optional)**: Run the `tf-gha-deploy-infra` workflow from the GitHub Actions tab to provision Azure resources.
4.  **Run Training Pipeline**: Run the `deploy-model-training-pipeline` workflow from the GitHub Actions tab. This will execute the prep, train, evaluate, and register steps in Azure ML.
5.  **Deploy Model (Optional)**:
    *   Run the `deploy-online-endpoint-pipeline` workflow to deploy the registered model to a real-time endpoint.
    *   Run the `deploy-batch-endpoint-pipeline` workflow to deploy the registered model to a batch scoring endpoint.

## Customization

*   **Data Source**: Modify `mlops/azureml/train/data.yml` if your raw data format or location differs.
*   **Feature Engineering**: Update `data-science/src/prep.py` and potentially `data-science/src/utils.py` to change feature transformations.
*   **Model Selection**: Modify `data-science/src/train.py` to include different classifiers or change the hyperparameter grids. Remember the current version is hardcoded for Decision Tree grid search only.
*   **Evaluation**: Adjust metrics or the deployment logic in `data-science/src/evaluate.py`.
*   **Environment**: Update `data-science/environment/train-conda.yml` to add or remove dependencies.
*   **Infrastructure**: Modify Terraform files in the `infrastructure` directory if different Azure resources are needed.

================
File: requirements.txt
================
# Requirements for local development or testing (if needed)
# The primary environment definition is in data-science/environment/train-conda.yml

# Linters and Formatters (for pre-commit)
black>=23.0,<25.0
flake8>=6.0,<8.0
isort>=5.10,<6.0
pre-commit>=3.0,<4.0

# Core ML libraries (versions should ideally match conda env for consistency)
# numpy>=1.19.5,<1.24.0
# pandas>=1.3.5,<1.6.0
# scikit-learn>=1.0.0,<1.2.0
# imbalanced-learn>=0.8.1,<0.11.0
# xgboost>=1.5.1,<1.8.0
# matplotlib
# seaborn>=0.11.2,<0.13.0
# joblib

# Azure ML & MLflow (Ensure compatibility)
# azure-ai-ml>=1.5.0 # V2 SDK
# azure-identity>=1.10.0
# or
# azureml-sdk>=1.50.0 # V1 SDK
# azureml-mlflow>=1.50.0 # V1 MLflow integration
# mlflow>=2.0,<2.10

# Other utilities
# GitPython>=3.1,<3.2 # For Git integration if needed locally

# Note: It's generally recommended to manage environments using Conda via the .yml file,
# especially when dealing with complex dependencies like those in scientific Python.
# This requirements.txt is mainly for tooling like pre-commit.

================
File: SECURITY.md
================
<!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc).

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

*   Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
*   Full paths of source file(s) related to the manifestation of the issue
*   The location of the affected source code (tag/branch/commit or direct URL)
*   Any special configuration required to reproduce the issue
*   Step-by-step instructions to reproduce the issue
*   Proof-of-concept or exploit code (if possible)
*   Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->

================
File: SUPPORT.md
================
# Support

## How to file issues and get help

This project uses GitHub Issues to track bugs and feature requests. Please search the existing
issues before filing new issues to avoid duplicates. For new issues, file your bug or
feature request as a new Issue.

For help and questions about using this project, please refer to the project's README.md file or file an issue in the GitHub repository. Community contributions and discussions via issues and pull requests are welcome.

## Microsoft Support Policy

Support for this project is limited to the resources listed above. This is a community-supported project and not officially supported by Microsoft Customer Service & Support (CSS).



